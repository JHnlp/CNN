test_yhat=sign(sign(test_yhat)+0.1)
test_error=sum(test_yhat!=ytest)/10000
return(c(train_error,test_error))
}
error=sapply(1:20, adaboost)
View(error)
error=sapply(1:20, adaboost)
View(error)
aboost=function(boost_iteration){
w=rep(1/2000,2000)
alpha=NULL
J=NULL
for(i in 1:boost_iteration){
bestL=Inf
for(j in 1:10){
loss=L(h(j,classify_threshold[j],X),w)
if(loss<bestL){
bestL=loss
bestj=j
}
}
alpha_j=log((1-bestL)/bestL)
w=w*exp(alpha_j*(y!=h(bestj,classify_threshold[bestj],X)))
alpha=c(alpha,alpha_j)
J=c(J,bestj)
}
rlt_ma=NULL
for(j in J){
rlt_ma=cbind(rlt_ma,h(j,classify_threshold[j],X))
}
yhat=rowSums(t(t(rlt_ma)*alpha))
yhat=sign(sign(yhat)+0.1)
train_error=sum(yhat!=y)/2000
test_rlt_ma=NULL
for(j in J){
test_rlt_ma=cbind(test_rlt_ma,h(j,classify_threshold[j],Xtest))
}
test_yhat=rowSums(t(t(test_rlt_ma)*alpha))
test_yhat=sign(sign(test_yhat)+0.1)
test_error=sum(test_yhat!=ytest)/10000
return(list(c(train_error,test_error),J,alpha))
}
out=sapply(1:20, adaboost)
View(out)
adaboost
adaboost=function(boost_iteration){
w=rep(1/2000,2000)
alpha=NULL
J=NULL
for(i in 1:boost_iteration){
bestL=Inf
for(j in 1:10){
loss=L(h(j,classify_threshold[j],X),w)
if(loss<bestL){
bestL=loss
bestj=j
}
}
alpha_j=log((1-bestL)/bestL)
w=w*exp(alpha_j*(y!=h(bestj,classify_threshold[bestj],X)))
alpha=c(alpha,alpha_j)
J=c(J,bestj)
}
rlt_ma=NULL
for(j in J){
rlt_ma=cbind(rlt_ma,h(j,classify_threshold[j],X))
}
yhat=rowSums(t(t(rlt_ma)*alpha))
yhat=sign(sign(yhat)+0.1)
train_error=sum(yhat!=y)/2000
test_rlt_ma=NULL
for(j in J){
test_rlt_ma=cbind(test_rlt_ma,h(j,classify_threshold[j],Xtest))
}
test_yhat=rowSums(t(t(test_rlt_ma)*alpha))
test_yhat=sign(sign(test_yhat)+0.1)
test_error=sum(test_yhat!=ytest)/10000
return(list(c(train_error,test_error),J,alpha))
}
adaboost
out=sapply(1:20, adaboost)
out[-1]
out[20]
sp=function(o){
return(rnorm(10))
}
X=sapply(1:2000,sp)
X=t(X)
y=rowSums(X^2)>9.34
y=y*2-1
Xtest=sapply(1:10000,sp)
Xtest=t(Xtest)
ytest=rowSums(Xtest^2)>9.34
ytest=ytest*2-1
decision_stump=function(i){ #the function to find the best threshold for the decision stump
a=seq(0.5,3,0.01)
best_thres=-1
best_loss=2000
for(thres in a ){
yhat=(X[,i]^2>thres)*2-1
loss=sum(yhat!=y)
if(loss<best_loss){
best_thres=thres
best_loss=loss
}
}
return(best_thres)
}
classify_threshold=sapply(1:10,decision_stump) #the threshold for 10 stumps
h=function(index,thres,data){ #function to compute the prediction of stump classifier
yhat=(data[,index]^2>thres)*2-1
return(yhat)
}
L=function(yhat,w){
temp=as.integer(y!=yhat)
return(t(w)%*%temp/sum(w))
}
#do adaboost
adaboost=function(boost_iteration){
w=rep(1/2000,2000)
alpha=NULL
J=NULL
for(i in 1:boost_iteration){
bestL=Inf
for(j in 1:10){
loss=L(h(j,classify_threshold[j],X),w)
if(loss<bestL){
bestL=loss
bestj=j
}
}
alpha_j=log((1-bestL)/bestL)
w=w*exp(alpha_j*(y!=h(bestj,classify_threshold[bestj],X)))
alpha=c(alpha,alpha_j)
J=c(J,bestj)
}
rlt_ma=NULL
for(j in J){
rlt_ma=cbind(rlt_ma,h(j,classify_threshold[j],X))
}
yhat=rowSums(t(t(rlt_ma)*alpha))
yhat=sign(sign(yhat)+0.1)
train_error=sum(yhat!=y)/2000
test_rlt_ma=NULL
for(j in J){
test_rlt_ma=cbind(test_rlt_ma,h(j,classify_threshold[j],Xtest))
}
test_yhat=rowSums(t(t(test_rlt_ma)*alpha))
test_yhat=sign(sign(test_yhat)+0.1)
test_error=sum(test_yhat!=ytest)/10000
print(J)
return(c(train_error,test_error))
}
out=sapply(1:20, adaboost)
out
sp=function(o){
return(rnorm(10))
}
set.seed(1234)
X=sapply(1:2000,sp)
X=t(X)
y=rowSums(X^2)>9.34
y=y*2-1
Xtest=sapply(1:10000,sp)
Xtest=t(Xtest)
ytest=rowSums(Xtest^2)>9.34
ytest=ytest*2-1
decision_stump=function(i){ #the function to find the best threshold for the decision stump
a=seq(0.5,3,0.01)
best_thres=-1
best_loss=2000
for(thres in a ){
yhat=(X[,i]^2>thres)*2-1
loss=sum(yhat!=y)
if(loss<best_loss){
best_thres=thres
best_loss=loss
}
}
return(best_thres)
}
classify_threshold=sapply(1:10,decision_stump) #the threshold for 10 stumps
h=function(index,thres,data){ #function to compute the prediction of stump classifier
yhat=(data[,index]^2>thres)*2-1
return(yhat)
}
L=function(yhat,w){
temp=as.integer(y!=yhat)
return(t(w)%*%temp/sum(w))
}
#do adaboost
adaboost=function(boost_iteration){
w=rep(1/2000,2000)
alpha=NULL
J=NULL
for(i in 1:boost_iteration){
bestL=Inf
for(j in 1:10){
loss=L(h(j,classify_threshold[j],X),w)
if(loss<bestL){
bestL=loss
bestj=j
}
}
alpha_j=log((1-bestL)/bestL)
w=w*exp(alpha_j*(y!=h(bestj,classify_threshold[bestj],X)))
alpha=c(alpha,alpha_j)
J=c(J,bestj)
}
rlt_ma=NULL
for(j in J){
rlt_ma=cbind(rlt_ma,h(j,classify_threshold[j],X))
}
yhat=rowSums(t(t(rlt_ma)*alpha))
yhat=sign(sign(yhat)+0.1)
train_error=sum(yhat!=y)/2000
test_rlt_ma=NULL
for(j in J){
test_rlt_ma=cbind(test_rlt_ma,h(j,classify_threshold[j],Xtest))
}
test_yhat=rowSums(t(t(test_rlt_ma)*alpha))
test_yhat=sign(sign(test_yhat)+0.1)
test_error=sum(test_yhat!=ytest)/10000
print(J)
return(c(train_error,test_error))
}
out=sapply(1:20, adaboost)
out
sp=function(o){
return(rnorm(10))
}
set.seed(123)
X=sapply(1:2000,sp)
X=t(X)
y=rowSums(X^2)>9.34
y=y*2-1
Xtest=sapply(1:10000,sp)
Xtest=t(Xtest)
ytest=rowSums(Xtest^2)>9.34
ytest=ytest*2-1
decision_stump=function(i){ #the function to find the best threshold for the decision stump
a=seq(0.5,3,0.01)
best_thres=-1
best_loss=2000
for(thres in a ){
yhat=(X[,i]^2>thres)*2-1
loss=sum(yhat!=y)
if(loss<best_loss){
best_thres=thres
best_loss=loss
}
}
return(best_thres)
}
classify_threshold=sapply(1:10,decision_stump) #the threshold for 10 stumps
h=function(index,thres,data){ #function to compute the prediction of stump classifier
yhat=(data[,index]^2>thres)*2-1
return(yhat)
}
L=function(yhat,w){
temp=as.integer(y!=yhat)
return(t(w)%*%temp/sum(w))
}
#do adaboost
adaboost=function(boost_iteration){
w=rep(1/2000,2000)
alpha=NULL
J=NULL
for(i in 1:boost_iteration){
bestL=Inf
for(j in 1:10){
loss=L(h(j,classify_threshold[j],X),w)
if(loss<bestL){
bestL=loss
bestj=j
}
}
alpha_j=log((1-bestL)/bestL)
w=w*exp(alpha_j*(y!=h(bestj,classify_threshold[bestj],X)))
alpha=c(alpha,alpha_j)
J=c(J,bestj)
}
rlt_ma=NULL
for(j in J){
rlt_ma=cbind(rlt_ma,h(j,classify_threshold[j],X))
}
yhat=rowSums(t(t(rlt_ma)*alpha))
yhat=sign(sign(yhat)+0.1)
train_error=sum(yhat!=y)/2000
test_rlt_ma=NULL
for(j in J){
test_rlt_ma=cbind(test_rlt_ma,h(j,classify_threshold[j],Xtest))
}
test_yhat=rowSums(t(t(test_rlt_ma)*alpha))
test_yhat=sign(sign(test_yhat)+0.1)
test_error=sum(test_yhat!=ytest)/10000
print(J)
return(c(train_error,test_error))
}
out=sapply(1:20, adaboost)
out
sp=function(o){
return(rnorm(10))
}
set.seed(1)
X=sapply(1:2000,sp)
X=t(X)
y=rowSums(X^2)>9.34
y=y*2-1
Xtest=sapply(1:10000,sp)
Xtest=t(Xtest)
ytest=rowSums(Xtest^2)>9.34
ytest=ytest*2-1
decision_stump=function(i){ #the function to find the best threshold for the decision stump
a=seq(0.5,3,0.01)
best_thres=-1
best_loss=2000
for(thres in a ){
yhat=(X[,i]^2>thres)*2-1
loss=sum(yhat!=y)
if(loss<best_loss){
best_thres=thres
best_loss=loss
}
}
return(best_thres)
}
classify_threshold=sapply(1:10,decision_stump) #the threshold for 10 stumps
h=function(index,thres,data){ #function to compute the prediction of stump classifier
yhat=(data[,index]^2>thres)*2-1
return(yhat)
}
L=function(yhat,w){
temp=as.integer(y!=yhat)
return(t(w)%*%temp/sum(w))
}
#do adaboost
adaboost=function(boost_iteration){
w=rep(1/2000,2000)
alpha=NULL
J=NULL
for(i in 1:boost_iteration){
bestL=Inf
for(j in 1:10){
loss=L(h(j,classify_threshold[j],X),w)
if(loss<bestL){
bestL=loss
bestj=j
}
}
alpha_j=log((1-bestL)/bestL)
w=w*exp(alpha_j*(y!=h(bestj,classify_threshold[bestj],X)))
alpha=c(alpha,alpha_j)
J=c(J,bestj)
}
rlt_ma=NULL
for(j in J){
rlt_ma=cbind(rlt_ma,h(j,classify_threshold[j],X))
}
yhat=rowSums(t(t(rlt_ma)*alpha))
yhat=sign(sign(yhat)+0.1)
train_error=sum(yhat!=y)/2000
test_rlt_ma=NULL
for(j in J){
test_rlt_ma=cbind(test_rlt_ma,h(j,classify_threshold[j],Xtest))
}
test_yhat=rowSums(t(t(test_rlt_ma)*alpha))
test_yhat=sign(sign(test_yhat)+0.1)
test_error=sum(test_yhat!=ytest)/10000
print(J)
return(c(train_error,test_error))
}
out=sapply(1:20, adaboost)
out
sp=function(o){
return(rnorm(10))
}
set.seed(1)
X=sapply(1:2000,sp)
X=t(X)
y=rowSums(X^2)>9.34
y=y*2-1
Xtest=sapply(1:10000,sp)
Xtest=t(Xtest)
ytest=rowSums(Xtest^2)>9.34
ytest=ytest*2-1
decision_stump=function(i){ #the function to find the best threshold for the decision stump
a=seq(0.5,3,0.01)
best_thres=-1
best_loss=2000
for(thres in a ){
yhat=(X[,i]^2>thres)*2-1
loss=sum(yhat!=y)
if(loss<best_loss){
best_thres=thres
best_loss=loss
}
}
return(best_thres)
}
classify_threshold=sapply(1:10,decision_stump) #the threshold for 10 stumps
h=function(index,thres,data){ #function to compute the prediction of stump classifier
yhat=(data[,index]^2>thres)*2-1
return(yhat)
}
L=function(yhat,w){
temp=as.integer(y!=yhat)
return(t(w)%*%temp/sum(w))
}
#do adaboost
adaboost=function(boost_iteration){
w=rep(1/2000,2000)
alpha=NULL
J=NULL
for(i in 1:boost_iteration){
bestL=Inf
for(j in 1:10){
loss=L(h(j,classify_threshold[j],X),w)
if(loss<bestL){
bestL=loss
bestj=j
}
}
alpha_j=log((1-bestL)/bestL)
w=w*exp(alpha_j*(y!=h(bestj,classify_threshold[bestj],X)))
alpha=c(alpha,alpha_j)
J=c(J,bestj)
}
rlt_ma=NULL
for(j in J){
rlt_ma=cbind(rlt_ma,h(j,classify_threshold[j],X))
}
yhat=rowSums(t(t(rlt_ma)*alpha))
yhat=sign(sign(yhat)+0.1)
train_error=sum(yhat!=y)/2000
test_rlt_ma=NULL
for(j in J){
test_rlt_ma=cbind(test_rlt_ma,h(j,classify_threshold[j],Xtest))
}
test_yhat=rowSums(t(t(test_rlt_ma)*alpha))
test_yhat=sign(sign(test_yhat)+0.1)
test_error=sum(test_yhat!=ytest)/10000
print(J)
return(c(train_error,test_error))
}
error=sapply(1:20, adaboost)
error
plot(1:20,error[1,],type='l',main='error rate vs. number of iteration',xlab='iter', ylab='error rate')
plot(1:20,error[1,],type='l',main='error rate vs. number of iteration',xlab='iter', ylab='error rate',ylim=c(0.32,0.41))
lines(1:20,error[2,],col=2,lty=2)
legend(c(12,0.4),c('train error','test error'),col=c(1,2),lty=c(1,2))
plot(1:20,error[1,],type='l',main='error rate vs. number of iteration',xlab='iter', ylab='error rate',ylim=c(0.32,0.41))
lines(1:20,error[2,],col=2,lty=2)
legend(12,0.4,c('train error','test error'),col=c(1,2),lty=c(1,2))
exp(-1.35)
1.64/4/2000
sqrt(0.0002)
.33/sqrt(2000)*1.65
0.125/(sqrt(0.11/2000))*1.645
pnorm(3.21)
1-ans
1-_
1-pnorm(3.21)
_*2
2*(1-pnorm(3.21))
qnorm(0.5)
pchisq(0.00134)
pchisq(0.00134,df=1)
pnorm(3.13)
(1-pnorm(3.13))*2
1.65^2/2000
1.65/sqrt(2000)
0.007395*1.645
1.64/sqrt(2000)
0.25/2000
sqrt(0.25/2000)
sqrt(0.25/2000)*1.65
sp=function(o){
return(rnorm(10))
}
set.seed(1)
X=sapply(1:2000,sp)
X=t(X)
y=rowSums(X^2)>9.34
y=y*2-1
Xtest=sapply(1:10000,sp)
Xtest=t(Xtest)
ytest=rowSums(Xtest^2)>9.34
ytest=ytest*2-1
decision_stump=function(i){ #the function to find the best threshold for the decision stump
a=seq(0.5,3,0.01)
best_thres=-1
best_loss=2000
for(thres in a ){
yhat=(X[,i]^2>thres)*2-1
loss=sum(yhat!=y)
if(loss<best_loss){
best_thres=thres
best_loss=loss
}
}
return(best_thres)
}
classify_threshold=sapply(1:10,decision_stump) #the threshold for 10 stumps
setwd('~/python/Theanolearn/facecnn/lstm')
X=read.csv('strat1.csv')
X=read.csv('strat1.csv')
plot(X$sp,type='l',xlab='ticks',ylab='revenue rate',ylim=c(-0.14,0.02))
plot(X$sp,type='l',xlab='ticks',ylab='revenue rate')
lines(X$str,col=2,lty=2)
legend(250,-0.004,c('s$p index','strategy'),col=c(1,2),lty=c(1,2))
256*256*3*32*20000/1024/1024/1024
