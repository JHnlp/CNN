{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 750 Ti (CNMeM is disabled, CuDNN 3007)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import lasagne\n",
    "from lasagne.regularization import regularize_layer_params, l2, l1\n",
    "\n",
    "from lasagne.layers import LocalResponseNormalization2DLayer as LRNLayer\n",
    "from lasagne.layers import GlobalPoolLayer\n",
    "\n",
    "batch_size = 40\n",
    "Conv2DLayer = lasagne.layers.Conv2DLayer\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    face=sklearn.datasets.fetch_olivetti_faces(shuffle=True)\n",
    "    train_set=(face.data[0:200,].reshape((200,1,64,64)),face.target[0:200,].astype(np.int32))\n",
    "    test_set =(face.data[200:400,].reshape((200,1,64,64)),face.target[200:400,].astype(np.int32))\n",
    "    rval = [train_set, test_set]\n",
    "    return rval\n",
    "\n",
    "\n",
    "\n",
    "def inception_module(l_in,pool_filters, num_1x1, reduce_3x3, num_3x3, reduce_5x5, num_5x5, bias=0):\n",
    "    \"\"\"\n",
    "    inception module (without the 3x3s1 pooling and projection because that's difficult in Theano right now)\n",
    "    \"\"\"\n",
    "    out_layers = []\n",
    "\n",
    "    if pool_filters > 0:\n",
    "        l_pool = lasagne.layers.MaxPool2DLayer(l_in, pool_size=3, stride=1, pad=1)\n",
    "        l_pool_reduced = lasagne.layers.NINLayer(l_pool, num_units=pool_filters,nonlinearity = lasagne.nonlinearities.rectify, W=lasagne.init.HeNormal(gain='relu'), b=lasagne.init.Constant(bias))\n",
    "        out_layers.append(l_pool_reduced)\n",
    "\n",
    "    # 1x1\n",
    "    if num_1x1 > 0:\n",
    "        l_1x1 = lasagne.layers.NINLayer(l_in, num_units=num_1x1,nonlinearity = lasagne.nonlinearities.rectify, W=lasagne.init.HeNormal(gain='relu'), b=lasagne.init.Constant(bias))\n",
    "        out_layers.append(l_1x1)\n",
    "    \n",
    "    # 3x3\n",
    "    if num_3x3 > 0:\n",
    "        if reduce_3x3 > 0:\n",
    "            l_reduce_3x3 = lasagne.layers.NINLayer(l_in, num_units=reduce_3x3,nonlinearity = lasagne.nonlinearities.rectify, W=lasagne.init.HeNormal(gain='relu'), b=lasagne.init.Constant(bias))\n",
    "        else:\n",
    "            l_reduce_3x3 = l_in\n",
    "        l_3x3 = Conv2DLayer(l_reduce_3x3, num_filters=num_3x3, filter_size=(3, 3), pad=\"same\",nonlinearity = lasagne.nonlinearities.rectify, W=lasagne.init.HeNormal(gain='relu'), b=lasagne.init.Constant(bias))\n",
    "        out_layers.append(l_3x3)\n",
    "    \n",
    "    # 5x5\n",
    "    if num_5x5 > 0:\n",
    "        if reduce_5x5 > 0:\n",
    "            l_reduce_5x5 = lasagne.layers.NINLayer(l_in, num_units=reduce_5x5, nonlinearity = lasagne.nonlinearities.rectify, W=lasagne.init.HeNormal(gain='relu'), b=lasagne.init.Constant(bias))\n",
    "        else:\n",
    "            l_reduce_5x5 = l_in\n",
    "        l_5x5 = Conv2DLayer(l_reduce_5x5, num_filters=num_5x5, filter_size=(5, 5), pad=\"same\", nonlinearity = lasagne.nonlinearities.rectify, W=lasagne.init.HeNormal(gain='relu'), b=lasagne.init.Constant(bias))\n",
    "        out_layers.append(l_5x5)\n",
    "    \n",
    "    # stack\n",
    "    l_out = lasagne.layers.concat(out_layers)\n",
    "    return l_out\n",
    "\n",
    "\n",
    "\n",
    "def build_cnn(input_var=None):\n",
    "    \n",
    "    network = lasagne.layers.InputLayer(shape=(batch_size, 1, 64, 64),\n",
    "                                        input_var=input_var)\n",
    "   \n",
    "    network = lasagne.layers.NINLayer(network, num_units=32,nonlinearity = lasagne.nonlinearities.rectify, W=lasagne.init.HeNormal(gain='relu'), b=lasagne.init.Constant(0))\n",
    "    \n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=3, stride=2)\n",
    "    \n",
    "    network = inception_module(\n",
    "            network,pool_filters=32, num_1x1=64, reduce_3x3=64, num_3x3=64, reduce_5x5=16, num_5x5=32)\n",
    "    \n",
    "    network = inception_module(\n",
    "            network,pool_filters=32, num_1x1=128, reduce_3x3=96, num_3x3=96, reduce_5x5=32, num_5x5=32)\n",
    "    \n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=3, stride=2)\n",
    "\n",
    "\n",
    "    network = inception_module(\n",
    "            network,pool_filters=32, num_1x1=192, reduce_3x3=96, num_3x3=208, reduce_5x5=16, num_5x5=48)\n",
    "\n",
    "    network = inception_module(\n",
    "            network,pool_filters=32, num_1x1=160, reduce_3x3=112, num_3x3=224, reduce_5x5=24, num_5x5=64)\n",
    "    \n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=3, stride=2)\n",
    "    \n",
    "    network = lasagne.layers.DenseLayer(\n",
    "        lasagne.layers.dropout(network, p=.5),\n",
    "        num_units=256,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    \n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=40,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(num_epochs=200):\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    datasets = load_data()\n",
    "    X_train, y_train = datasets[0]\n",
    "    X_test, y_test = datasets[1]\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "    learnrate=0.02\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "\n",
    "    network = build_cnn(input_var)\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    l1_penalty = regularize_layer_params(network, l1)\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()+0.002*l1_penalty\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    #updates = lasagne.updates.adadelta(loss, params,learning_rate=learnrate)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=learnrate, momentum=0.9)\n",
    "    \n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "    \n",
    "    best_acc = 0\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        if epoch % 30 == 29:\n",
    "            learnrate*= 0.8\n",
    "            #updates = lasagne.updates.adadelta(loss, params,learning_rate=learnrate)\n",
    "            updates = lasagne.updates.nesterov_momentum(\n",
    "                loss, params, learning_rate=learnrate, momentum=0.9)\n",
    "            train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "        for batch in iterate_minibatches(X_train, y_train, batch_size, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        test_err = 0\n",
    "        test_acc = 0\n",
    "        test_batches = 0\n",
    "        for batch in iterate_minibatches(X_test, y_test,batch_size, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            test_err += err\n",
    "            test_acc += acc\n",
    "            test_batches += 1\n",
    "        test_err = test_err / test_batches\n",
    "        test_acc = test_acc / test_batches\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  test loss:\\t\\t{:.6f}\".format(test_err))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            test_acc * 100))\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            np.savez('ORL_inception.npz', *lasagne.layers.get_all_param_values(network))\n",
    "    return best_acc\n",
    "\n",
    "    # Optionally, you could now dump the network weights to a file like this:\n",
    "    #\n",
    "    # And load them again later on like this:\n",
    "    # with np.load('model.npz') as f:\n",
    "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    # lasagne.layers.set_all_param_values(network, param_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 500 took 3.376s\n",
      "  training loss:\t\t7.664221\n",
      "  test loss:\t\t3.726206\n",
      "  validation accuracy:\t\t3.00 %\n",
      "Epoch 2 of 500 took 4.376s\n",
      "  training loss:\t\t5.186757\n",
      "  test loss:\t\t3.710015\n",
      "  validation accuracy:\t\t4.00 %\n",
      "Epoch 3 of 500 took 4.408s\n",
      "  training loss:\t\t5.127435\n",
      "  test loss:\t\t3.706301\n",
      "  validation accuracy:\t\t1.50 %\n",
      "Epoch 4 of 500 took 4.386s\n",
      "  training loss:\t\t5.110086\n",
      "  test loss:\t\t3.720272\n",
      "  validation accuracy:\t\t1.50 %\n",
      "Epoch 5 of 500 took 4.415s\n",
      "  training loss:\t\t5.078242\n",
      "  test loss:\t\t3.724386\n",
      "  validation accuracy:\t\t2.00 %\n",
      "Epoch 6 of 500 took 4.405s\n",
      "  training loss:\t\t5.036441\n",
      "  test loss:\t\t3.728787\n",
      "  validation accuracy:\t\t2.00 %\n",
      "Epoch 7 of 500 took 4.391s\n",
      "  training loss:\t\t5.016393\n",
      "  test loss:\t\t3.728318\n",
      "  validation accuracy:\t\t1.50 %\n",
      "Epoch 8 of 500 took 2.807s\n",
      "  training loss:\t\t4.954418\n",
      "  test loss:\t\t3.733736\n",
      "  validation accuracy:\t\t1.50 %\n",
      "Epoch 9 of 500 took 3.588s\n",
      "  training loss:\t\t4.916486\n",
      "  test loss:\t\t3.745411\n",
      "  validation accuracy:\t\t1.50 %\n",
      "Epoch 10 of 500 took 4.453s\n",
      "  training loss:\t\t4.865363\n",
      "  test loss:\t\t3.765842\n",
      "  validation accuracy:\t\t1.50 %\n",
      "Epoch 11 of 500 took 4.500s\n",
      "  training loss:\t\t4.841190\n",
      "  test loss:\t\t3.763556\n",
      "  validation accuracy:\t\t3.00 %\n",
      "Epoch 12 of 500 took 4.424s\n",
      "  training loss:\t\t4.799616\n",
      "  test loss:\t\t3.753535\n",
      "  validation accuracy:\t\t1.50 %\n",
      "Epoch 13 of 500 took 4.423s\n",
      "  training loss:\t\t4.759449\n",
      "  test loss:\t\t3.753585\n",
      "  validation accuracy:\t\t2.00 %\n",
      "Epoch 14 of 500 took 4.368s\n",
      "  training loss:\t\t4.719068\n",
      "  test loss:\t\t3.751653\n",
      "  validation accuracy:\t\t1.50 %\n",
      "Epoch 15 of 500 took 4.553s\n",
      "  training loss:\t\t4.676161\n",
      "  test loss:\t\t3.741383\n",
      "  validation accuracy:\t\t5.00 %\n",
      "Epoch 16 of 500 took 2.923s\n",
      "  training loss:\t\t4.627246\n",
      "  test loss:\t\t3.746918\n",
      "  validation accuracy:\t\t4.00 %\n",
      "Epoch 17 of 500 took 3.617s\n",
      "  training loss:\t\t4.568578\n",
      "  test loss:\t\t3.733020\n",
      "  validation accuracy:\t\t6.50 %\n",
      "Epoch 18 of 500 took 4.442s\n",
      "  training loss:\t\t4.502808\n",
      "  test loss:\t\t3.724027\n",
      "  validation accuracy:\t\t6.00 %\n",
      "Epoch 19 of 500 took 4.516s\n",
      "  training loss:\t\t4.466318\n",
      "  test loss:\t\t3.681910\n",
      "  validation accuracy:\t\t8.50 %\n",
      "Epoch 20 of 500 took 4.631s\n",
      "  training loss:\t\t4.378913\n",
      "  test loss:\t\t3.638440\n",
      "  validation accuracy:\t\t5.50 %\n",
      "Epoch 21 of 500 took 4.547s\n",
      "  training loss:\t\t4.310310\n",
      "  test loss:\t\t3.698505\n",
      "  validation accuracy:\t\t3.00 %\n",
      "Epoch 22 of 500 took 4.733s\n",
      "  training loss:\t\t4.211504\n",
      "  test loss:\t\t3.553263\n",
      "  validation accuracy:\t\t5.50 %\n",
      "Epoch 23 of 500 took 4.539s\n",
      "  training loss:\t\t4.118444\n",
      "  test loss:\t\t3.313129\n",
      "  validation accuracy:\t\t10.50 %\n",
      "Epoch 24 of 500 took 2.234s\n",
      "  training loss:\t\t4.116612\n",
      "  test loss:\t\t3.422689\n",
      "  validation accuracy:\t\t10.50 %\n",
      "Epoch 25 of 500 took 4.428s\n",
      "  training loss:\t\t3.986882\n",
      "  test loss:\t\t3.288598\n",
      "  validation accuracy:\t\t10.50 %\n",
      "Epoch 26 of 500 took 4.413s\n",
      "  training loss:\t\t3.880329\n",
      "  test loss:\t\t4.069079\n",
      "  validation accuracy:\t\t4.50 %\n",
      "Epoch 27 of 500 took 4.394s\n",
      "  training loss:\t\t4.301347\n",
      "  test loss:\t\t3.450070\n",
      "  validation accuracy:\t\t6.00 %\n",
      "Epoch 28 of 500 took 4.382s\n",
      "  training loss:\t\t4.056209\n",
      "  test loss:\t\t3.357598\n",
      "  validation accuracy:\t\t10.50 %\n",
      "Epoch 29 of 500 took 4.400s\n",
      "  training loss:\t\t3.857176\n",
      "  test loss:\t\t3.186963\n",
      "  validation accuracy:\t\t9.00 %\n",
      "Epoch 30 of 500 took 13.975s\n",
      "  training loss:\t\t3.631626\n",
      "  test loss:\t\t3.091607\n",
      "  validation accuracy:\t\t11.00 %\n",
      "Epoch 31 of 500 took 4.417s\n",
      "  training loss:\t\t3.556434\n",
      "  test loss:\t\t2.917396\n",
      "  validation accuracy:\t\t18.00 %\n",
      "Epoch 32 of 500 took 4.428s\n",
      "  training loss:\t\t3.402828\n",
      "  test loss:\t\t2.777334\n",
      "  validation accuracy:\t\t20.50 %\n",
      "Epoch 33 of 500 took 4.414s\n",
      "  training loss:\t\t3.368648\n",
      "  test loss:\t\t2.732341\n",
      "  validation accuracy:\t\t23.50 %\n",
      "Epoch 34 of 500 took 4.421s\n",
      "  training loss:\t\t3.174258\n",
      "  test loss:\t\t2.442383\n",
      "  validation accuracy:\t\t27.50 %\n",
      "Epoch 35 of 500 took 2.322s\n",
      "  training loss:\t\t3.182295\n",
      "  test loss:\t\t2.407529\n",
      "  validation accuracy:\t\t33.00 %\n",
      "Epoch 36 of 500 took 4.346s\n",
      "  training loss:\t\t3.044098\n",
      "  test loss:\t\t2.442133\n",
      "  validation accuracy:\t\t31.50 %\n",
      "Epoch 37 of 500 took 4.416s\n",
      "  training loss:\t\t3.007753\n",
      "  test loss:\t\t2.345030\n",
      "  validation accuracy:\t\t31.50 %\n",
      "Epoch 38 of 500 took 4.465s\n",
      "  training loss:\t\t2.977727\n",
      "  test loss:\t\t2.401383\n",
      "  validation accuracy:\t\t31.50 %\n",
      "Epoch 39 of 500 took 4.361s\n",
      "  training loss:\t\t2.758439\n",
      "  test loss:\t\t2.089827\n",
      "  validation accuracy:\t\t42.00 %\n",
      "Epoch 40 of 500 took 4.027s\n",
      "  training loss:\t\t2.709697\n",
      "  test loss:\t\t1.983156\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 41 of 500 took 2.202s\n",
      "  training loss:\t\t2.869563\n",
      "  test loss:\t\t2.227930\n",
      "  validation accuracy:\t\t32.00 %\n",
      "Epoch 42 of 500 took 3.485s\n",
      "  training loss:\t\t2.822930\n",
      "  test loss:\t\t2.059398\n",
      "  validation accuracy:\t\t44.00 %\n",
      "Epoch 43 of 500 took 4.373s\n",
      "  training loss:\t\t2.576140\n",
      "  test loss:\t\t1.906277\n",
      "  validation accuracy:\t\t44.50 %\n",
      "Epoch 44 of 500 took 4.422s\n",
      "  training loss:\t\t2.444975\n",
      "  test loss:\t\t1.782854\n",
      "  validation accuracy:\t\t49.50 %\n",
      "Epoch 45 of 500 took 4.678s\n",
      "  training loss:\t\t2.490784\n",
      "  test loss:\t\t1.862020\n",
      "  validation accuracy:\t\t41.00 %\n",
      "Epoch 46 of 500 took 4.481s\n",
      "  training loss:\t\t2.472495\n",
      "  test loss:\t\t1.758517\n",
      "  validation accuracy:\t\t53.00 %\n",
      "Epoch 47 of 500 took 4.416s\n",
      "  training loss:\t\t2.349455\n",
      "  test loss:\t\t1.840394\n",
      "  validation accuracy:\t\t46.50 %\n",
      "Epoch 48 of 500 took 4.253s\n",
      "  training loss:\t\t2.193844\n",
      "  test loss:\t\t1.574302\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 49 of 500 took 2.773s\n",
      "  training loss:\t\t1.979982\n",
      "  test loss:\t\t1.439132\n",
      "  validation accuracy:\t\t59.00 %\n",
      "Epoch 50 of 500 took 4.399s\n",
      "  training loss:\t\t2.031510\n",
      "  test loss:\t\t1.336548\n",
      "  validation accuracy:\t\t61.50 %\n",
      "Epoch 51 of 500 took 4.528s\n",
      "  training loss:\t\t1.896210\n",
      "  test loss:\t\t1.339260\n",
      "  validation accuracy:\t\t63.00 %\n",
      "Epoch 52 of 500 took 4.699s\n",
      "  training loss:\t\t1.897651\n",
      "  test loss:\t\t1.338729\n",
      "  validation accuracy:\t\t63.00 %\n",
      "Epoch 53 of 500 took 4.414s\n",
      "  training loss:\t\t1.802736\n",
      "  test loss:\t\t1.269813\n",
      "  validation accuracy:\t\t63.00 %\n",
      "Epoch 54 of 500 took 4.464s\n",
      "  training loss:\t\t1.789646\n",
      "  test loss:\t\t1.247449\n",
      "  validation accuracy:\t\t63.50 %\n",
      "Epoch 55 of 500 took 4.732s\n",
      "  training loss:\t\t1.817678\n",
      "  test loss:\t\t1.436893\n",
      "  validation accuracy:\t\t54.00 %\n",
      "Epoch 56 of 500 took 2.994s\n",
      "  training loss:\t\t1.825497\n",
      "  test loss:\t\t1.412221\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 57 of 500 took 3.575s\n",
      "  training loss:\t\t1.852437\n",
      "  test loss:\t\t1.475026\n",
      "  validation accuracy:\t\t56.50 %\n",
      "Epoch 58 of 500 took 4.408s\n",
      "  training loss:\t\t1.784337\n",
      "  test loss:\t\t1.152878\n",
      "  validation accuracy:\t\t70.50 %\n",
      "Epoch 59 of 500 took 4.414s\n",
      "  training loss:\t\t1.717788\n",
      "  test loss:\t\t1.091442\n",
      "  validation accuracy:\t\t66.50 %\n",
      "Epoch 60 of 500 took 12.224s\n",
      "  training loss:\t\t1.587601\n",
      "  test loss:\t\t1.178292\n",
      "  validation accuracy:\t\t67.00 %\n",
      "Epoch 61 of 500 took 4.272s\n",
      "  training loss:\t\t1.611634\n",
      "  test loss:\t\t1.298847\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 62 of 500 took 4.491s\n",
      "  training loss:\t\t1.464861\n",
      "  test loss:\t\t1.015631\n",
      "  validation accuracy:\t\t71.00 %\n",
      "Epoch 63 of 500 took 4.399s\n",
      "  training loss:\t\t1.457003\n",
      "  test loss:\t\t0.999446\n",
      "  validation accuracy:\t\t72.00 %\n",
      "Epoch 64 of 500 took 4.503s\n",
      "  training loss:\t\t1.361201\n",
      "  test loss:\t\t0.888040\n",
      "  validation accuracy:\t\t74.50 %\n",
      "Epoch 65 of 500 took 4.526s\n",
      "  training loss:\t\t1.340641\n",
      "  test loss:\t\t0.933033\n",
      "  validation accuracy:\t\t72.50 %\n",
      "Epoch 66 of 500 took 4.747s\n",
      "  training loss:\t\t1.264770\n",
      "  test loss:\t\t0.920642\n",
      "  validation accuracy:\t\t75.50 %\n",
      "Epoch 67 of 500 took 3.776s\n",
      "  training loss:\t\t1.157698\n",
      "  test loss:\t\t0.966717\n",
      "  validation accuracy:\t\t75.50 %\n",
      "Epoch 68 of 500 took 2.895s\n",
      "  training loss:\t\t1.359775\n",
      "  test loss:\t\t1.038535\n",
      "  validation accuracy:\t\t72.50 %\n",
      "Epoch 69 of 500 took 4.436s\n",
      "  training loss:\t\t1.207071\n",
      "  test loss:\t\t0.869029\n",
      "  validation accuracy:\t\t77.00 %\n",
      "Epoch 70 of 500 took 4.410s\n",
      "  training loss:\t\t1.253672\n",
      "  test loss:\t\t0.899000\n",
      "  validation accuracy:\t\t75.50 %\n",
      "Epoch 71 of 500 took 4.656s\n",
      "  training loss:\t\t1.364115\n",
      "  test loss:\t\t0.954327\n",
      "  validation accuracy:\t\t75.00 %\n",
      "Epoch 72 of 500 took 4.519s\n",
      "  training loss:\t\t1.145117\n",
      "  test loss:\t\t0.818555\n",
      "  validation accuracy:\t\t80.00 %\n",
      "Epoch 73 of 500 took 4.560s\n",
      "  training loss:\t\t1.299638\n",
      "  test loss:\t\t0.782464\n",
      "  validation accuracy:\t\t77.50 %\n",
      "Epoch 74 of 500 took 4.639s\n",
      "  training loss:\t\t1.122296\n",
      "  test loss:\t\t0.796422\n",
      "  validation accuracy:\t\t80.50 %\n",
      "Epoch 75 of 500 took 2.997s\n",
      "  training loss:\t\t1.260532\n",
      "  test loss:\t\t0.826433\n",
      "  validation accuracy:\t\t80.00 %\n",
      "Epoch 76 of 500 took 3.562s\n",
      "  training loss:\t\t1.255085\n",
      "  test loss:\t\t0.770853\n",
      "  validation accuracy:\t\t82.00 %\n",
      "Epoch 77 of 500 took 4.479s\n",
      "  training loss:\t\t1.080113\n",
      "  test loss:\t\t0.805636\n",
      "  validation accuracy:\t\t77.00 %\n",
      "Epoch 78 of 500 took 4.479s\n",
      "  training loss:\t\t1.060381\n",
      "  test loss:\t\t0.808718\n",
      "  validation accuracy:\t\t77.00 %\n",
      "Epoch 79 of 500 took 4.419s\n",
      "  training loss:\t\t1.076023\n",
      "  test loss:\t\t0.721031\n",
      "  validation accuracy:\t\t84.00 %\n",
      "Epoch 80 of 500 took 4.440s\n",
      "  training loss:\t\t1.049924\n",
      "  test loss:\t\t0.695610\n",
      "  validation accuracy:\t\t83.50 %\n",
      "Epoch 81 of 500 took 4.444s\n",
      "  training loss:\t\t0.954463\n",
      "  test loss:\t\t0.815663\n",
      "  validation accuracy:\t\t80.00 %\n",
      "Epoch 82 of 500 took 4.410s\n",
      "  training loss:\t\t1.104815\n",
      "  test loss:\t\t0.876111\n",
      "  validation accuracy:\t\t78.50 %\n",
      "Epoch 83 of 500 took 2.662s\n",
      "  training loss:\t\t0.968928\n",
      "  test loss:\t\t0.779676\n",
      "  validation accuracy:\t\t84.00 %\n",
      "Epoch 84 of 500 took 3.774s\n",
      "  training loss:\t\t1.041537\n",
      "  test loss:\t\t0.687782\n",
      "  validation accuracy:\t\t82.00 %\n",
      "Epoch 85 of 500 took 4.424s\n",
      "  training loss:\t\t0.929541\n",
      "  test loss:\t\t0.732546\n",
      "  validation accuracy:\t\t85.50 %\n",
      "Epoch 86 of 500 took 4.397s\n",
      "  training loss:\t\t0.904695\n",
      "  test loss:\t\t0.688261\n",
      "  validation accuracy:\t\t86.00 %\n",
      "Epoch 87 of 500 took 4.418s\n",
      "  training loss:\t\t0.871827\n",
      "  test loss:\t\t0.758916\n",
      "  validation accuracy:\t\t84.00 %\n",
      "Epoch 88 of 500 took 4.452s\n",
      "  training loss:\t\t0.848445\n",
      "  test loss:\t\t0.748695\n",
      "  validation accuracy:\t\t85.00 %\n",
      "Epoch 89 of 500 took 4.468s\n",
      "  training loss:\t\t0.871750\n",
      "  test loss:\t\t0.707396\n",
      "  validation accuracy:\t\t84.00 %\n",
      "Epoch 90 of 500 took 15.147s\n",
      "  training loss:\t\t0.882259\n",
      "  test loss:\t\t0.763706\n",
      "  validation accuracy:\t\t82.50 %\n",
      "Epoch 91 of 500 took 4.582s\n",
      "  training loss:\t\t0.805722\n",
      "  test loss:\t\t0.901820\n",
      "  validation accuracy:\t\t85.00 %\n",
      "Epoch 92 of 500 took 4.476s\n",
      "  training loss:\t\t0.740239\n",
      "  test loss:\t\t0.754587\n",
      "  validation accuracy:\t\t86.50 %\n",
      "Epoch 93 of 500 took 4.482s\n",
      "  training loss:\t\t0.903101\n",
      "  test loss:\t\t0.668549\n",
      "  validation accuracy:\t\t86.00 %\n",
      "Epoch 94 of 500 took 3.029s\n",
      "  training loss:\t\t0.794250\n",
      "  test loss:\t\t0.633220\n",
      "  validation accuracy:\t\t86.50 %\n",
      "Epoch 95 of 500 took 3.432s\n",
      "  training loss:\t\t0.907059\n",
      "  test loss:\t\t0.893653\n",
      "  validation accuracy:\t\t79.50 %\n",
      "Epoch 96 of 500 took 4.521s\n",
      "  training loss:\t\t0.774314\n",
      "  test loss:\t\t0.748008\n",
      "  validation accuracy:\t\t82.00 %\n",
      "Epoch 97 of 500 took 4.463s\n",
      "  training loss:\t\t0.793512\n",
      "  test loss:\t\t0.652883\n",
      "  validation accuracy:\t\t85.50 %\n",
      "Epoch 98 of 500 took 4.445s\n",
      "  training loss:\t\t0.722171\n",
      "  test loss:\t\t0.627340\n",
      "  validation accuracy:\t\t87.00 %\n",
      "Epoch 99 of 500 took 4.451s\n",
      "  training loss:\t\t0.674641\n",
      "  test loss:\t\t0.604612\n",
      "  validation accuracy:\t\t85.00 %\n",
      "Epoch 100 of 500 took 4.517s\n",
      "  training loss:\t\t0.708404\n",
      "  test loss:\t\t0.696580\n",
      "  validation accuracy:\t\t84.50 %\n",
      "Epoch 101 of 500 took 4.458s\n",
      "  training loss:\t\t0.754281\n",
      "  test loss:\t\t0.792656\n",
      "  validation accuracy:\t\t82.00 %\n",
      "Epoch 102 of 500 took 3.284s\n",
      "  training loss:\t\t0.759173\n",
      "  test loss:\t\t0.629326\n",
      "  validation accuracy:\t\t85.00 %\n",
      "Epoch 103 of 500 took 3.376s\n",
      "  training loss:\t\t0.762187\n",
      "  test loss:\t\t0.604958\n",
      "  validation accuracy:\t\t87.50 %\n",
      "Epoch 104 of 500 took 4.395s\n",
      "  training loss:\t\t0.870725\n",
      "  test loss:\t\t0.622738\n",
      "  validation accuracy:\t\t86.00 %\n",
      "Epoch 105 of 500 took 4.392s\n",
      "  training loss:\t\t0.771771\n",
      "  test loss:\t\t0.705765\n",
      "  validation accuracy:\t\t83.00 %\n",
      "Epoch 106 of 500 took 4.397s\n",
      "  training loss:\t\t0.815018\n",
      "  test loss:\t\t0.682454\n",
      "  validation accuracy:\t\t82.50 %\n",
      "Epoch 107 of 500 took 4.521s\n",
      "  training loss:\t\t0.731180\n",
      "  test loss:\t\t0.684264\n",
      "  validation accuracy:\t\t84.50 %\n",
      "Epoch 108 of 500 took 4.522s\n",
      "  training loss:\t\t0.694395\n",
      "  test loss:\t\t0.659670\n",
      "  validation accuracy:\t\t83.00 %\n",
      "Epoch 109 of 500 took 4.398s\n",
      "  training loss:\t\t0.734611\n",
      "  test loss:\t\t0.698147\n",
      "  validation accuracy:\t\t84.00 %\n",
      "Epoch 110 of 500 took 3.402s\n",
      "  training loss:\t\t0.692180\n",
      "  test loss:\t\t0.735055\n",
      "  validation accuracy:\t\t85.00 %\n",
      "Epoch 111 of 500 took 3.078s\n",
      "  training loss:\t\t0.706198\n",
      "  test loss:\t\t0.699838\n",
      "  validation accuracy:\t\t85.50 %\n",
      "Epoch 112 of 500 took 4.546s\n",
      "  training loss:\t\t0.605504\n",
      "  test loss:\t\t0.750670\n",
      "  validation accuracy:\t\t84.50 %\n",
      "Epoch 113 of 500 took 4.472s\n",
      "  training loss:\t\t0.751276\n",
      "  test loss:\t\t0.678827\n",
      "  validation accuracy:\t\t84.50 %\n",
      "Epoch 114 of 500 took 4.493s\n",
      "  training loss:\t\t0.748554\n",
      "  test loss:\t\t0.639081\n",
      "  validation accuracy:\t\t87.50 %\n",
      "Epoch 115 of 500 took 4.453s\n",
      "  training loss:\t\t0.682551\n",
      "  test loss:\t\t0.691320\n",
      "  validation accuracy:\t\t86.50 %\n",
      "Epoch 116 of 500 took 4.555s\n",
      "  training loss:\t\t0.733157\n",
      "  test loss:\t\t0.612371\n",
      "  validation accuracy:\t\t87.00 %\n",
      "Epoch 117 of 500 took 4.592s\n",
      "  training loss:\t\t0.849783\n",
      "  test loss:\t\t0.686328\n",
      "  validation accuracy:\t\t85.00 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7c9e589ebb04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-4541814456ce>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(num_epochs)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    909\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 911\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    912\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
