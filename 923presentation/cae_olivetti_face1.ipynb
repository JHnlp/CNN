{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 750 Ti (CNMeM is disabled, cuDNN Version is too old. Update to v5, was 3007.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda_convnet (faster)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, urllib, gzip\n",
    "sys.path.append('/home/rui/pylearn2')\n",
    "from __future__ import print_function\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "import numpy as np\n",
    "from lasagne.layers import get_output, InputLayer, DenseLayer, Upscale2DLayer, ReshapeLayer\n",
    "from lasagne.nonlinearities import rectify, leaky_rectify, tanh\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from lasagne.objectives import categorical_crossentropy\n",
    "import pylearn2\n",
    "from lasagne.layers.cuda_convnet import Conv2DCCLayer as Conv2DLayerFast\n",
    "from lasagne.regularization import regularize_layer_params, l2, l1\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import lasagne\n",
    "from lasagne.layers import Conv2DLayer as Conv2DLayerSlow\n",
    "from lasagne.layers import MaxPool2DLayer as MaxPool2DLayerSlow\n",
    "try:\n",
    "    from lasagne.layers.cuda_convnet import Conv2DCCLayer as Conv2DLayerFast\n",
    "    from lasagne.layers.cuda_convnet import MaxPool2DCCLayer as MaxPool2DLayerFast\n",
    "    print('Using cuda_convnet (faster)')\n",
    "except ImportError:\n",
    "    from lasagne.layers import Conv2DLayer as Conv2DLayerFast\n",
    "    from lasagne.layers import MaxPool2DLayer as MaxPool2DLayerFast\n",
    "    print('Using lasagne.layers (slower)')\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # Change matplotlib backend, in case we have no X server running..\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from IPython.display import Image as IPImage\n",
    "from PIL import Image\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "batch_size = 40\n",
    "Conv2DLayer = lasagne.layers.Conv2DLayer\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    face=sklearn.datasets.fetch_olivetti_faces(shuffle=True)\n",
    "    train_set=(face.data[0:200,].reshape((200,1,64,64)),face.target[0:200,].astype(np.int32))\n",
    "    test_set =(face.data[200:400,].reshape((200,1,64,64)),face.target[200:400,].astype(np.int32))\n",
    "    rval = [train_set, test_set]\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set, test_set = load_data()\n",
    "train_face = train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_picture_array(X, rescale=2):\n",
    "    array = X.reshape(64,64)\n",
    "    array = np.clip(array, a_min = 0, a_max = 255)\n",
    "    return  array.repeat(rescale, axis = 0).repeat(rescale, axis = 1).astype(np.uint8())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAOtklEQVR4nM3by3Jc2ZUe4O/cT14I\nFAmQ1VUqS3K7utWWK7oHtsNhe+SJI/x2fhSHPfILeGKHPWgprI7qlqpKLRWrSJDEJTPPfXtwFiC/\nATMZQICIPAD2v/da61//v3b2r1x5pfGfbORKhVJl1MolGTIpPgpJMhuMZhRms4SkUsgVZrlMZlTI\nZCYFklKmUChs/S//2R8Ndia5j/wqC5d2XtnK1AqVTK4ySzKZHJk8MHj8mkxmUSgsWORKmUQg0JjN\nckU8s36eVSa/9O/8V7VOcwYIvPDKvU//v90jIcNskSviNPzpb82U8Y5FbjEqZcjjPDAq4ulBLTfF\nzysx2/sP/qejTDoDBJ7LbHxmjtO6WBRSnPgFYv2ZLE7GIsVqi/h+BjKLBaUkKS0Kx6cn/3R+Jn/t\nX/iNFcGP/Cq3Otf2RoVRZVBKxqc1rzgUsfZ15wuZkTjhhRyLRTKb5EZT7DqLo0ahMMoMko1Z4d/7\nrS/85gwQqHS+fIr0ZNGZHdS2cpMlMlytlMkUWHdzkSSQjNaYGRzMklmmMEVENFqVrUzhQaV28jf+\nm81Z5IHkwgujSSszePAehUFSyt1LFrlrW8leK3vKerMx4uEBvQd3Do46k1Ipl6nNWjuZZz5TqZAk\nn/nSN56dAQL81MY7G4XJje/NNmqDo1udXm622NrLPfOZra2dzGCxmMxO7i06t06Obg1mmUGmtFPa\nuLPx4MGnXsZvyv3SD/IzQGDrz2KXRv/oO4VarsNW79Yo6cxObrUOJq/ktpEDi4j/wUnvzp3RYNFJ\nFrVSa6tV29q6cOPGV64U7vzCf48c8nER+LlW0qj86B+99EyhcIHawY0Pjh50FrlWZfFB5YUq4rwy\nYDBG9lyZQquxtXPpExulyk6LS7/ztVZj8BPP/d0ZIPC5Wmly77XMJ/5MYXpihMle5+DopNbIVRq1\nteIxy+y19l5485QHC5deurSxU0rx3B65xTe+9lcqlS/9lzNA4NLJova9rVcuiTy31m2R90rJvU5t\nq9X6EwtcsLVX+qMbH/RSIFQiVxglpb3a7OS53NF7lc5XZ8GIFo3aQemV2mA0gaT3ztHJEP8mozH6\ngK1WLn/iBYsPfu+NQSbp/GjwwtaDrUKvtdc4GfVaO73Oxj/35Rkg0BgVCpc2Mgez2WySWdw6miwe\nIrcnvZNc42CvUAaDZPbWj7rI7ZlM7xRo5Z4ZvVUqI3dstQajxr85AwQKucqFSWnC0VGmNHvrtW8t\nXvnW5y4kk87iQtIZVBbMwRmPeGF0iNjpvXXwQqPwpc995t7BS0lvtJE7Gn11BggklVwf7H72EGuY\n/B+f6vzBSz/6udJt8MRCrbIYIsfNJkXwqlu3TkYXFg9+8MaVH5289tpXTh5sHCWLxuzevz4DBLLo\ng1ZW3Lv33nvXfuHeH136tW/9zOc+8a1K7tpOpTFKMo1cZ1D4wnu/lynlGjuVyei1D05e+Jmv/eAv\nbOxlOi3o/eQMEFjXPksGncFRZ+vShX/pG4uvPHdpL3epQulCq4hesjJhtNj4C5NF6WhQa11pfarz\nuU995rm3TiZJa7RI0St9fAQqmUxpjL//U88VtiaX/qPfSiq1waxVWOxcKsyolZJCZQYv/JXkD6jM\nZrtArrKX+Zm/NKqi4x61yHRngMDKUiprD/fMpUWuklu8MOj1Jsmi0tq40ljMingPRfSCs2t/Y+O1\ngx6zwqK1VSvUdjK50UEVikM6CwQemWylsngms8hUOpPcXmuyRFV7bq8IhaAOjSgFL2Axu/DXrr12\nRG8O5TEz6VUas0wtBaOsDGeAAEtU6dKitjLBQhvVfA7dp9JqLFYVrFQrMIWOkkcPndR+4tJ7nRGM\nSrlKFz9/MmmU8evPQSmdiG6/Mj/poT3BdjKZSqmWmz1qQRShBK664qqbrNjQurQ1GomYylRgURo1\n8tBYzkElm+NcZ4qnCBCq2KPKW8apZTGYo3au/1+VokxhNj3VVTK1ShVdQVJHf1FqlE8a3DlEwaR5\n0j2zqHCJ2N3sSfOFWWeUFEoiIsZQE1eddJTL9aaoD7SR9ar4WPXzPNTmczgDoxT5bD0B65qzJ/W/\niN2HyaSUP+nEqyK6VcqVRrkkObgxR4VINoSrUIbyvuqq2ZPP8JFf5RjRnELxXzli9rTuJTIho04V\nET8qtUZHk0yyC46wUbr3vdlWaTJHpV1z5eP6V41x8RhlH/VVJkNwvSW+9ajzLvH1ZJJCK9vKDSaT\nUWZ2MDgGn7yXK2yN3psNoalPtkjBIkqVxSwFqz4HlWyKmH30AVJk90yymCwR++vfXEfv9OgQHvXW\nOLr3xuDaT30ITS0p9EaD/CkfVh7dyKMqWNVHfpWzyRyxv579R/UrGQ1Gh1BBZoPFB984euaVJHOv\nt8gMbvytWz+1eCdJBrNcFbmj0dsa1U/q2/tQJD4+AmtGnyP6l8hyLEaD3uBolIcmdHLrRq226CVH\nD3IbRzfuTSYPFpXBbIiKz62d1qwNPlxaJJP5LNTyR18ntzoZc1T6OdZ/itORq1UGF36mcknwgnuV\nnVHtn6lcee5DaEdrdUw4oDTLHYieurUYlGeAwBgcb44anwKLVSubTMH7y3BCd0pj+MWVndykVNh5\npnWhcVRJpmASjw5kozYZHeWB3WLWnwECmWQiWMG6/hmz2SIpCedwzZCPXuFg1pjUpuDQq++2KO0U\nDhEDZWhPg0xldlJYVLH7d2eAQB6TDuue1LJgumu1TrH64gmhxb1epZObPUR9mCQnlY1Z5TK4wOqo\nlqGqr2td5HEuCqM3Z4DA8rQyEQ+PTGWdkVnVjDUqSoOj3hQV8ORosVGprQ5ip9TYSFEjVjfpmVqu\n1UZ/mCK3cHMGCNxFrzIpzYQSAF34GrNRHmr55Fav1CHX6cJnXHW25GSxU1rMaGzMbgyeo1ZpTYo4\nY0lxFp3R9zahWNVPfc46CfVWbwxlN1MZdKF3ZBqjSk945IshpgW6yA+r69z6xOCNFz4JDTWXhRad\nqT2cAQJvfWG29sitJfrDQu+djVvrzFjSKwm/tFF7nLTa2UtmpUyltubIRRecYOPaD9661sgiC05q\nvczkwxkgcPTIiXpVsNZF7t57uUEv16iNcqXGqNZoMNjGE72OYJR18IDZjdatnStbndE26mATelFu\nsD0DBJ57cI3MaLGVher5YMbW0aDQqPRmY+hafXiJKZzG1Ws8qlWSo3feeONa8sGlrTb0xEmhe8o4\nJxdngMDn3vmpWa6JSrjWq50XKuQOfjT6J5EnckwWoylQ6o0ePeZKafTGO70iGEAv94kqampm7UNX\n3Xg8AwQyf/DLmAdc/cwKi08cHELjO+q8cSHXhz6w8ofRgz5yYaXS2KiMTuEqrepKr5BHthUZozQr\nnHxzBgj8D2/dPmW22RCdfOXCwaiw95m9jdVXy6M2rPpoZbDRaOQKlUal8Fl0FAcniz6eWUyhIi0K\ng9oP/v4MEPiVjR9dySw6j7w3t2i0Oqvy+8o2nJP0pCmt2XNWqVRSMOfMzqXRD9Y5stzomZV5jnKN\nWdJJPvi1d2eAwOzoG19qYiqoetLwSpvIhlf2MTlyCja4sr3HfHYMDlkFH1w1waSyjfcko8aod0Vo\nRb/yD2fRHe9878aNF7ZW72DGSaEIhW9no3ja48Go0xvjlI+hBrY2LjSSPnTUVtKaots4hj7W2ZsU\nbv1f41moZLnk4AEbW51WFzUgRXe/dohLaKFVzIwc3bkPLjxptbbRXY1ag1mtV+vlcoPWGNrhZND6\nxtcGmzNAoFJ77Qu/8tw+uNwLR5PCg8FGq5SZnzROcbsAGplBZauRBxvMlHq5xckmZm9qo9mDa8mt\njc6vHWRnEQUnV36wMfqd1l6SudXJtQq9xlr71mq2zkkdDXE+ELPYg15ljvsJuUynMtkZNJEFBqPa\nbPS3RmzOwi+Y7XSOvvAb10qzZAoem6niNDzeEXhkTFns9eqJnMJRrJ9mUdY8ufEQOmznECrBqPPg\nVw7+dPfio77K3CT3nZ/7zu+8NAdbm5307p08c7BRWdQyhY119qGWOz05ImXMDOaK8AWPNnYOMgeZ\n3iQzO7mReaePqvvxEagNWr/1Cy/9xj/4XKnwzsbsgHXavjcobeTm6I3LOOu5pFQo1DZKS1TUBe/s\nohK2Sne2Fm/duJC5f3IUPzYCpdIHvzfY+nNfa12hVzk5WRQqhaNkb7ZE/doaHLD3YNUWGpXk8WZR\n0mscPEihuK71o/CtF0bvCR768RFIMs/dOqp9avS//VuZUedWa7DDrcWVZFB7vGFSu7CoPd4kqmOi\nYFahk+stUf8y3Bok3xn9pbdOGpPsLDyjpNAY3MhsvXLj723xEFUud1LKHIPtZJEHV1d4lKs8k0cP\nlcV0ff/kR4/hkry3eNB56blvzSqz9ixqQWWw98HiFWb/1He+daWRzBHZ6y250dGiUVicIr7nyI6r\noj4FDqOj2qhXOqiNTnoPei8VZr3GoDVzBgiss2T01jmvredeOxpC79xikKucYtcG652KzGi2Uemd\ntE9TMw0WtyqjSu4OD3pJq1IrdZIWxVnMEa2TEIU7nR/sla69dNCpQ/Nfle6DxYXe3qSTnCSZQf+U\nDRar19qE1nZnkTTe2Ot8cO25Fy5k7mKiYjqLPLB6XqNJZqMwG7100qkNkjom5nNFOCWdZDAoFWZH\ngwtbs85sMDvZO1kn6Fonb5Vyu7hps97GS5rQIz4+Amvns/OgcaXXKzWu/VGvlbmz6L2SLO5cxTzA\n7Iit5GAKNjzHe/fuzRYbnTdmnVuTT1Ra37tUSkq95kz6glUTyxwMOrOj9b5pY6d3Arc2cRdvozSH\n63My2isd3ctlWplZ6V7j6EKjcu/OldLGYjK51uqtt3RH1VnohLNKLnNU6+SulU5Ki9HWLOlNjhbJ\n1oOdJqrd4oOTEYdwFNY5hEkyGd25R4oseqX0UmGKSdKkPpM8UOltLY6S0oX1rsRzd26wuFA46fXG\ncNZKyVEhd3SjkftgY6O31RvNdiZv7Qw6z+x0roJrF9iHbzafR1/w/wAYBnpNLM6b7gAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic_array = get_picture_array(train_face[5]*255)\n",
    "image = Image.fromarray(pic_array)\n",
    "image.save('temp.png', format=\"PNG\")  \n",
    "IPImage('temp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_face_out = train_face.reshape((train_face.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_num_filters = 16\n",
    "filter_size = 5\n",
    "pool_size = 2\n",
    "encode_size = 48\n",
    "dense_mid_size = 128\n",
    "pad_in = 'valid'    \n",
    "pad_out = 'full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn(input_var=None):\n",
    "    \n",
    "    network = InputLayer(shape=(None,  train_face.shape[1], train_face.shape[2], train_face.shape[3]),input_var=input_var)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "\n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "\n",
    "    network = MaxPool2DLayerFast(network, pool_size=pool_size)\n",
    "\n",
    "    network = Conv2DLayerFast(network, num_filters=2*conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=2*conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "\n",
    "    network = MaxPool2DLayerFast(network, pool_size=pool_size)\n",
    "    \n",
    "    network = ReshapeLayer(network, shape =(([0], -1)))\n",
    "    \n",
    "    network = DenseLayer(network, num_units= dense_mid_size, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    encode_layer = DenseLayer(network, name= 'encode', num_units= encode_size, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    action_layer = DenseLayer(encode_layer, name= 'action', num_units= encode_size, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                            nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    network = DenseLayer(action_layer, num_units= 3200, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    network = ReshapeLayer(network, shape =(([0], 2*conv_num_filters, 10, 10)))\n",
    "    \n",
    "    network = Upscale2DLayer(network, scale_factor = pool_size)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=2 * conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_out)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_out)\n",
    "\n",
    "    network = Upscale2DLayer(network, scale_factor = pool_size)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_out)\n",
    "\n",
    "    network = Conv2DLayerSlow(network, num_filters=1, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.sigmoid, filter_size=filter_size, pad=pad_out)\n",
    "    \n",
    "    network = ReshapeLayer(network, shape =(([0], -1)))\n",
    "\n",
    "    return network\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.matrix('targets')\n",
    "# Create neural network model (depending on first command line parameter)\n",
    "network = build_cnn(input_var)\n",
    "with np.load('CAE_face.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "lasagne.layers.set_all_param_values(network, param_values)\n",
    "reconstructed = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.squared_error(reconstructed, target_var)\n",
    "loss = loss.mean()\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 1000 took 0.730s\n",
      "  training loss:\t\t0.032523\n",
      "Epoch 2 of 1000 took 0.746s\n",
      "  training loss:\t\t0.030011\n",
      "Epoch 3 of 1000 took 0.760s\n",
      "  training loss:\t\t0.031232\n",
      "Epoch 4 of 1000 took 0.753s\n",
      "  training loss:\t\t0.031644\n",
      "Epoch 5 of 1000 took 0.734s\n",
      "  training loss:\t\t0.029590\n",
      "Epoch 6 of 1000 took 0.758s\n",
      "  training loss:\t\t0.028783\n",
      "Epoch 7 of 1000 took 0.736s\n",
      "  training loss:\t\t0.029179\n",
      "Epoch 8 of 1000 took 0.759s\n",
      "  training loss:\t\t0.028153\n",
      "Epoch 9 of 1000 took 0.732s\n",
      "  training loss:\t\t0.030876\n",
      "Epoch 10 of 1000 took 0.755s\n",
      "  training loss:\t\t0.028307\n",
      "Epoch 11 of 1000 took 0.733s\n",
      "  training loss:\t\t0.025603\n",
      "Epoch 12 of 1000 took 0.733s\n",
      "  training loss:\t\t0.026766\n",
      "Epoch 13 of 1000 took 0.740s\n",
      "  training loss:\t\t0.040464\n",
      "Epoch 14 of 1000 took 0.705s\n",
      "  training loss:\t\t0.033592\n",
      "Epoch 15 of 1000 took 0.704s\n",
      "  training loss:\t\t0.027857\n",
      "Epoch 16 of 1000 took 0.756s\n",
      "  training loss:\t\t0.025508\n",
      "Epoch 17 of 1000 took 0.704s\n",
      "  training loss:\t\t0.033551\n",
      "Epoch 18 of 1000 took 0.704s\n",
      "  training loss:\t\t0.032195\n",
      "Epoch 19 of 1000 took 0.702s\n",
      "  training loss:\t\t0.029634\n",
      "Epoch 20 of 1000 took 0.740s\n",
      "  training loss:\t\t0.028366\n",
      "Epoch 21 of 1000 took 0.706s\n",
      "  training loss:\t\t0.027768\n",
      "Epoch 22 of 1000 took 0.701s\n",
      "  training loss:\t\t0.029762\n",
      "Epoch 23 of 1000 took 0.701s\n",
      "  training loss:\t\t0.030804\n",
      "Epoch 24 of 1000 took 0.704s\n",
      "  training loss:\t\t0.028172\n",
      "Epoch 25 of 1000 took 0.707s\n",
      "  training loss:\t\t0.027037\n",
      "Epoch 26 of 1000 took 0.743s\n",
      "  training loss:\t\t0.028673\n",
      "Epoch 27 of 1000 took 0.705s\n",
      "  training loss:\t\t0.026246\n",
      "Epoch 28 of 1000 took 0.704s\n",
      "  training loss:\t\t0.038480\n",
      "Epoch 29 of 1000 took 0.701s\n",
      "  training loss:\t\t0.030231\n",
      "Epoch 30 of 1000 took 0.745s\n",
      "  training loss:\t\t0.030648\n",
      "Epoch 31 of 1000 took 0.726s\n",
      "  training loss:\t\t0.027171\n",
      "Epoch 32 of 1000 took 0.706s\n",
      "  training loss:\t\t0.026333\n",
      "Epoch 33 of 1000 took 0.704s\n",
      "  training loss:\t\t0.025928\n",
      "Epoch 34 of 1000 took 0.704s\n",
      "  training loss:\t\t0.024495\n",
      "Epoch 35 of 1000 took 0.703s\n",
      "  training loss:\t\t0.024061\n",
      "Epoch 36 of 1000 took 0.737s\n",
      "  training loss:\t\t0.023870\n",
      "Epoch 37 of 1000 took 0.705s\n",
      "  training loss:\t\t0.024225\n",
      "Epoch 38 of 1000 took 0.702s\n",
      "  training loss:\t\t0.025899\n",
      "Epoch 39 of 1000 took 0.744s\n",
      "  training loss:\t\t0.025831\n",
      "Epoch 40 of 1000 took 0.703s\n",
      "  training loss:\t\t0.028083\n",
      "Epoch 41 of 1000 took 0.705s\n",
      "  training loss:\t\t0.029698\n",
      "Epoch 42 of 1000 took 0.705s\n",
      "  training loss:\t\t0.026406\n",
      "Epoch 43 of 1000 took 0.744s\n",
      "  training loss:\t\t0.025207\n",
      "Epoch 44 of 1000 took 0.702s\n",
      "  training loss:\t\t0.028778\n",
      "Epoch 45 of 1000 took 0.704s\n",
      "  training loss:\t\t0.030050\n",
      "Epoch 46 of 1000 took 0.702s\n",
      "  training loss:\t\t0.038479\n",
      "Epoch 47 of 1000 took 0.732s\n",
      "  training loss:\t\t0.030936\n",
      "Epoch 48 of 1000 took 0.704s\n",
      "  training loss:\t\t0.026327\n",
      "Epoch 49 of 1000 took 0.704s\n",
      "  training loss:\t\t0.026716\n",
      "Epoch 50 of 1000 took 0.703s\n",
      "  training loss:\t\t0.028278\n",
      "Epoch 51 of 1000 took 0.703s\n",
      "  training loss:\t\t0.031524\n",
      "Epoch 52 of 1000 took 0.774s\n",
      "  training loss:\t\t0.026194\n",
      "Epoch 53 of 1000 took 0.767s\n",
      "  training loss:\t\t0.023947\n",
      "Epoch 54 of 1000 took 0.740s\n",
      "  training loss:\t\t0.023280\n",
      "Epoch 55 of 1000 took 0.704s\n",
      "  training loss:\t\t0.022956\n",
      "Epoch 56 of 1000 took 0.704s\n",
      "  training loss:\t\t0.022733\n",
      "Epoch 57 of 1000 took 0.707s\n",
      "  training loss:\t\t0.022806\n",
      "Epoch 58 of 1000 took 0.705s\n",
      "  training loss:\t\t0.024899\n",
      "Epoch 59 of 1000 took 0.706s\n",
      "  training loss:\t\t0.031920\n",
      "Epoch 60 of 1000 took 0.701s\n",
      "  training loss:\t\t0.027217\n",
      "Epoch 61 of 1000 took 0.701s\n",
      "  training loss:\t\t0.024485\n",
      "Epoch 62 of 1000 took 0.704s\n",
      "  training loss:\t\t0.023735\n",
      "Epoch 63 of 1000 took 0.704s\n",
      "  training loss:\t\t0.023716\n",
      "Epoch 64 of 1000 took 0.733s\n",
      "  training loss:\t\t0.025326\n",
      "Epoch 65 of 1000 took 0.706s\n",
      "  training loss:\t\t0.031286\n",
      "Epoch 66 of 1000 took 0.708s\n",
      "  training loss:\t\t0.025945\n",
      "Epoch 67 of 1000 took 0.751s\n",
      "  training loss:\t\t0.023605\n",
      "Epoch 68 of 1000 took 0.783s\n",
      "  training loss:\t\t0.023089\n",
      "Epoch 69 of 1000 took 0.782s\n",
      "  training loss:\t\t0.022675\n",
      "Epoch 70 of 1000 took 0.711s\n",
      "  training loss:\t\t0.022648\n",
      "Epoch 71 of 1000 took 0.707s\n",
      "  training loss:\t\t0.022531\n",
      "Epoch 72 of 1000 took 0.707s\n",
      "  training loss:\t\t0.024403\n",
      "Epoch 73 of 1000 took 0.706s\n",
      "  training loss:\t\t0.028169\n",
      "Epoch 74 of 1000 took 0.704s\n",
      "  training loss:\t\t0.029694\n",
      "Epoch 75 of 1000 took 0.708s\n",
      "  training loss:\t\t0.025785\n",
      "Epoch 76 of 1000 took 0.708s\n",
      "  training loss:\t\t0.024267\n",
      "Epoch 77 of 1000 took 0.709s\n",
      "  training loss:\t\t0.025998\n",
      "Epoch 78 of 1000 took 0.702s\n",
      "  training loss:\t\t0.028214\n",
      "Epoch 79 of 1000 took 0.705s\n",
      "  training loss:\t\t0.026292\n",
      "Epoch 80 of 1000 took 0.708s\n",
      "  training loss:\t\t0.023698\n",
      "Epoch 81 of 1000 took 0.710s\n",
      "  training loss:\t\t0.023884\n",
      "Epoch 82 of 1000 took 0.703s\n",
      "  training loss:\t\t0.030507\n",
      "Epoch 83 of 1000 took 0.769s\n",
      "  training loss:\t\t0.028388\n",
      "Epoch 84 of 1000 took 0.798s\n",
      "  training loss:\t\t0.025724\n",
      "Epoch 85 of 1000 took 0.806s\n",
      "  training loss:\t\t0.024830\n",
      "Epoch 86 of 1000 took 0.726s\n",
      "  training loss:\t\t0.026182\n",
      "Epoch 87 of 1000 took 0.747s\n",
      "  training loss:\t\t0.030181\n",
      "Epoch 88 of 1000 took 0.752s\n",
      "  training loss:\t\t0.027686\n",
      "Epoch 89 of 1000 took 0.739s\n",
      "  training loss:\t\t0.024084\n",
      "Epoch 90 of 1000 took 0.737s\n",
      "  training loss:\t\t0.022502\n",
      "Epoch 91 of 1000 took 0.740s\n",
      "  training loss:\t\t0.022467\n",
      "Epoch 92 of 1000 took 0.749s\n",
      "  training loss:\t\t0.025536\n",
      "Epoch 93 of 1000 took 0.737s\n",
      "  training loss:\t\t0.025768\n",
      "Epoch 94 of 1000 took 0.737s\n",
      "  training loss:\t\t0.023735\n",
      "Epoch 95 of 1000 took 0.759s\n",
      "  training loss:\t\t0.024182\n",
      "Epoch 96 of 1000 took 0.771s\n",
      "  training loss:\t\t0.028097\n",
      "Epoch 97 of 1000 took 0.773s\n",
      "  training loss:\t\t0.023457\n",
      "Epoch 98 of 1000 took 0.776s\n",
      "  training loss:\t\t0.022167\n",
      "Epoch 99 of 1000 took 0.772s\n",
      "  training loss:\t\t0.022128\n",
      "Epoch 100 of 1000 took 0.776s\n",
      "  training loss:\t\t0.024258\n",
      "Epoch 101 of 1000 took 0.776s\n",
      "  training loss:\t\t0.026162\n",
      "Epoch 102 of 1000 took 0.778s\n",
      "  training loss:\t\t0.026447\n",
      "Epoch 103 of 1000 took 0.780s\n",
      "  training loss:\t\t0.022469\n",
      "Epoch 104 of 1000 took 0.735s\n",
      "  training loss:\t\t0.021588\n",
      "Epoch 105 of 1000 took 0.723s\n",
      "  training loss:\t\t0.021369\n",
      "Epoch 106 of 1000 took 0.738s\n",
      "  training loss:\t\t0.021360\n",
      "Epoch 107 of 1000 took 0.763s\n",
      "  training loss:\t\t0.021718\n",
      "Epoch 108 of 1000 took 0.765s\n",
      "  training loss:\t\t0.023789\n",
      "Epoch 109 of 1000 took 0.766s\n",
      "  training loss:\t\t0.023473\n",
      "Epoch 110 of 1000 took 0.775s\n",
      "  training loss:\t\t0.022529\n",
      "Epoch 111 of 1000 took 0.779s\n",
      "  training loss:\t\t0.021766\n",
      "Epoch 112 of 1000 took 0.774s\n",
      "  training loss:\t\t0.022461\n",
      "Epoch 113 of 1000 took 0.774s\n",
      "  training loss:\t\t0.021608\n",
      "Epoch 114 of 1000 took 0.767s\n",
      "  training loss:\t\t0.022355\n",
      "Epoch 115 of 1000 took 0.774s\n",
      "  training loss:\t\t0.021424\n",
      "Epoch 116 of 1000 took 0.775s\n",
      "  training loss:\t\t0.021966\n",
      "Epoch 117 of 1000 took 0.776s\n",
      "  training loss:\t\t0.021373\n",
      "Epoch 118 of 1000 took 0.776s\n",
      "  training loss:\t\t0.022083\n",
      "Epoch 119 of 1000 took 0.775s\n",
      "  training loss:\t\t0.021192\n",
      "Epoch 120 of 1000 took 0.770s\n",
      "  training loss:\t\t0.021744\n",
      "Epoch 121 of 1000 took 0.770s\n",
      "  training loss:\t\t0.022192\n",
      "Epoch 122 of 1000 took 0.775s\n",
      "  training loss:\t\t0.023282\n",
      "Epoch 123 of 1000 took 0.779s\n",
      "  training loss:\t\t0.022891\n",
      "Epoch 124 of 1000 took 0.774s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 125 of 1000 took 0.764s\n",
      "  training loss:\t\t0.023646\n",
      "Epoch 126 of 1000 took 0.766s\n",
      "  training loss:\t\t0.025761\n",
      "Epoch 127 of 1000 took 0.757s\n",
      "  training loss:\t\t0.026819\n",
      "Epoch 128 of 1000 took 0.772s\n",
      "  training loss:\t\t0.023363\n",
      "Epoch 129 of 1000 took 0.766s\n",
      "  training loss:\t\t0.027313\n",
      "Epoch 130 of 1000 took 0.726s\n",
      "  training loss:\t\t0.028099\n",
      "Epoch 131 of 1000 took 0.752s\n",
      "  training loss:\t\t0.024212\n",
      "Epoch 132 of 1000 took 0.746s\n",
      "  training loss:\t\t0.029273\n",
      "Epoch 133 of 1000 took 0.750s\n",
      "  training loss:\t\t0.028495\n",
      "Epoch 134 of 1000 took 0.702s\n",
      "  training loss:\t\t0.028423\n",
      "Epoch 135 of 1000 took 0.750s\n",
      "  training loss:\t\t0.023712\n",
      "Epoch 136 of 1000 took 0.724s\n",
      "  training loss:\t\t0.023445\n",
      "Epoch 137 of 1000 took 0.706s\n",
      "  training loss:\t\t0.021969\n",
      "Epoch 138 of 1000 took 0.745s\n",
      "  training loss:\t\t0.021267\n",
      "Epoch 139 of 1000 took 0.702s\n",
      "  training loss:\t\t0.022137\n",
      "Epoch 140 of 1000 took 0.766s\n",
      "  training loss:\t\t0.021244\n",
      "Epoch 141 of 1000 took 0.782s\n",
      "  training loss:\t\t0.022260\n",
      "Epoch 142 of 1000 took 0.773s\n",
      "  training loss:\t\t0.020212\n",
      "Epoch 143 of 1000 took 0.775s\n",
      "  training loss:\t\t0.019992\n",
      "Epoch 144 of 1000 took 0.771s\n",
      "  training loss:\t\t0.019921\n",
      "Epoch 145 of 1000 took 0.769s\n",
      "  training loss:\t\t0.020377\n",
      "Epoch 146 of 1000 took 0.773s\n",
      "  training loss:\t\t0.021231\n",
      "Epoch 147 of 1000 took 0.773s\n",
      "  training loss:\t\t0.028887\n",
      "Epoch 148 of 1000 took 0.770s\n",
      "  training loss:\t\t0.030967\n",
      "Epoch 149 of 1000 took 0.770s\n",
      "  training loss:\t\t0.025665\n",
      "Epoch 150 of 1000 took 0.769s\n",
      "  training loss:\t\t0.024630\n",
      "Epoch 151 of 1000 took 0.777s\n",
      "  training loss:\t\t0.025085\n",
      "Epoch 152 of 1000 took 0.775s\n",
      "  training loss:\t\t0.024900\n",
      "Epoch 153 of 1000 took 0.776s\n",
      "  training loss:\t\t0.024680\n",
      "Epoch 154 of 1000 took 0.781s\n",
      "  training loss:\t\t0.025565\n",
      "Epoch 155 of 1000 took 0.776s\n",
      "  training loss:\t\t0.022000\n",
      "Epoch 156 of 1000 took 0.774s\n",
      "  training loss:\t\t0.023025\n",
      "Epoch 157 of 1000 took 0.773s\n",
      "  training loss:\t\t0.024605\n",
      "Epoch 158 of 1000 took 0.773s\n",
      "  training loss:\t\t0.023064\n",
      "Epoch 159 of 1000 took 0.775s\n",
      "  training loss:\t\t0.022069\n",
      "Epoch 160 of 1000 took 0.773s\n",
      "  training loss:\t\t0.021868\n",
      "Epoch 161 of 1000 took 0.773s\n",
      "  training loss:\t\t0.021593\n",
      "Epoch 162 of 1000 took 0.775s\n",
      "  training loss:\t\t0.022278\n",
      "Epoch 163 of 1000 took 0.775s\n",
      "  training loss:\t\t0.021820\n",
      "Epoch 164 of 1000 took 0.775s\n",
      "  training loss:\t\t0.021634\n",
      "Epoch 165 of 1000 took 0.776s\n",
      "  training loss:\t\t0.020874\n",
      "Epoch 166 of 1000 took 0.777s\n",
      "  training loss:\t\t0.020932\n",
      "Epoch 167 of 1000 took 0.773s\n",
      "  training loss:\t\t0.021204\n",
      "Epoch 168 of 1000 took 0.773s\n",
      "  training loss:\t\t0.021874\n",
      "Epoch 169 of 1000 took 0.776s\n",
      "  training loss:\t\t0.023231\n",
      "Epoch 170 of 1000 took 0.777s\n",
      "  training loss:\t\t0.021126\n",
      "Epoch 171 of 1000 took 0.776s\n",
      "  training loss:\t\t0.021648\n",
      "Epoch 172 of 1000 took 0.774s\n",
      "  training loss:\t\t0.021307\n",
      "Epoch 173 of 1000 took 0.770s\n",
      "  training loss:\t\t0.022704\n",
      "Epoch 174 of 1000 took 0.773s\n",
      "  training loss:\t\t0.021052\n",
      "Epoch 175 of 1000 took 0.768s\n",
      "  training loss:\t\t0.020829\n",
      "Epoch 176 of 1000 took 0.775s\n",
      "  training loss:\t\t0.021665\n",
      "Epoch 177 of 1000 took 0.776s\n",
      "  training loss:\t\t0.020566\n",
      "Epoch 178 of 1000 took 0.773s\n",
      "  training loss:\t\t0.020951\n",
      "Epoch 179 of 1000 took 0.770s\n",
      "  training loss:\t\t0.021881\n",
      "Epoch 180 of 1000 took 0.769s\n",
      "  training loss:\t\t0.020893\n",
      "Epoch 181 of 1000 took 0.769s\n",
      "  training loss:\t\t0.021026\n",
      "Epoch 182 of 1000 took 0.772s\n",
      "  training loss:\t\t0.019681\n",
      "Epoch 183 of 1000 took 0.769s\n",
      "  training loss:\t\t0.019771\n",
      "Epoch 184 of 1000 took 0.771s\n",
      "  training loss:\t\t0.019806\n",
      "Epoch 185 of 1000 took 0.767s\n",
      "  training loss:\t\t0.020355\n",
      "Epoch 186 of 1000 took 0.771s\n",
      "  training loss:\t\t0.020886\n",
      "Epoch 187 of 1000 took 0.768s\n",
      "  training loss:\t\t0.022344\n",
      "Epoch 188 of 1000 took 0.766s\n",
      "  training loss:\t\t0.024992\n",
      "Epoch 189 of 1000 took 0.767s\n",
      "  training loss:\t\t0.021330\n",
      "Epoch 190 of 1000 took 0.773s\n",
      "  training loss:\t\t0.020704\n",
      "Epoch 191 of 1000 took 0.770s\n",
      "  training loss:\t\t0.020430\n",
      "Epoch 192 of 1000 took 0.769s\n",
      "  training loss:\t\t0.023523\n",
      "Epoch 193 of 1000 took 0.767s\n",
      "  training loss:\t\t0.021456\n",
      "Epoch 194 of 1000 took 0.768s\n",
      "  training loss:\t\t0.023661\n",
      "Epoch 195 of 1000 took 0.768s\n",
      "  training loss:\t\t0.024087\n",
      "Epoch 196 of 1000 took 0.768s\n",
      "  training loss:\t\t0.022350\n",
      "Epoch 197 of 1000 took 0.767s\n",
      "  training loss:\t\t0.022967\n",
      "Epoch 198 of 1000 took 0.767s\n",
      "  training loss:\t\t0.024081\n",
      "Epoch 199 of 1000 took 0.766s\n",
      "  training loss:\t\t0.026342\n",
      "Epoch 200 of 1000 took 0.766s\n",
      "  training loss:\t\t0.025285\n",
      "Epoch 201 of 1000 took 0.767s\n",
      "  training loss:\t\t0.029910\n",
      "Epoch 202 of 1000 took 0.766s\n",
      "  training loss:\t\t0.023854\n",
      "Epoch 203 of 1000 took 0.767s\n",
      "  training loss:\t\t0.021559\n",
      "Epoch 204 of 1000 took 0.767s\n",
      "  training loss:\t\t0.020735\n",
      "Epoch 205 of 1000 took 0.766s\n",
      "  training loss:\t\t0.022093\n",
      "Epoch 206 of 1000 took 0.767s\n",
      "  training loss:\t\t0.024224\n",
      "Epoch 207 of 1000 took 0.766s\n",
      "  training loss:\t\t0.020909\n",
      "Epoch 208 of 1000 took 0.767s\n",
      "  training loss:\t\t0.019884\n",
      "Epoch 209 of 1000 took 0.768s\n",
      "  training loss:\t\t0.021761\n",
      "Epoch 210 of 1000 took 0.767s\n",
      "  training loss:\t\t0.019810\n",
      "Epoch 211 of 1000 took 0.772s\n",
      "  training loss:\t\t0.018353\n",
      "Epoch 212 of 1000 took 0.772s\n",
      "  training loss:\t\t0.018351\n",
      "Epoch 213 of 1000 took 0.777s\n",
      "  training loss:\t\t0.019939\n",
      "Epoch 214 of 1000 took 0.772s\n",
      "  training loss:\t\t0.020140\n",
      "Epoch 215 of 1000 took 0.773s\n",
      "  training loss:\t\t0.018518\n",
      "Epoch 216 of 1000 took 0.768s\n",
      "  training loss:\t\t0.017887\n",
      "Epoch 217 of 1000 took 0.768s\n",
      "  training loss:\t\t0.018907\n",
      "Epoch 218 of 1000 took 0.769s\n",
      "  training loss:\t\t0.016880\n",
      "Epoch 219 of 1000 took 0.769s\n",
      "  training loss:\t\t0.018801\n",
      "Epoch 220 of 1000 took 0.768s\n",
      "  training loss:\t\t0.023567\n",
      "Epoch 221 of 1000 took 0.765s\n",
      "  training loss:\t\t0.025905\n",
      "Epoch 222 of 1000 took 0.774s\n",
      "  training loss:\t\t0.023713\n",
      "Epoch 223 of 1000 took 0.768s\n",
      "  training loss:\t\t0.021196\n",
      "Epoch 224 of 1000 took 0.774s\n",
      "  training loss:\t\t0.022298\n",
      "Epoch 225 of 1000 took 0.767s\n",
      "  training loss:\t\t0.020500\n",
      "Epoch 226 of 1000 took 0.768s\n",
      "  training loss:\t\t0.019947\n",
      "Epoch 227 of 1000 took 0.769s\n",
      "  training loss:\t\t0.021410\n",
      "Epoch 228 of 1000 took 0.767s\n",
      "  training loss:\t\t0.018673\n",
      "Epoch 229 of 1000 took 0.768s\n",
      "  training loss:\t\t0.018212\n",
      "Epoch 230 of 1000 took 0.768s\n",
      "  training loss:\t\t0.017640\n",
      "Epoch 231 of 1000 took 0.768s\n",
      "  training loss:\t\t0.016464\n",
      "Epoch 232 of 1000 took 0.769s\n",
      "  training loss:\t\t0.017367\n",
      "Epoch 233 of 1000 took 0.770s\n",
      "  training loss:\t\t0.023146\n",
      "Epoch 234 of 1000 took 0.774s\n",
      "  training loss:\t\t0.021405\n",
      "Epoch 235 of 1000 took 0.767s\n",
      "  training loss:\t\t0.019459\n",
      "Epoch 236 of 1000 took 0.769s\n",
      "  training loss:\t\t0.019907\n",
      "Epoch 237 of 1000 took 0.769s\n",
      "  training loss:\t\t0.019160\n",
      "Epoch 238 of 1000 took 0.771s\n",
      "  training loss:\t\t0.016400\n",
      "Epoch 239 of 1000 took 0.781s\n",
      "  training loss:\t\t0.017690\n",
      "Epoch 240 of 1000 took 0.749s\n",
      "  training loss:\t\t0.025810\n",
      "Epoch 241 of 1000 took 0.738s\n",
      "  training loss:\t\t0.022271\n",
      "Epoch 242 of 1000 took 0.708s\n",
      "  training loss:\t\t0.022297\n",
      "Epoch 243 of 1000 took 0.703s\n",
      "  training loss:\t\t0.023320\n",
      "Epoch 244 of 1000 took 0.704s\n",
      "  training loss:\t\t0.018364\n",
      "Epoch 245 of 1000 took 0.702s\n",
      "  training loss:\t\t0.016415\n",
      "Epoch 246 of 1000 took 0.758s\n",
      "  training loss:\t\t0.016066\n",
      "Epoch 247 of 1000 took 0.748s\n",
      "  training loss:\t\t0.016918\n",
      "Epoch 248 of 1000 took 0.757s\n",
      "  training loss:\t\t0.015432\n",
      "Epoch 249 of 1000 took 0.758s\n",
      "  training loss:\t\t0.016147\n",
      "Epoch 250 of 1000 took 0.749s\n",
      "  training loss:\t\t0.017400\n",
      "Epoch 251 of 1000 took 0.706s\n",
      "  training loss:\t\t0.019243\n",
      "Epoch 252 of 1000 took 0.708s\n",
      "  training loss:\t\t0.019523\n",
      "Epoch 253 of 1000 took 0.708s\n",
      "  training loss:\t\t0.015142\n",
      "Epoch 254 of 1000 took 0.728s\n",
      "  training loss:\t\t0.014777\n",
      "Epoch 255 of 1000 took 0.730s\n",
      "  training loss:\t\t0.014352\n",
      "Epoch 256 of 1000 took 0.704s\n",
      "  training loss:\t\t0.014981\n",
      "Epoch 257 of 1000 took 0.743s\n",
      "  training loss:\t\t0.014237\n",
      "Epoch 258 of 1000 took 0.702s\n",
      "  training loss:\t\t0.014227\n",
      "Epoch 259 of 1000 took 0.734s\n",
      "  training loss:\t\t0.016982\n",
      "Epoch 260 of 1000 took 0.707s\n",
      "  training loss:\t\t0.015578\n",
      "Epoch 261 of 1000 took 0.704s\n",
      "  training loss:\t\t0.016460\n",
      "Epoch 262 of 1000 took 0.752s\n",
      "  training loss:\t\t0.020003\n",
      "Epoch 263 of 1000 took 0.778s\n",
      "  training loss:\t\t0.020421\n",
      "Epoch 264 of 1000 took 0.771s\n",
      "  training loss:\t\t0.021091\n",
      "Epoch 265 of 1000 took 0.770s\n",
      "  training loss:\t\t0.016064\n",
      "Epoch 266 of 1000 took 0.771s\n",
      "  training loss:\t\t0.019154\n",
      "Epoch 267 of 1000 took 0.769s\n",
      "  training loss:\t\t0.015448\n",
      "Epoch 268 of 1000 took 0.771s\n",
      "  training loss:\t\t0.019114\n",
      "Epoch 269 of 1000 took 0.772s\n",
      "  training loss:\t\t0.017142\n",
      "Epoch 270 of 1000 took 0.770s\n",
      "  training loss:\t\t0.014739\n",
      "Epoch 271 of 1000 took 0.768s\n",
      "  training loss:\t\t0.014558\n",
      "Epoch 272 of 1000 took 0.769s\n",
      "  training loss:\t\t0.015253\n",
      "Epoch 273 of 1000 took 0.770s\n",
      "  training loss:\t\t0.016556\n",
      "Epoch 274 of 1000 took 0.768s\n",
      "  training loss:\t\t0.017279\n",
      "Epoch 275 of 1000 took 0.771s\n",
      "  training loss:\t\t0.013937\n",
      "Epoch 276 of 1000 took 0.773s\n",
      "  training loss:\t\t0.012668\n",
      "Epoch 277 of 1000 took 0.769s\n",
      "  training loss:\t\t0.012162\n",
      "Epoch 278 of 1000 took 0.767s\n",
      "  training loss:\t\t0.012234\n",
      "Epoch 279 of 1000 took 0.767s\n",
      "  training loss:\t\t0.013120\n",
      "Epoch 280 of 1000 took 0.768s\n",
      "  training loss:\t\t0.016641\n",
      "Epoch 281 of 1000 took 0.765s\n",
      "  training loss:\t\t0.013439\n",
      "Epoch 282 of 1000 took 0.769s\n",
      "  training loss:\t\t0.013793\n",
      "Epoch 283 of 1000 took 0.771s\n",
      "  training loss:\t\t0.014767\n",
      "Epoch 284 of 1000 took 0.769s\n",
      "  training loss:\t\t0.012371\n",
      "Epoch 285 of 1000 took 0.769s\n",
      "  training loss:\t\t0.011938\n",
      "Epoch 286 of 1000 took 0.769s\n",
      "  training loss:\t\t0.011931\n",
      "Epoch 287 of 1000 took 0.770s\n",
      "  training loss:\t\t0.013704\n",
      "Epoch 288 of 1000 took 0.769s\n",
      "  training loss:\t\t0.012010\n",
      "Epoch 289 of 1000 took 0.770s\n",
      "  training loss:\t\t0.013925\n",
      "Epoch 290 of 1000 took 0.770s\n",
      "  training loss:\t\t0.014802\n",
      "Epoch 291 of 1000 took 0.767s\n",
      "  training loss:\t\t0.025550\n",
      "Epoch 292 of 1000 took 0.772s\n",
      "  training loss:\t\t0.023093\n",
      "Epoch 293 of 1000 took 0.771s\n",
      "  training loss:\t\t0.020970\n",
      "Epoch 294 of 1000 took 0.770s\n",
      "  training loss:\t\t0.024255\n",
      "Epoch 295 of 1000 took 0.770s\n",
      "  training loss:\t\t0.019325\n",
      "Epoch 296 of 1000 took 0.769s\n",
      "  training loss:\t\t0.033692\n",
      "Epoch 297 of 1000 took 0.769s\n",
      "  training loss:\t\t0.022728\n",
      "Epoch 298 of 1000 took 0.768s\n",
      "  training loss:\t\t0.018377\n",
      "Epoch 299 of 1000 took 0.768s\n",
      "  training loss:\t\t0.021636\n",
      "Epoch 300 of 1000 took 0.768s\n",
      "  training loss:\t\t0.030379\n",
      "Epoch 301 of 1000 took 0.772s\n",
      "  training loss:\t\t0.026813\n",
      "Epoch 302 of 1000 took 0.769s\n",
      "  training loss:\t\t0.025357\n",
      "Epoch 303 of 1000 took 0.771s\n",
      "  training loss:\t\t0.034708\n",
      "Epoch 304 of 1000 took 0.772s\n",
      "  training loss:\t\t0.024512\n",
      "Epoch 305 of 1000 took 0.774s\n",
      "  training loss:\t\t0.020660\n",
      "Epoch 306 of 1000 took 0.772s\n",
      "  training loss:\t\t0.017833\n",
      "Epoch 307 of 1000 took 0.771s\n",
      "  training loss:\t\t0.016495\n",
      "Epoch 308 of 1000 took 0.770s\n",
      "  training loss:\t\t0.023431\n",
      "Epoch 309 of 1000 took 0.770s\n",
      "  training loss:\t\t0.025649\n",
      "Epoch 310 of 1000 took 0.770s\n",
      "  training loss:\t\t0.022643\n",
      "Epoch 311 of 1000 took 0.771s\n",
      "  training loss:\t\t0.027576\n",
      "Epoch 312 of 1000 took 0.775s\n",
      "  training loss:\t\t0.026058\n",
      "Epoch 313 of 1000 took 0.776s\n",
      "  training loss:\t\t0.020838\n",
      "Epoch 314 of 1000 took 0.776s\n",
      "  training loss:\t\t0.018477\n",
      "Epoch 315 of 1000 took 0.767s\n",
      "  training loss:\t\t0.017671\n",
      "Epoch 316 of 1000 took 0.766s\n",
      "  training loss:\t\t0.018113\n",
      "Epoch 317 of 1000 took 0.769s\n",
      "  training loss:\t\t0.021027\n",
      "Epoch 318 of 1000 took 0.766s\n",
      "  training loss:\t\t0.019886\n",
      "Epoch 319 of 1000 took 0.768s\n",
      "  training loss:\t\t0.015289\n",
      "Epoch 320 of 1000 took 0.769s\n",
      "  training loss:\t\t0.013892\n",
      "Epoch 321 of 1000 took 0.771s\n",
      "  training loss:\t\t0.013402\n",
      "Epoch 322 of 1000 took 0.781s\n",
      "  training loss:\t\t0.013635\n",
      "Epoch 323 of 1000 took 0.773s\n",
      "  training loss:\t\t0.016829\n",
      "Epoch 324 of 1000 took 0.770s\n",
      "  training loss:\t\t0.013335\n",
      "Epoch 325 of 1000 took 0.770s\n",
      "  training loss:\t\t0.016122\n",
      "Epoch 326 of 1000 took 0.769s\n",
      "  training loss:\t\t0.018714\n",
      "Epoch 327 of 1000 took 0.768s\n",
      "  training loss:\t\t0.019052\n",
      "Epoch 328 of 1000 took 0.771s\n",
      "  training loss:\t\t0.016210\n",
      "Epoch 329 of 1000 took 0.775s\n",
      "  training loss:\t\t0.016222\n",
      "Epoch 330 of 1000 took 0.771s\n",
      "  training loss:\t\t0.017352\n",
      "Epoch 331 of 1000 took 0.771s\n",
      "  training loss:\t\t0.013429\n",
      "Epoch 332 of 1000 took 0.772s\n",
      "  training loss:\t\t0.013189\n",
      "Epoch 333 of 1000 took 0.771s\n",
      "  training loss:\t\t0.016276\n",
      "Epoch 334 of 1000 took 0.771s\n",
      "  training loss:\t\t0.011986\n",
      "Epoch 335 of 1000 took 0.778s\n",
      "  training loss:\t\t0.011333\n",
      "Epoch 336 of 1000 took 0.773s\n",
      "  training loss:\t\t0.011470\n",
      "Epoch 337 of 1000 took 0.774s\n",
      "  training loss:\t\t0.011941\n",
      "Epoch 338 of 1000 took 0.772s\n",
      "  training loss:\t\t0.013124\n",
      "Epoch 339 of 1000 took 0.772s\n",
      "  training loss:\t\t0.012734\n",
      "Epoch 340 of 1000 took 0.775s\n",
      "  training loss:\t\t0.011364\n",
      "Epoch 341 of 1000 took 0.768s\n",
      "  training loss:\t\t0.011927\n",
      "Epoch 342 of 1000 took 0.770s\n",
      "  training loss:\t\t0.012035\n",
      "Epoch 343 of 1000 took 0.772s\n",
      "  training loss:\t\t0.014679\n",
      "Epoch 344 of 1000 took 0.774s\n",
      "  training loss:\t\t0.010746\n",
      "Epoch 345 of 1000 took 0.778s\n",
      "  training loss:\t\t0.009969\n",
      "Epoch 346 of 1000 took 0.775s\n",
      "  training loss:\t\t0.009901\n",
      "Epoch 347 of 1000 took 0.773s\n",
      "  training loss:\t\t0.010376\n",
      "Epoch 348 of 1000 took 0.768s\n",
      "  training loss:\t\t0.013442\n",
      "Epoch 349 of 1000 took 0.773s\n",
      "  training loss:\t\t0.015174\n",
      "Epoch 350 of 1000 took 0.773s\n",
      "  training loss:\t\t0.013757\n",
      "Epoch 351 of 1000 took 0.774s\n",
      "  training loss:\t\t0.013201\n",
      "Epoch 352 of 1000 took 0.776s\n",
      "  training loss:\t\t0.015549\n",
      "Epoch 353 of 1000 took 0.773s\n",
      "  training loss:\t\t0.010434\n",
      "Epoch 354 of 1000 took 0.776s\n",
      "  training loss:\t\t0.009968\n",
      "Epoch 355 of 1000 took 0.776s\n",
      "  training loss:\t\t0.010335\n",
      "Epoch 356 of 1000 took 0.774s\n",
      "  training loss:\t\t0.010859\n",
      "Epoch 357 of 1000 took 0.776s\n",
      "  training loss:\t\t0.010656\n",
      "Epoch 358 of 1000 took 0.776s\n",
      "  training loss:\t\t0.010957\n",
      "Epoch 359 of 1000 took 0.776s\n",
      "  training loss:\t\t0.011810\n",
      "Epoch 360 of 1000 took 0.774s\n",
      "  training loss:\t\t0.011688\n",
      "Epoch 361 of 1000 took 0.774s\n",
      "  training loss:\t\t0.010077\n",
      "Epoch 362 of 1000 took 0.780s\n",
      "  training loss:\t\t0.010547\n",
      "Epoch 363 of 1000 took 0.775s\n",
      "  training loss:\t\t0.010102\n",
      "Epoch 364 of 1000 took 0.776s\n",
      "  training loss:\t\t0.011676\n",
      "Epoch 365 of 1000 took 0.779s\n",
      "  training loss:\t\t0.009819\n",
      "Epoch 366 of 1000 took 0.774s\n",
      "  training loss:\t\t0.010838\n",
      "Epoch 367 of 1000 took 0.771s\n",
      "  training loss:\t\t0.011392\n",
      "Epoch 368 of 1000 took 0.775s\n",
      "  training loss:\t\t0.014264\n",
      "Epoch 369 of 1000 took 0.774s\n",
      "  training loss:\t\t0.010763\n",
      "Epoch 370 of 1000 took 0.777s\n",
      "  training loss:\t\t0.010954\n",
      "Epoch 371 of 1000 took 0.775s\n",
      "  training loss:\t\t0.013304\n",
      "Epoch 372 of 1000 took 0.771s\n",
      "  training loss:\t\t0.012387\n",
      "Epoch 373 of 1000 took 0.769s\n",
      "  training loss:\t\t0.014911\n",
      "Epoch 374 of 1000 took 0.770s\n",
      "  training loss:\t\t0.014064\n",
      "Epoch 375 of 1000 took 0.772s\n",
      "  training loss:\t\t0.011636\n",
      "Epoch 376 of 1000 took 0.776s\n",
      "  training loss:\t\t0.013732\n",
      "Epoch 377 of 1000 took 0.777s\n",
      "  training loss:\t\t0.012164\n",
      "Epoch 378 of 1000 took 0.777s\n",
      "  training loss:\t\t0.011892\n",
      "Epoch 379 of 1000 took 0.778s\n",
      "  training loss:\t\t0.011797\n",
      "Epoch 380 of 1000 took 0.778s\n",
      "  training loss:\t\t0.013556\n",
      "Epoch 381 of 1000 took 0.775s\n",
      "  training loss:\t\t0.010056\n",
      "Epoch 382 of 1000 took 0.776s\n",
      "  training loss:\t\t0.009259\n",
      "Epoch 383 of 1000 took 0.778s\n",
      "  training loss:\t\t0.009502\n",
      "Epoch 384 of 1000 took 0.779s\n",
      "  training loss:\t\t0.010652\n",
      "Epoch 385 of 1000 took 0.779s\n",
      "  training loss:\t\t0.013864\n",
      "Epoch 386 of 1000 took 0.778s\n",
      "  training loss:\t\t0.009637\n",
      "Epoch 387 of 1000 took 0.778s\n",
      "  training loss:\t\t0.010191\n",
      "Epoch 388 of 1000 took 0.774s\n",
      "  training loss:\t\t0.009554\n",
      "Epoch 389 of 1000 took 0.775s\n",
      "  training loss:\t\t0.010397\n",
      "Epoch 390 of 1000 took 0.771s\n",
      "  training loss:\t\t0.009550\n",
      "Epoch 391 of 1000 took 0.775s\n",
      "  training loss:\t\t0.010496\n",
      "Epoch 392 of 1000 took 0.781s\n",
      "  training loss:\t\t0.009403\n",
      "Epoch 393 of 1000 took 0.776s\n",
      "  training loss:\t\t0.009380\n",
      "Epoch 394 of 1000 took 0.779s\n",
      "  training loss:\t\t0.008565\n",
      "Epoch 395 of 1000 took 0.779s\n",
      "  training loss:\t\t0.008963\n",
      "Epoch 396 of 1000 took 0.776s\n",
      "  training loss:\t\t0.008661\n",
      "Epoch 397 of 1000 took 0.776s\n",
      "  training loss:\t\t0.010163\n",
      "Epoch 398 of 1000 took 0.773s\n",
      "  training loss:\t\t0.009157\n",
      "Epoch 399 of 1000 took 0.776s\n",
      "  training loss:\t\t0.011667\n",
      "Epoch 400 of 1000 took 0.777s\n",
      "  training loss:\t\t0.012352\n",
      "Epoch 401 of 1000 took 0.772s\n",
      "  training loss:\t\t0.009417\n",
      "Epoch 402 of 1000 took 0.774s\n",
      "  training loss:\t\t0.010698\n",
      "Epoch 403 of 1000 took 0.774s\n",
      "  training loss:\t\t0.008790\n",
      "Epoch 404 of 1000 took 0.772s\n",
      "  training loss:\t\t0.008816\n",
      "Epoch 405 of 1000 took 0.770s\n",
      "  training loss:\t\t0.008115\n",
      "Epoch 406 of 1000 took 0.768s\n",
      "  training loss:\t\t0.008350\n",
      "Epoch 407 of 1000 took 0.770s\n",
      "  training loss:\t\t0.008206\n",
      "Epoch 408 of 1000 took 0.770s\n",
      "  training loss:\t\t0.009241\n",
      "Epoch 409 of 1000 took 0.773s\n",
      "  training loss:\t\t0.009276\n",
      "Epoch 410 of 1000 took 0.776s\n",
      "  training loss:\t\t0.010197\n",
      "Epoch 411 of 1000 took 0.773s\n",
      "  training loss:\t\t0.010832\n",
      "Epoch 412 of 1000 took 0.779s\n",
      "  training loss:\t\t0.010764\n",
      "Epoch 413 of 1000 took 0.771s\n",
      "  training loss:\t\t0.012663\n",
      "Epoch 414 of 1000 took 0.774s\n",
      "  training loss:\t\t0.009244\n",
      "Epoch 415 of 1000 took 0.779s\n",
      "  training loss:\t\t0.011269\n",
      "Epoch 416 of 1000 took 0.770s\n",
      "  training loss:\t\t0.012129\n",
      "Epoch 417 of 1000 took 0.771s\n",
      "  training loss:\t\t0.011089\n",
      "Epoch 418 of 1000 took 0.771s\n",
      "  training loss:\t\t0.010541\n",
      "Epoch 419 of 1000 took 0.771s\n",
      "  training loss:\t\t0.011053\n",
      "Epoch 420 of 1000 took 0.772s\n",
      "  training loss:\t\t0.009387\n",
      "Epoch 421 of 1000 took 0.774s\n",
      "  training loss:\t\t0.011181\n",
      "Epoch 422 of 1000 took 0.773s\n",
      "  training loss:\t\t0.011042\n",
      "Epoch 423 of 1000 took 0.775s\n",
      "  training loss:\t\t0.009596\n",
      "Epoch 424 of 1000 took 0.770s\n",
      "  training loss:\t\t0.009018\n",
      "Epoch 425 of 1000 took 0.769s\n",
      "  training loss:\t\t0.009596\n",
      "Epoch 426 of 1000 took 0.768s\n",
      "  training loss:\t\t0.008234\n",
      "Epoch 427 of 1000 took 0.768s\n",
      "  training loss:\t\t0.008156\n",
      "Epoch 428 of 1000 took 0.768s\n",
      "  training loss:\t\t0.007746\n",
      "Epoch 429 of 1000 took 0.770s\n",
      "  training loss:\t\t0.008076\n",
      "Epoch 430 of 1000 took 0.772s\n",
      "  training loss:\t\t0.008258\n",
      "Epoch 431 of 1000 took 0.771s\n",
      "  training loss:\t\t0.009232\n",
      "Epoch 432 of 1000 took 0.769s\n",
      "  training loss:\t\t0.008117\n",
      "Epoch 433 of 1000 took 0.768s\n",
      "  training loss:\t\t0.008613\n",
      "Epoch 434 of 1000 took 0.771s\n",
      "  training loss:\t\t0.007746\n",
      "Epoch 435 of 1000 took 0.773s\n",
      "  training loss:\t\t0.008349\n",
      "Epoch 436 of 1000 took 0.776s\n",
      "  training loss:\t\t0.007679\n",
      "Epoch 437 of 1000 took 0.774s\n",
      "  training loss:\t\t0.008652\n",
      "Epoch 438 of 1000 took 0.773s\n",
      "  training loss:\t\t0.007688\n",
      "Epoch 439 of 1000 took 0.771s\n",
      "  training loss:\t\t0.008474\n",
      "Epoch 440 of 1000 took 0.769s\n",
      "  training loss:\t\t0.009326\n",
      "Epoch 441 of 1000 took 0.767s\n",
      "  training loss:\t\t0.009385\n",
      "Epoch 442 of 1000 took 0.770s\n",
      "  training loss:\t\t0.009926\n",
      "Epoch 443 of 1000 took 0.775s\n",
      "  training loss:\t\t0.008886\n",
      "Epoch 444 of 1000 took 0.771s\n",
      "  training loss:\t\t0.010290\n",
      "Epoch 445 of 1000 took 0.768s\n",
      "  training loss:\t\t0.008633\n",
      "Epoch 446 of 1000 took 0.768s\n",
      "  training loss:\t\t0.009089\n",
      "Epoch 447 of 1000 took 0.768s\n",
      "  training loss:\t\t0.009332\n",
      "Epoch 448 of 1000 took 0.768s\n",
      "  training loss:\t\t0.008959\n",
      "Epoch 449 of 1000 took 0.769s\n",
      "  training loss:\t\t0.008793\n",
      "Epoch 450 of 1000 took 0.771s\n",
      "  training loss:\t\t0.008705\n",
      "Epoch 451 of 1000 took 0.777s\n",
      "  training loss:\t\t0.009290\n",
      "Epoch 452 of 1000 took 0.778s\n",
      "  training loss:\t\t0.008305\n",
      "Epoch 453 of 1000 took 0.775s\n",
      "  training loss:\t\t0.008539\n",
      "Epoch 454 of 1000 took 0.778s\n",
      "  training loss:\t\t0.007649\n",
      "Epoch 455 of 1000 took 0.779s\n",
      "  training loss:\t\t0.008504\n",
      "Epoch 456 of 1000 took 0.780s\n",
      "  training loss:\t\t0.007542\n",
      "Epoch 457 of 1000 took 0.775s\n",
      "  training loss:\t\t0.007948\n",
      "Epoch 458 of 1000 took 0.776s\n",
      "  training loss:\t\t0.007658\n",
      "Epoch 459 of 1000 took 0.776s\n",
      "  training loss:\t\t0.008366\n",
      "Epoch 460 of 1000 took 0.773s\n",
      "  training loss:\t\t0.007239\n",
      "Epoch 461 of 1000 took 0.779s\n",
      "  training loss:\t\t0.008015\n",
      "Epoch 462 of 1000 took 0.776s\n",
      "  training loss:\t\t0.007624\n",
      "Epoch 463 of 1000 took 0.774s\n",
      "  training loss:\t\t0.009169\n",
      "Epoch 464 of 1000 took 0.775s\n",
      "  training loss:\t\t0.008740\n",
      "Epoch 465 of 1000 took 0.774s\n",
      "  training loss:\t\t0.008589\n",
      "Epoch 466 of 1000 took 0.773s\n",
      "  training loss:\t\t0.010143\n",
      "Epoch 467 of 1000 took 0.772s\n",
      "  training loss:\t\t0.008078\n",
      "Epoch 468 of 1000 took 0.768s\n",
      "  training loss:\t\t0.008110\n",
      "Epoch 469 of 1000 took 0.768s\n",
      "  training loss:\t\t0.007761\n",
      "Epoch 470 of 1000 took 0.768s\n",
      "  training loss:\t\t0.008068\n",
      "Epoch 471 of 1000 took 0.767s\n",
      "  training loss:\t\t0.007920\n",
      "Epoch 472 of 1000 took 0.767s\n",
      "  training loss:\t\t0.007770\n",
      "Epoch 473 of 1000 took 0.767s\n",
      "  training loss:\t\t0.007115\n",
      "Epoch 474 of 1000 took 0.768s\n",
      "  training loss:\t\t0.006723\n",
      "Epoch 475 of 1000 took 0.765s\n",
      "  training loss:\t\t0.006550\n",
      "Epoch 476 of 1000 took 0.766s\n",
      "  training loss:\t\t0.006923\n",
      "Epoch 477 of 1000 took 0.767s\n",
      "  training loss:\t\t0.007417\n",
      "Epoch 478 of 1000 took 0.766s\n",
      "  training loss:\t\t0.009272\n",
      "Epoch 479 of 1000 took 0.767s\n",
      "  training loss:\t\t0.008203\n",
      "Epoch 480 of 1000 took 0.767s\n",
      "  training loss:\t\t0.007990\n",
      "Epoch 481 of 1000 took 0.767s\n",
      "  training loss:\t\t0.010179\n",
      "Epoch 482 of 1000 took 0.766s\n",
      "  training loss:\t\t0.009214\n",
      "Epoch 483 of 1000 took 0.767s\n",
      "  training loss:\t\t0.010138\n",
      "Epoch 484 of 1000 took 0.766s\n",
      "  training loss:\t\t0.007672\n",
      "Epoch 485 of 1000 took 0.767s\n",
      "  training loss:\t\t0.009180\n",
      "Epoch 486 of 1000 took 0.770s\n",
      "  training loss:\t\t0.008816\n",
      "Epoch 487 of 1000 took 0.769s\n",
      "  training loss:\t\t0.008466\n",
      "Epoch 488 of 1000 took 0.771s\n",
      "  training loss:\t\t0.007692\n",
      "Epoch 489 of 1000 took 0.773s\n",
      "  training loss:\t\t0.008693\n",
      "Epoch 490 of 1000 took 0.772s\n",
      "  training loss:\t\t0.009168\n",
      "Epoch 491 of 1000 took 0.773s\n",
      "  training loss:\t\t0.006968\n",
      "Epoch 492 of 1000 took 0.774s\n",
      "  training loss:\t\t0.006752\n",
      "Epoch 493 of 1000 took 0.773s\n",
      "  training loss:\t\t0.006670\n",
      "Epoch 494 of 1000 took 0.769s\n",
      "  training loss:\t\t0.007121\n",
      "Epoch 495 of 1000 took 0.767s\n",
      "  training loss:\t\t0.007698\n",
      "Epoch 496 of 1000 took 0.767s\n",
      "  training loss:\t\t0.009524\n",
      "Epoch 497 of 1000 took 0.768s\n",
      "  training loss:\t\t0.007488\n",
      "Epoch 498 of 1000 took 0.770s\n",
      "  training loss:\t\t0.007753\n",
      "Epoch 499 of 1000 took 0.768s\n",
      "  training loss:\t\t0.007210\n",
      "Epoch 500 of 1000 took 0.767s\n",
      "  training loss:\t\t0.007635\n",
      "Epoch 501 of 1000 took 0.767s\n",
      "  training loss:\t\t0.007095\n",
      "Epoch 502 of 1000 took 0.767s\n",
      "  training loss:\t\t0.007571\n",
      "Epoch 503 of 1000 took 0.767s\n",
      "  training loss:\t\t0.007354\n",
      "Epoch 504 of 1000 took 0.769s\n",
      "  training loss:\t\t0.008142\n",
      "Epoch 505 of 1000 took 0.775s\n",
      "  training loss:\t\t0.007367\n",
      "Epoch 506 of 1000 took 0.776s\n",
      "  training loss:\t\t0.007838\n",
      "Epoch 507 of 1000 took 0.771s\n",
      "  training loss:\t\t0.007376\n",
      "Epoch 508 of 1000 took 0.768s\n",
      "  training loss:\t\t0.008257\n",
      "Epoch 509 of 1000 took 0.767s\n",
      "  training loss:\t\t0.008136\n",
      "Epoch 510 of 1000 took 0.766s\n",
      "  training loss:\t\t0.009210\n",
      "Epoch 511 of 1000 took 0.766s\n",
      "  training loss:\t\t0.010910\n",
      "Epoch 512 of 1000 took 0.767s\n",
      "  training loss:\t\t0.010667\n",
      "Epoch 513 of 1000 took 0.769s\n",
      "  training loss:\t\t0.010511\n",
      "Epoch 514 of 1000 took 0.769s\n",
      "  training loss:\t\t0.008858\n",
      "Epoch 515 of 1000 took 0.770s\n",
      "  training loss:\t\t0.008747\n",
      "Epoch 516 of 1000 took 0.771s\n",
      "  training loss:\t\t0.008176\n",
      "Epoch 517 of 1000 took 0.770s\n",
      "  training loss:\t\t0.007174\n",
      "Epoch 518 of 1000 took 0.773s\n",
      "  training loss:\t\t0.006775\n",
      "Epoch 519 of 1000 took 0.769s\n",
      "  training loss:\t\t0.007303\n",
      "Epoch 520 of 1000 took 0.771s\n",
      "  training loss:\t\t0.007039\n",
      "Epoch 521 of 1000 took 0.768s\n",
      "  training loss:\t\t0.007891\n",
      "Epoch 522 of 1000 took 0.770s\n",
      "  training loss:\t\t0.007171\n",
      "Epoch 523 of 1000 took 0.774s\n",
      "  training loss:\t\t0.008066\n",
      "Epoch 524 of 1000 took 0.774s\n",
      "  training loss:\t\t0.007243\n",
      "Epoch 525 of 1000 took 0.769s\n",
      "  training loss:\t\t0.007827\n",
      "Epoch 526 of 1000 took 0.768s\n",
      "  training loss:\t\t0.006817\n",
      "Epoch 527 of 1000 took 0.772s\n",
      "  training loss:\t\t0.007259\n",
      "Epoch 528 of 1000 took 0.771s\n",
      "  training loss:\t\t0.007107\n",
      "Epoch 529 of 1000 took 0.768s\n",
      "  training loss:\t\t0.007651\n",
      "Epoch 530 of 1000 took 0.772s\n",
      "  training loss:\t\t0.007465\n",
      "Epoch 531 of 1000 took 0.769s\n",
      "  training loss:\t\t0.007843\n",
      "Epoch 532 of 1000 took 0.775s\n",
      "  training loss:\t\t0.007208\n",
      "Epoch 533 of 1000 took 0.771s\n",
      "  training loss:\t\t0.006976\n",
      "Epoch 534 of 1000 took 0.776s\n",
      "  training loss:\t\t0.006839\n",
      "Epoch 535 of 1000 took 0.774s\n",
      "  training loss:\t\t0.006497\n",
      "Epoch 536 of 1000 took 0.773s\n",
      "  training loss:\t\t0.006807\n",
      "Epoch 537 of 1000 took 0.771s\n",
      "  training loss:\t\t0.006637\n",
      "Epoch 538 of 1000 took 0.772s\n",
      "  training loss:\t\t0.007131\n",
      "Epoch 539 of 1000 took 0.776s\n",
      "  training loss:\t\t0.007280\n",
      "Epoch 540 of 1000 took 0.777s\n",
      "  training loss:\t\t0.006842\n",
      "Epoch 541 of 1000 took 0.772s\n",
      "  training loss:\t\t0.007025\n",
      "Epoch 542 of 1000 took 0.769s\n",
      "  training loss:\t\t0.006558\n",
      "Epoch 543 of 1000 took 0.772s\n",
      "  training loss:\t\t0.006734\n",
      "Epoch 544 of 1000 took 0.778s\n",
      "  training loss:\t\t0.007796\n",
      "Epoch 545 of 1000 took 0.780s\n",
      "  training loss:\t\t0.007244\n",
      "Epoch 546 of 1000 took 0.777s\n",
      "  training loss:\t\t0.009541\n",
      "Epoch 547 of 1000 took 0.777s\n",
      "  training loss:\t\t0.008118\n",
      "Epoch 548 of 1000 took 0.773s\n",
      "  training loss:\t\t0.008008\n",
      "Epoch 549 of 1000 took 0.775s\n",
      "  training loss:\t\t0.007122\n",
      "Epoch 550 of 1000 took 0.776s\n",
      "  training loss:\t\t0.006850\n",
      "Epoch 551 of 1000 took 0.781s\n",
      "  training loss:\t\t0.006920\n",
      "Epoch 552 of 1000 took 0.781s\n",
      "  training loss:\t\t0.007641\n",
      "Epoch 553 of 1000 took 0.772s\n",
      "  training loss:\t\t0.007971\n",
      "Epoch 554 of 1000 took 0.775s\n",
      "  training loss:\t\t0.007339\n",
      "Epoch 555 of 1000 took 0.774s\n",
      "  training loss:\t\t0.006720\n",
      "Epoch 556 of 1000 took 0.771s\n",
      "  training loss:\t\t0.007276\n",
      "Epoch 557 of 1000 took 0.772s\n",
      "  training loss:\t\t0.006787\n",
      "Epoch 558 of 1000 took 0.769s\n",
      "  training loss:\t\t0.007681\n",
      "Epoch 559 of 1000 took 0.771s\n",
      "  training loss:\t\t0.006638\n",
      "Epoch 560 of 1000 took 0.771s\n",
      "  training loss:\t\t0.007399\n",
      "Epoch 561 of 1000 took 0.770s\n",
      "  training loss:\t\t0.006620\n",
      "Epoch 562 of 1000 took 0.775s\n",
      "  training loss:\t\t0.008016\n",
      "Epoch 563 of 1000 took 0.778s\n",
      "  training loss:\t\t0.006787\n",
      "Epoch 564 of 1000 took 0.772s\n",
      "  training loss:\t\t0.006965\n",
      "Epoch 565 of 1000 took 0.773s\n",
      "  training loss:\t\t0.007664\n",
      "Epoch 566 of 1000 took 0.770s\n",
      "  training loss:\t\t0.006898\n",
      "Epoch 567 of 1000 took 0.771s\n",
      "  training loss:\t\t0.007132\n",
      "Epoch 568 of 1000 took 0.779s\n",
      "  training loss:\t\t0.006410\n",
      "Epoch 569 of 1000 took 0.770s\n",
      "  training loss:\t\t0.007051\n",
      "Epoch 570 of 1000 took 0.773s\n",
      "  training loss:\t\t0.006928\n",
      "Epoch 571 of 1000 took 0.772s\n",
      "  training loss:\t\t0.007959\n",
      "Epoch 572 of 1000 took 0.773s\n",
      "  training loss:\t\t0.006209\n",
      "Epoch 573 of 1000 took 0.775s\n",
      "  training loss:\t\t0.006233\n",
      "Epoch 574 of 1000 took 0.768s\n",
      "  training loss:\t\t0.005912\n",
      "Epoch 575 of 1000 took 0.767s\n",
      "  training loss:\t\t0.006181\n",
      "Epoch 576 of 1000 took 0.772s\n",
      "  training loss:\t\t0.005957\n",
      "Epoch 577 of 1000 took 0.774s\n",
      "  training loss:\t\t0.006236\n",
      "Epoch 578 of 1000 took 0.778s\n",
      "  training loss:\t\t0.006226\n",
      "Epoch 579 of 1000 took 0.780s\n",
      "  training loss:\t\t0.006589\n",
      "Epoch 580 of 1000 took 0.776s\n",
      "  training loss:\t\t0.006992\n",
      "Epoch 581 of 1000 took 0.774s\n",
      "  training loss:\t\t0.007058\n",
      "Epoch 582 of 1000 took 0.774s\n",
      "  training loss:\t\t0.007274\n",
      "Epoch 583 of 1000 took 0.772s\n",
      "  training loss:\t\t0.006246\n",
      "Epoch 584 of 1000 took 0.774s\n",
      "  training loss:\t\t0.006252\n",
      "Epoch 585 of 1000 took 0.775s\n",
      "  training loss:\t\t0.006734\n",
      "Epoch 586 of 1000 took 0.775s\n",
      "  training loss:\t\t0.007959\n",
      "Epoch 587 of 1000 took 0.772s\n",
      "  training loss:\t\t0.008466\n",
      "Epoch 588 of 1000 took 0.771s\n",
      "  training loss:\t\t0.008628\n",
      "Epoch 589 of 1000 took 0.773s\n",
      "  training loss:\t\t0.008114\n",
      "Epoch 590 of 1000 took 0.780s\n",
      "  training loss:\t\t0.007928\n",
      "Epoch 591 of 1000 took 0.772s\n",
      "  training loss:\t\t0.006332\n",
      "Epoch 592 of 1000 took 0.770s\n",
      "  training loss:\t\t0.006295\n",
      "Epoch 593 of 1000 took 0.772s\n",
      "  training loss:\t\t0.006070\n",
      "Epoch 594 of 1000 took 0.769s\n",
      "  training loss:\t\t0.006444\n",
      "Epoch 595 of 1000 took 0.773s\n",
      "  training loss:\t\t0.006925\n",
      "Epoch 596 of 1000 took 0.777s\n",
      "  training loss:\t\t0.006155\n",
      "Epoch 597 of 1000 took 0.770s\n",
      "  training loss:\t\t0.006611\n",
      "Epoch 598 of 1000 took 0.771s\n",
      "  training loss:\t\t0.007193\n",
      "Epoch 599 of 1000 took 0.768s\n",
      "  training loss:\t\t0.007362\n",
      "Epoch 600 of 1000 took 0.775s\n",
      "  training loss:\t\t0.006914\n",
      "Epoch 601 of 1000 took 0.769s\n",
      "  training loss:\t\t0.006294\n",
      "Epoch 602 of 1000 took 0.770s\n",
      "  training loss:\t\t0.005942\n",
      "Epoch 603 of 1000 took 0.776s\n",
      "  training loss:\t\t0.006342\n",
      "Epoch 604 of 1000 took 0.772s\n",
      "  training loss:\t\t0.007114\n",
      "Epoch 605 of 1000 took 0.774s\n",
      "  training loss:\t\t0.008444\n",
      "Epoch 606 of 1000 took 0.774s\n",
      "  training loss:\t\t0.007579\n",
      "Epoch 607 of 1000 took 0.772s\n",
      "  training loss:\t\t0.008228\n",
      "Epoch 608 of 1000 took 0.774s\n",
      "  training loss:\t\t0.006263\n",
      "Epoch 609 of 1000 took 0.773s\n",
      "  training loss:\t\t0.006607\n",
      "Epoch 610 of 1000 took 0.770s\n",
      "  training loss:\t\t0.006111\n",
      "Epoch 611 of 1000 took 0.771s\n",
      "  training loss:\t\t0.006705\n",
      "Epoch 612 of 1000 took 0.771s\n",
      "  training loss:\t\t0.006192\n",
      "Epoch 613 of 1000 took 0.771s\n",
      "  training loss:\t\t0.006776\n",
      "Epoch 614 of 1000 took 0.776s\n",
      "  training loss:\t\t0.006413\n",
      "Epoch 615 of 1000 took 0.772s\n",
      "  training loss:\t\t0.006646\n",
      "Epoch 616 of 1000 took 0.771s\n",
      "  training loss:\t\t0.006458\n",
      "Epoch 617 of 1000 took 0.772s\n",
      "  training loss:\t\t0.006333\n",
      "Epoch 618 of 1000 took 0.774s\n",
      "  training loss:\t\t0.006378\n",
      "Epoch 619 of 1000 took 0.777s\n",
      "  training loss:\t\t0.006635\n",
      "Epoch 620 of 1000 took 0.771s\n",
      "  training loss:\t\t0.007283\n",
      "Epoch 621 of 1000 took 0.769s\n",
      "  training loss:\t\t0.006131\n",
      "Epoch 622 of 1000 took 0.770s\n",
      "  training loss:\t\t0.006146\n",
      "Epoch 623 of 1000 took 0.768s\n",
      "  training loss:\t\t0.006456\n",
      "Epoch 624 of 1000 took 0.770s\n",
      "  training loss:\t\t0.006422\n",
      "Epoch 625 of 1000 took 0.768s\n",
      "  training loss:\t\t0.006794\n",
      "Epoch 626 of 1000 took 0.773s\n",
      "  training loss:\t\t0.005777\n",
      "Epoch 627 of 1000 took 0.781s\n",
      "  training loss:\t\t0.006096\n",
      "Epoch 628 of 1000 took 0.774s\n",
      "  training loss:\t\t0.006218\n",
      "Epoch 629 of 1000 took 0.775s\n",
      "  training loss:\t\t0.007775\n",
      "Epoch 630 of 1000 took 0.772s\n",
      "  training loss:\t\t0.006335\n",
      "Epoch 631 of 1000 took 0.768s\n",
      "  training loss:\t\t0.006199\n",
      "Epoch 632 of 1000 took 0.767s\n",
      "  training loss:\t\t0.005763\n",
      "Epoch 633 of 1000 took 0.769s\n",
      "  training loss:\t\t0.006196\n",
      "Epoch 634 of 1000 took 0.771s\n",
      "  training loss:\t\t0.005977\n",
      "Epoch 635 of 1000 took 0.773s\n",
      "  training loss:\t\t0.007459\n",
      "Epoch 636 of 1000 took 0.773s\n",
      "  training loss:\t\t0.007319\n",
      "Epoch 637 of 1000 took 0.771s\n",
      "  training loss:\t\t0.008269\n",
      "Epoch 638 of 1000 took 0.771s\n",
      "  training loss:\t\t0.007586\n",
      "Epoch 639 of 1000 took 0.772s\n",
      "  training loss:\t\t0.007113\n",
      "Epoch 640 of 1000 took 0.772s\n",
      "  training loss:\t\t0.007003\n",
      "Epoch 641 of 1000 took 0.771s\n",
      "  training loss:\t\t0.006975\n",
      "Epoch 642 of 1000 took 0.773s\n",
      "  training loss:\t\t0.005935\n",
      "Epoch 643 of 1000 took 0.775s\n",
      "  training loss:\t\t0.005576\n",
      "Epoch 644 of 1000 took 0.769s\n",
      "  training loss:\t\t0.005382\n",
      "Epoch 645 of 1000 took 0.767s\n",
      "  training loss:\t\t0.005562\n",
      "Epoch 646 of 1000 took 0.767s\n",
      "  training loss:\t\t0.005643\n",
      "Epoch 647 of 1000 took 0.767s\n",
      "  training loss:\t\t0.006460\n",
      "Epoch 648 of 1000 took 0.767s\n",
      "  training loss:\t\t0.005897\n",
      "Epoch 649 of 1000 took 0.768s\n",
      "  training loss:\t\t0.006439\n",
      "Epoch 650 of 1000 took 0.767s\n",
      "  training loss:\t\t0.005770\n",
      "Epoch 651 of 1000 took 0.768s\n",
      "  training loss:\t\t0.005845\n",
      "Epoch 652 of 1000 took 0.768s\n",
      "  training loss:\t\t0.005858\n",
      "Epoch 653 of 1000 took 0.768s\n",
      "  training loss:\t\t0.005872\n",
      "Epoch 654 of 1000 took 0.770s\n",
      "  training loss:\t\t0.006651\n",
      "Epoch 655 of 1000 took 0.774s\n",
      "  training loss:\t\t0.006712\n",
      "Epoch 656 of 1000 took 0.777s\n",
      "  training loss:\t\t0.007626\n",
      "Epoch 657 of 1000 took 0.771s\n",
      "  training loss:\t\t0.006215\n",
      "Epoch 658 of 1000 took 0.769s\n",
      "  training loss:\t\t0.006614\n",
      "Epoch 659 of 1000 took 0.771s\n",
      "  training loss:\t\t0.006403\n",
      "Epoch 660 of 1000 took 0.772s\n",
      "  training loss:\t\t0.007330\n",
      "Epoch 661 of 1000 took 0.774s\n",
      "  training loss:\t\t0.006162\n",
      "Epoch 662 of 1000 took 0.770s\n",
      "  training loss:\t\t0.006571\n",
      "Epoch 663 of 1000 took 0.772s\n",
      "  training loss:\t\t0.005863\n",
      "Epoch 664 of 1000 took 0.772s\n",
      "  training loss:\t\t0.006578\n",
      "Epoch 665 of 1000 took 0.771s\n",
      "  training loss:\t\t0.005887\n",
      "Epoch 666 of 1000 took 0.774s\n",
      "  training loss:\t\t0.006392\n",
      "Epoch 667 of 1000 took 0.771s\n",
      "  training loss:\t\t0.006232\n",
      "Epoch 668 of 1000 took 0.772s\n",
      "  training loss:\t\t0.006466\n",
      "Epoch 669 of 1000 took 0.768s\n",
      "  training loss:\t\t0.006206\n",
      "Epoch 670 of 1000 took 0.769s\n",
      "  training loss:\t\t0.006335\n",
      "Epoch 671 of 1000 took 0.772s\n",
      "  training loss:\t\t0.007314\n",
      "Epoch 672 of 1000 took 0.770s\n",
      "  training loss:\t\t0.007898\n",
      "Epoch 673 of 1000 took 0.768s\n",
      "  training loss:\t\t0.008321\n",
      "Epoch 674 of 1000 took 0.767s\n",
      "  training loss:\t\t0.006150\n",
      "Epoch 675 of 1000 took 0.768s\n",
      "  training loss:\t\t0.005445\n",
      "Epoch 676 of 1000 took 0.770s\n",
      "  training loss:\t\t0.005062\n",
      "Epoch 677 of 1000 took 0.772s\n",
      "  training loss:\t\t0.005154\n",
      "Epoch 678 of 1000 took 0.770s\n",
      "  training loss:\t\t0.005404\n",
      "Epoch 679 of 1000 took 0.768s\n",
      "  training loss:\t\t0.006346\n",
      "Epoch 680 of 1000 took 0.766s\n",
      "  training loss:\t\t0.006135\n",
      "Epoch 681 of 1000 took 0.766s\n",
      "  training loss:\t\t0.007021\n",
      "Epoch 682 of 1000 took 0.767s\n",
      "  training loss:\t\t0.005495\n",
      "Epoch 683 of 1000 took 0.768s\n",
      "  training loss:\t\t0.006007\n",
      "Epoch 684 of 1000 took 0.770s\n",
      "  training loss:\t\t0.005959\n",
      "Epoch 685 of 1000 took 0.768s\n",
      "  training loss:\t\t0.006277\n",
      "Epoch 686 of 1000 took 0.766s\n",
      "  training loss:\t\t0.006327\n",
      "Epoch 687 of 1000 took 0.768s\n",
      "  training loss:\t\t0.005985\n",
      "Epoch 688 of 1000 took 0.767s\n",
      "  training loss:\t\t0.006666\n",
      "Epoch 689 of 1000 took 0.766s\n",
      "  training loss:\t\t0.006129\n",
      "Epoch 690 of 1000 took 0.766s\n",
      "  training loss:\t\t0.006755\n",
      "Epoch 691 of 1000 took 0.771s\n",
      "  training loss:\t\t0.006039\n",
      "Epoch 692 of 1000 took 0.770s\n",
      "  training loss:\t\t0.006377\n",
      "Epoch 693 of 1000 took 0.768s\n",
      "  training loss:\t\t0.005673\n",
      "Epoch 694 of 1000 took 0.769s\n",
      "  training loss:\t\t0.005727\n",
      "Epoch 695 of 1000 took 0.778s\n",
      "  training loss:\t\t0.005295\n",
      "Epoch 696 of 1000 took 0.782s\n",
      "  training loss:\t\t0.005408\n",
      "Epoch 697 of 1000 took 0.775s\n",
      "  training loss:\t\t0.005285\n",
      "Epoch 698 of 1000 took 0.778s\n",
      "  training loss:\t\t0.005573\n",
      "Epoch 699 of 1000 took 0.776s\n",
      "  training loss:\t\t0.005778\n",
      "Epoch 700 of 1000 took 0.773s\n",
      "  training loss:\t\t0.006131\n",
      "Epoch 701 of 1000 took 0.774s\n",
      "  training loss:\t\t0.006634\n",
      "Epoch 702 of 1000 took 0.775s\n",
      "  training loss:\t\t0.005626\n",
      "Epoch 703 of 1000 took 0.775s\n",
      "  training loss:\t\t0.005392\n",
      "Epoch 704 of 1000 took 0.779s\n",
      "  training loss:\t\t0.005143\n",
      "Epoch 705 of 1000 took 0.776s\n",
      "  training loss:\t\t0.005325\n",
      "Epoch 706 of 1000 took 0.777s\n",
      "  training loss:\t\t0.005361\n",
      "Epoch 707 of 1000 took 0.775s\n",
      "  training loss:\t\t0.006021\n",
      "Epoch 708 of 1000 took 0.780s\n",
      "  training loss:\t\t0.005964\n",
      "Epoch 709 of 1000 took 0.778s\n",
      "  training loss:\t\t0.006838\n",
      "Epoch 710 of 1000 took 0.776s\n",
      "  training loss:\t\t0.006523\n",
      "Epoch 711 of 1000 took 0.775s\n",
      "  training loss:\t\t0.007496\n",
      "Epoch 712 of 1000 took 0.778s\n",
      "  training loss:\t\t0.007781\n",
      "Epoch 713 of 1000 took 0.774s\n",
      "  training loss:\t\t0.007036\n",
      "Epoch 714 of 1000 took 0.777s\n",
      "  training loss:\t\t0.008035\n",
      "Epoch 715 of 1000 took 0.774s\n",
      "  training loss:\t\t0.006774\n",
      "Epoch 716 of 1000 took 0.768s\n",
      "  training loss:\t\t0.007389\n",
      "Epoch 717 of 1000 took 0.768s\n",
      "  training loss:\t\t0.006015\n",
      "Epoch 718 of 1000 took 0.766s\n",
      "  training loss:\t\t0.005315\n",
      "Epoch 719 of 1000 took 0.768s\n",
      "  training loss:\t\t0.005415\n",
      "Epoch 720 of 1000 took 0.769s\n",
      "  training loss:\t\t0.005638\n",
      "Epoch 721 of 1000 took 0.772s\n",
      "  training loss:\t\t0.006149\n",
      "Epoch 722 of 1000 took 0.771s\n",
      "  training loss:\t\t0.007122\n",
      "Epoch 723 of 1000 took 0.773s\n",
      "  training loss:\t\t0.006382\n",
      "Epoch 724 of 1000 took 0.773s\n",
      "  training loss:\t\t0.006024\n",
      "Epoch 725 of 1000 took 0.773s\n",
      "  training loss:\t\t0.006891\n",
      "Epoch 726 of 1000 took 0.774s\n",
      "  training loss:\t\t0.006532\n",
      "Epoch 727 of 1000 took 0.771s\n",
      "  training loss:\t\t0.005726\n",
      "Epoch 728 of 1000 took 0.767s\n",
      "  training loss:\t\t0.005325\n",
      "Epoch 729 of 1000 took 0.771s\n",
      "  training loss:\t\t0.005988\n",
      "Epoch 730 of 1000 took 0.768s\n",
      "  training loss:\t\t0.005840\n",
      "Epoch 731 of 1000 took 0.778s\n",
      "  training loss:\t\t0.006271\n",
      "Epoch 732 of 1000 took 0.771s\n",
      "  training loss:\t\t0.005386\n",
      "Epoch 733 of 1000 took 0.771s\n",
      "  training loss:\t\t0.005549\n",
      "Epoch 734 of 1000 took 0.772s\n",
      "  training loss:\t\t0.005376\n",
      "Epoch 735 of 1000 took 0.774s\n",
      "  training loss:\t\t0.006361\n",
      "Epoch 736 of 1000 took 0.772s\n",
      "  training loss:\t\t0.005727\n",
      "Epoch 737 of 1000 took 0.771s\n",
      "  training loss:\t\t0.006015\n",
      "Epoch 738 of 1000 took 0.774s\n",
      "  training loss:\t\t0.005909\n",
      "Epoch 739 of 1000 took 0.774s\n",
      "  training loss:\t\t0.005706\n",
      "Epoch 740 of 1000 took 0.770s\n",
      "  training loss:\t\t0.006541\n",
      "Epoch 741 of 1000 took 0.766s\n",
      "  training loss:\t\t0.006053\n",
      "Epoch 742 of 1000 took 0.766s\n",
      "  training loss:\t\t0.006765\n",
      "Epoch 743 of 1000 took 0.766s\n",
      "  training loss:\t\t0.005675\n",
      "Epoch 744 of 1000 took 0.772s\n",
      "  training loss:\t\t0.005874\n",
      "Epoch 745 of 1000 took 0.770s\n",
      "  training loss:\t\t0.005538\n",
      "Epoch 746 of 1000 took 0.775s\n",
      "  training loss:\t\t0.005777\n",
      "Epoch 747 of 1000 took 0.770s\n",
      "  training loss:\t\t0.005395\n",
      "Epoch 748 of 1000 took 0.771s\n",
      "  training loss:\t\t0.005804\n",
      "Epoch 749 of 1000 took 0.774s\n",
      "  training loss:\t\t0.005468\n",
      "Epoch 750 of 1000 took 0.772s\n",
      "  training loss:\t\t0.006062\n",
      "Epoch 751 of 1000 took 0.773s\n",
      "  training loss:\t\t0.005999\n",
      "Epoch 752 of 1000 took 0.776s\n",
      "  training loss:\t\t0.006297\n",
      "Epoch 753 of 1000 took 0.781s\n",
      "  training loss:\t\t0.006303\n",
      "Epoch 754 of 1000 took 0.773s\n",
      "  training loss:\t\t0.005617\n",
      "Epoch 755 of 1000 took 0.774s\n",
      "  training loss:\t\t0.005752\n",
      "Epoch 756 of 1000 took 0.769s\n",
      "  training loss:\t\t0.005434\n",
      "Epoch 757 of 1000 took 0.771s\n",
      "  training loss:\t\t0.005924\n",
      "Epoch 758 of 1000 took 0.769s\n",
      "  training loss:\t\t0.005444\n",
      "Epoch 759 of 1000 took 0.767s\n",
      "  training loss:\t\t0.005941\n",
      "Epoch 760 of 1000 took 0.769s\n",
      "  training loss:\t\t0.005297\n",
      "Epoch 761 of 1000 took 0.772s\n",
      "  training loss:\t\t0.005812\n",
      "Epoch 762 of 1000 took 0.770s\n",
      "  training loss:\t\t0.005184\n",
      "Epoch 763 of 1000 took 0.773s\n",
      "  training loss:\t\t0.005700\n",
      "Epoch 764 of 1000 took 0.779s\n",
      "  training loss:\t\t0.005363\n",
      "Epoch 765 of 1000 took 0.773s\n",
      "  training loss:\t\t0.006022\n",
      "Epoch 766 of 1000 took 0.773s\n",
      "  training loss:\t\t0.006018\n",
      "Epoch 767 of 1000 took 0.776s\n",
      "  training loss:\t\t0.006760\n",
      "Epoch 768 of 1000 took 0.779s\n",
      "  training loss:\t\t0.006335\n",
      "Epoch 769 of 1000 took 0.779s\n",
      "  training loss:\t\t0.007110\n",
      "Epoch 770 of 1000 took 0.778s\n",
      "  training loss:\t\t0.006080\n",
      "Epoch 771 of 1000 took 0.775s\n",
      "  training loss:\t\t0.005753\n",
      "Epoch 772 of 1000 took 0.772s\n",
      "  training loss:\t\t0.005049\n",
      "Epoch 773 of 1000 took 0.777s\n",
      "  training loss:\t\t0.005373\n",
      "Epoch 774 of 1000 took 0.777s\n",
      "  training loss:\t\t0.005374\n",
      "Epoch 775 of 1000 took 0.774s\n",
      "  training loss:\t\t0.006141\n",
      "Epoch 776 of 1000 took 0.771s\n",
      "  training loss:\t\t0.006282\n",
      "Epoch 777 of 1000 took 0.775s\n",
      "  training loss:\t\t0.007055\n",
      "Epoch 778 of 1000 took 0.774s\n",
      "  training loss:\t\t0.005290\n",
      "Epoch 779 of 1000 took 0.769s\n",
      "  training loss:\t\t0.005101\n",
      "Epoch 780 of 1000 took 0.770s\n",
      "  training loss:\t\t0.005199\n",
      "Epoch 781 of 1000 took 0.775s\n",
      "  training loss:\t\t0.005535\n",
      "Epoch 782 of 1000 took 0.772s\n",
      "  training loss:\t\t0.006207\n",
      "Epoch 783 of 1000 took 0.772s\n",
      "  training loss:\t\t0.006138\n",
      "Epoch 784 of 1000 took 0.771s\n",
      "  training loss:\t\t0.006988\n",
      "Epoch 785 of 1000 took 0.775s\n",
      "  training loss:\t\t0.005738\n",
      "Epoch 786 of 1000 took 0.775s\n",
      "  training loss:\t\t0.005843\n",
      "Epoch 787 of 1000 took 0.774s\n",
      "  training loss:\t\t0.005197\n",
      "Epoch 788 of 1000 took 0.773s\n",
      "  training loss:\t\t0.005281\n",
      "Epoch 789 of 1000 took 0.774s\n",
      "  training loss:\t\t0.004980\n",
      "Epoch 790 of 1000 took 0.773s\n",
      "  training loss:\t\t0.005309\n",
      "Epoch 791 of 1000 took 0.771s\n",
      "  training loss:\t\t0.005068\n",
      "Epoch 792 of 1000 took 0.770s\n",
      "  training loss:\t\t0.005647\n",
      "Epoch 793 of 1000 took 0.769s\n",
      "  training loss:\t\t0.005303\n",
      "Epoch 794 of 1000 took 0.771s\n",
      "  training loss:\t\t0.006286\n",
      "Epoch 795 of 1000 took 0.773s\n",
      "  training loss:\t\t0.005026\n",
      "Epoch 796 of 1000 took 0.779s\n",
      "  training loss:\t\t0.005350\n",
      "Epoch 797 of 1000 took 0.773s\n",
      "  training loss:\t\t0.004987\n",
      "Epoch 798 of 1000 took 0.770s\n",
      "  training loss:\t\t0.005262\n",
      "Epoch 799 of 1000 took 0.776s\n",
      "  training loss:\t\t0.005345\n",
      "Epoch 800 of 1000 took 0.774s\n",
      "  training loss:\t\t0.006061\n",
      "Epoch 801 of 1000 took 0.773s\n",
      "  training loss:\t\t0.007247\n",
      "Epoch 802 of 1000 took 0.774s\n",
      "  training loss:\t\t0.006386\n",
      "Epoch 803 of 1000 took 0.773s\n",
      "  training loss:\t\t0.006724\n",
      "Epoch 804 of 1000 took 0.773s\n",
      "  training loss:\t\t0.005395\n",
      "Epoch 805 of 1000 took 0.780s\n",
      "  training loss:\t\t0.005415\n",
      "Epoch 806 of 1000 took 0.774s\n",
      "  training loss:\t\t0.005262\n",
      "Epoch 807 of 1000 took 0.769s\n",
      "  training loss:\t\t0.005783\n",
      "Epoch 808 of 1000 took 0.773s\n",
      "  training loss:\t\t0.005944\n",
      "Epoch 809 of 1000 took 0.771s\n",
      "  training loss:\t\t0.006262\n",
      "Epoch 810 of 1000 took 0.772s\n",
      "  training loss:\t\t0.006147\n",
      "Epoch 811 of 1000 took 0.772s\n",
      "  training loss:\t\t0.005364\n",
      "Epoch 812 of 1000 took 0.773s\n",
      "  training loss:\t\t0.005067\n",
      "Epoch 813 of 1000 took 0.771s\n",
      "  training loss:\t\t0.005042\n",
      "Epoch 814 of 1000 took 0.778s\n",
      "  training loss:\t\t0.004931\n",
      "Epoch 815 of 1000 took 0.776s\n",
      "  training loss:\t\t0.005308\n",
      "Epoch 816 of 1000 took 0.774s\n",
      "  training loss:\t\t0.005373\n",
      "Epoch 817 of 1000 took 0.770s\n",
      "  training loss:\t\t0.006234\n",
      "Epoch 818 of 1000 took 0.770s\n",
      "  training loss:\t\t0.006205\n",
      "Epoch 819 of 1000 took 0.775s\n",
      "  training loss:\t\t0.005521\n",
      "Epoch 820 of 1000 took 0.769s\n",
      "  training loss:\t\t0.006554\n",
      "Epoch 821 of 1000 took 0.771s\n",
      "  training loss:\t\t0.005822\n",
      "Epoch 822 of 1000 took 0.776s\n",
      "  training loss:\t\t0.006213\n",
      "Epoch 823 of 1000 took 0.776s\n",
      "  training loss:\t\t0.005350\n",
      "Epoch 824 of 1000 took 0.780s\n",
      "  training loss:\t\t0.005255\n",
      "Epoch 825 of 1000 took 0.779s\n",
      "  training loss:\t\t0.004898\n",
      "Epoch 826 of 1000 took 0.774s\n",
      "  training loss:\t\t0.005004\n",
      "Epoch 827 of 1000 took 0.773s\n",
      "  training loss:\t\t0.004995\n",
      "Epoch 828 of 1000 took 0.769s\n",
      "  training loss:\t\t0.005311\n",
      "Epoch 829 of 1000 took 0.771s\n",
      "  training loss:\t\t0.005577\n",
      "Epoch 830 of 1000 took 0.779s\n",
      "  training loss:\t\t0.005748\n",
      "Epoch 831 of 1000 took 0.772s\n",
      "  training loss:\t\t0.005668\n",
      "Epoch 832 of 1000 took 0.780s\n",
      "  training loss:\t\t0.005838\n",
      "Epoch 833 of 1000 took 0.749s\n",
      "  training loss:\t\t0.005284\n",
      "Epoch 834 of 1000 took 0.749s\n",
      "  training loss:\t\t0.005363\n",
      "Epoch 835 of 1000 took 0.744s\n",
      "  training loss:\t\t0.005535\n",
      "Epoch 836 of 1000 took 0.742s\n",
      "  training loss:\t\t0.005458\n",
      "Epoch 837 of 1000 took 0.760s\n",
      "  training loss:\t\t0.005645\n",
      "Epoch 838 of 1000 took 0.761s\n",
      "  training loss:\t\t0.005773\n",
      "Epoch 839 of 1000 took 0.752s\n",
      "  training loss:\t\t0.005809\n",
      "Epoch 840 of 1000 took 0.775s\n",
      "  training loss:\t\t0.005677\n",
      "Epoch 841 of 1000 took 0.748s\n",
      "  training loss:\t\t0.007036\n",
      "Epoch 842 of 1000 took 0.766s\n",
      "  training loss:\t\t0.006079\n",
      "Epoch 843 of 1000 took 0.759s\n",
      "  training loss:\t\t0.007100\n",
      "Epoch 844 of 1000 took 0.760s\n",
      "  training loss:\t\t0.006695\n",
      "Epoch 845 of 1000 took 0.740s\n",
      "  training loss:\t\t0.007615\n",
      "Epoch 846 of 1000 took 0.778s\n",
      "  training loss:\t\t0.004800\n",
      "Epoch 847 of 1000 took 0.754s\n",
      "  training loss:\t\t0.004465\n",
      "Epoch 848 of 1000 took 0.737s\n",
      "  training loss:\t\t0.004501\n",
      "Epoch 849 of 1000 took 0.740s\n",
      "  training loss:\t\t0.004836\n",
      "Epoch 850 of 1000 took 0.736s\n",
      "  training loss:\t\t0.005337\n",
      "Epoch 851 of 1000 took 0.734s\n",
      "  training loss:\t\t0.006269\n",
      "Epoch 852 of 1000 took 0.746s\n",
      "  training loss:\t\t0.006062\n",
      "Epoch 853 of 1000 took 0.775s\n",
      "  training loss:\t\t0.005645\n",
      "Epoch 854 of 1000 took 0.749s\n",
      "  training loss:\t\t0.006256\n",
      "Epoch 855 of 1000 took 0.735s\n",
      "  training loss:\t\t0.005671\n",
      "Epoch 856 of 1000 took 0.735s\n",
      "  training loss:\t\t0.005789\n",
      "Epoch 857 of 1000 took 0.736s\n",
      "  training loss:\t\t0.005384\n",
      "Epoch 858 of 1000 took 0.752s\n",
      "  training loss:\t\t0.005683\n",
      "Epoch 859 of 1000 took 0.744s\n",
      "  training loss:\t\t0.004847\n",
      "Epoch 860 of 1000 took 0.752s\n",
      "  training loss:\t\t0.005057\n",
      "Epoch 861 of 1000 took 0.749s\n",
      "  training loss:\t\t0.004783\n",
      "Epoch 862 of 1000 took 0.752s\n",
      "  training loss:\t\t0.005363\n",
      "Epoch 863 of 1000 took 0.770s\n",
      "  training loss:\t\t0.005463\n",
      "Epoch 864 of 1000 took 0.766s\n",
      "  training loss:\t\t0.005764\n",
      "Epoch 865 of 1000 took 0.765s\n",
      "  training loss:\t\t0.006210\n",
      "Epoch 866 of 1000 took 0.767s\n",
      "  training loss:\t\t0.005664\n",
      "Epoch 867 of 1000 took 0.762s\n",
      "  training loss:\t\t0.006171\n",
      "Epoch 868 of 1000 took 0.758s\n",
      "  training loss:\t\t0.005609\n",
      "Epoch 869 of 1000 took 0.768s\n",
      "  training loss:\t\t0.005712\n",
      "Epoch 870 of 1000 took 0.784s\n",
      "  training loss:\t\t0.005160\n",
      "Epoch 871 of 1000 took 0.784s\n",
      "  training loss:\t\t0.005119\n",
      "Epoch 872 of 1000 took 0.784s\n",
      "  training loss:\t\t0.004945\n",
      "Epoch 873 of 1000 took 0.785s\n",
      "  training loss:\t\t0.005156\n",
      "Epoch 874 of 1000 took 0.782s\n",
      "  training loss:\t\t0.005522\n",
      "Epoch 875 of 1000 took 0.781s\n",
      "  training loss:\t\t0.005665\n",
      "Epoch 876 of 1000 took 0.780s\n",
      "  training loss:\t\t0.005926\n",
      "Epoch 877 of 1000 took 0.781s\n",
      "  training loss:\t\t0.004995\n",
      "Epoch 878 of 1000 took 0.783s\n",
      "  training loss:\t\t0.004987\n",
      "Epoch 879 of 1000 took 0.778s\n",
      "  training loss:\t\t0.004833\n",
      "Epoch 880 of 1000 took 0.774s\n",
      "  training loss:\t\t0.005570\n",
      "Epoch 881 of 1000 took 0.772s\n",
      "  training loss:\t\t0.005732\n",
      "Epoch 882 of 1000 took 0.775s\n",
      "  training loss:\t\t0.006607\n",
      "Epoch 883 of 1000 took 0.775s\n",
      "  training loss:\t\t0.005382\n",
      "Epoch 884 of 1000 took 0.779s\n",
      "  training loss:\t\t0.005598\n",
      "Epoch 885 of 1000 took 0.778s\n",
      "  training loss:\t\t0.005519\n",
      "Epoch 886 of 1000 took 0.774s\n",
      "  training loss:\t\t0.005974\n",
      "Epoch 887 of 1000 took 0.777s\n",
      "  training loss:\t\t0.005481\n",
      "Epoch 888 of 1000 took 0.776s\n",
      "  training loss:\t\t0.005511\n",
      "Epoch 889 of 1000 took 0.774s\n",
      "  training loss:\t\t0.004946\n",
      "Epoch 890 of 1000 took 0.765s\n",
      "  training loss:\t\t0.005082\n",
      "Epoch 891 of 1000 took 0.772s\n",
      "  training loss:\t\t0.005054\n",
      "Epoch 892 of 1000 took 0.759s\n",
      "  training loss:\t\t0.005475\n",
      "Epoch 893 of 1000 took 0.746s\n",
      "  training loss:\t\t0.005671\n",
      "Epoch 894 of 1000 took 0.758s\n",
      "  training loss:\t\t0.004942\n",
      "Epoch 895 of 1000 took 0.769s\n",
      "  training loss:\t\t0.004718\n",
      "Epoch 896 of 1000 took 0.760s\n",
      "  training loss:\t\t0.004612\n",
      "Epoch 897 of 1000 took 0.772s\n",
      "  training loss:\t\t0.004744\n",
      "Epoch 898 of 1000 took 0.766s\n",
      "  training loss:\t\t0.004941\n",
      "Epoch 899 of 1000 took 0.765s\n",
      "  training loss:\t\t0.005899\n",
      "Epoch 900 of 1000 took 0.734s\n",
      "  training loss:\t\t0.005378\n",
      "Epoch 901 of 1000 took 0.742s\n",
      "  training loss:\t\t0.005517\n",
      "Epoch 902 of 1000 took 0.762s\n",
      "  training loss:\t\t0.005222\n",
      "Epoch 903 of 1000 took 0.772s\n",
      "  training loss:\t\t0.004797\n",
      "Epoch 904 of 1000 took 0.745s\n",
      "  training loss:\t\t0.004766\n",
      "Epoch 905 of 1000 took 0.753s\n",
      "  training loss:\t\t0.004693\n",
      "Epoch 906 of 1000 took 0.781s\n",
      "  training loss:\t\t0.004840\n",
      "Epoch 907 of 1000 took 0.737s\n",
      "  training loss:\t\t0.005513\n",
      "Epoch 908 of 1000 took 0.737s\n",
      "  training loss:\t\t0.006767\n",
      "Epoch 909 of 1000 took 0.736s\n",
      "  training loss:\t\t0.009381\n",
      "Epoch 910 of 1000 took 0.753s\n",
      "  training loss:\t\t0.007878\n",
      "Epoch 911 of 1000 took 0.736s\n",
      "  training loss:\t\t0.008281\n",
      "Epoch 912 of 1000 took 0.734s\n",
      "  training loss:\t\t0.005577\n",
      "Epoch 913 of 1000 took 0.735s\n",
      "  training loss:\t\t0.005158\n",
      "Epoch 914 of 1000 took 0.735s\n",
      "  training loss:\t\t0.005348\n",
      "Epoch 915 of 1000 took 0.778s\n",
      "  training loss:\t\t0.005426\n",
      "Epoch 916 of 1000 took 0.736s\n",
      "  training loss:\t\t0.004875\n",
      "Epoch 917 of 1000 took 0.756s\n",
      "  training loss:\t\t0.004601\n",
      "Epoch 918 of 1000 took 0.734s\n",
      "  training loss:\t\t0.004415\n",
      "Epoch 919 of 1000 took 0.735s\n",
      "  training loss:\t\t0.004656\n",
      "Epoch 920 of 1000 took 0.734s\n",
      "  training loss:\t\t0.005386\n",
      "Epoch 921 of 1000 took 0.742s\n",
      "  training loss:\t\t0.007509\n",
      "Epoch 922 of 1000 took 0.733s\n",
      "  training loss:\t\t0.006662\n",
      "Epoch 923 of 1000 took 0.733s\n",
      "  training loss:\t\t0.006118\n",
      "Epoch 924 of 1000 took 0.738s\n",
      "  training loss:\t\t0.005181\n",
      "Epoch 925 of 1000 took 0.755s\n",
      "  training loss:\t\t0.005243\n",
      "Epoch 926 of 1000 took 0.751s\n",
      "  training loss:\t\t0.004907\n",
      "Epoch 927 of 1000 took 0.748s\n",
      "  training loss:\t\t0.005337\n",
      "Epoch 928 of 1000 took 0.738s\n",
      "  training loss:\t\t0.004941\n",
      "Epoch 929 of 1000 took 0.738s\n",
      "  training loss:\t\t0.005481\n",
      "Epoch 930 of 1000 took 0.737s\n",
      "  training loss:\t\t0.004747\n",
      "Epoch 931 of 1000 took 0.738s\n",
      "  training loss:\t\t0.005200\n",
      "Epoch 932 of 1000 took 0.741s\n",
      "  training loss:\t\t0.004645\n",
      "Epoch 933 of 1000 took 0.739s\n",
      "  training loss:\t\t0.005022\n",
      "Epoch 934 of 1000 took 0.737s\n",
      "  training loss:\t\t0.004527\n",
      "Epoch 935 of 1000 took 0.736s\n",
      "  training loss:\t\t0.004792\n",
      "Epoch 936 of 1000 took 0.741s\n",
      "  training loss:\t\t0.004422\n",
      "Epoch 937 of 1000 took 0.737s\n",
      "  training loss:\t\t0.004638\n",
      "Epoch 938 of 1000 took 0.739s\n",
      "  training loss:\t\t0.004390\n",
      "Epoch 939 of 1000 took 0.740s\n",
      "  training loss:\t\t0.004694\n",
      "Epoch 940 of 1000 took 0.740s\n",
      "  training loss:\t\t0.004685\n",
      "Epoch 941 of 1000 took 0.738s\n",
      "  training loss:\t\t0.005485\n",
      "Epoch 942 of 1000 took 0.740s\n",
      "  training loss:\t\t0.005946\n",
      "Epoch 943 of 1000 took 0.740s\n",
      "  training loss:\t\t0.006938\n",
      "Epoch 944 of 1000 took 0.739s\n",
      "  training loss:\t\t0.005747\n",
      "Epoch 945 of 1000 took 0.743s\n",
      "  training loss:\t\t0.005619\n",
      "Epoch 946 of 1000 took 0.740s\n",
      "  training loss:\t\t0.005111\n",
      "Epoch 947 of 1000 took 0.735s\n",
      "  training loss:\t\t0.005171\n",
      "Epoch 948 of 1000 took 0.738s\n",
      "  training loss:\t\t0.005251\n",
      "Epoch 949 of 1000 took 0.738s\n",
      "  training loss:\t\t0.005689\n",
      "Epoch 950 of 1000 took 0.741s\n",
      "  training loss:\t\t0.004797\n",
      "Epoch 951 of 1000 took 0.744s\n",
      "  training loss:\t\t0.004995\n",
      "Epoch 952 of 1000 took 0.739s\n",
      "  training loss:\t\t0.004755\n",
      "Epoch 953 of 1000 took 0.744s\n",
      "  training loss:\t\t0.005242\n",
      "Epoch 954 of 1000 took 0.737s\n",
      "  training loss:\t\t0.004935\n",
      "Epoch 955 of 1000 took 0.739s\n",
      "  training loss:\t\t0.005094\n",
      "Epoch 956 of 1000 took 0.739s\n",
      "  training loss:\t\t0.004906\n",
      "Epoch 957 of 1000 took 0.737s\n",
      "  training loss:\t\t0.004937\n",
      "Epoch 958 of 1000 took 0.736s\n",
      "  training loss:\t\t0.005209\n",
      "Epoch 959 of 1000 took 0.735s\n",
      "  training loss:\t\t0.005415\n",
      "Epoch 960 of 1000 took 0.736s\n",
      "  training loss:\t\t0.006503\n",
      "Epoch 961 of 1000 took 0.739s\n",
      "  training loss:\t\t0.005290\n",
      "Epoch 962 of 1000 took 0.741s\n",
      "  training loss:\t\t0.005771\n",
      "Epoch 963 of 1000 took 0.739s\n",
      "  training loss:\t\t0.005328\n",
      "Epoch 964 of 1000 took 0.739s\n",
      "  training loss:\t\t0.005958\n",
      "Epoch 965 of 1000 took 0.738s\n",
      "  training loss:\t\t0.004983\n",
      "Epoch 966 of 1000 took 0.735s\n",
      "  training loss:\t\t0.005215\n",
      "Epoch 967 of 1000 took 0.736s\n",
      "  training loss:\t\t0.004514\n",
      "Epoch 968 of 1000 took 0.741s\n",
      "  training loss:\t\t0.004731\n",
      "Epoch 969 of 1000 took 0.740s\n",
      "  training loss:\t\t0.004401\n",
      "Epoch 970 of 1000 took 0.740s\n",
      "  training loss:\t\t0.004714\n",
      "Epoch 971 of 1000 took 0.741s\n",
      "  training loss:\t\t0.004462\n",
      "Epoch 972 of 1000 took 0.786s\n",
      "  training loss:\t\t0.004765\n",
      "Epoch 973 of 1000 took 0.768s\n",
      "  training loss:\t\t0.004565\n",
      "Epoch 974 of 1000 took 0.748s\n",
      "  training loss:\t\t0.004854\n",
      "Epoch 975 of 1000 took 0.775s\n",
      "  training loss:\t\t0.005023\n",
      "Epoch 976 of 1000 took 0.748s\n",
      "  training loss:\t\t0.005525\n",
      "Epoch 977 of 1000 took 0.736s\n",
      "  training loss:\t\t0.006308\n",
      "Epoch 978 of 1000 took 0.733s\n",
      "  training loss:\t\t0.005654\n",
      "Epoch 979 of 1000 took 0.736s\n",
      "  training loss:\t\t0.005145\n",
      "Epoch 980 of 1000 took 0.733s\n",
      "  training loss:\t\t0.006284\n",
      "Epoch 981 of 1000 took 0.744s\n",
      "  training loss:\t\t0.007554\n",
      "Epoch 982 of 1000 took 0.755s\n",
      "  training loss:\t\t0.006279\n",
      "Epoch 983 of 1000 took 0.748s\n",
      "  training loss:\t\t0.004984\n",
      "Epoch 984 of 1000 took 0.739s\n",
      "  training loss:\t\t0.005145\n",
      "Epoch 985 of 1000 took 0.745s\n",
      "  training loss:\t\t0.004740\n",
      "Epoch 986 of 1000 took 0.735s\n",
      "  training loss:\t\t0.004808\n",
      "Epoch 987 of 1000 took 0.739s\n",
      "  training loss:\t\t0.004549\n",
      "Epoch 988 of 1000 took 0.740s\n",
      "  training loss:\t\t0.004571\n",
      "Epoch 989 of 1000 took 0.762s\n",
      "  training loss:\t\t0.004542\n",
      "Epoch 990 of 1000 took 0.766s\n",
      "  training loss:\t\t0.004696\n",
      "Epoch 991 of 1000 took 0.763s\n",
      "  training loss:\t\t0.004957\n",
      "Epoch 992 of 1000 took 0.772s\n",
      "  training loss:\t\t0.005327\n",
      "Epoch 993 of 1000 took 0.754s\n",
      "  training loss:\t\t0.005283\n",
      "Epoch 994 of 1000 took 0.747s\n",
      "  training loss:\t\t0.005408\n",
      "Epoch 995 of 1000 took 0.746s\n",
      "  training loss:\t\t0.005164\n",
      "Epoch 996 of 1000 took 0.749s\n",
      "  training loss:\t\t0.004884\n",
      "Epoch 997 of 1000 took 0.748s\n",
      "  training loss:\t\t0.005172\n",
      "Epoch 998 of 1000 took 0.739s\n",
      "  training loss:\t\t0.005985\n",
      "Epoch 999 of 1000 took 0.738s\n",
      "  training loss:\t\t0.004866\n",
      "Epoch 1000 of 1000 took 0.738s\n",
      "  training loss:\t\t0.004922\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.matrix('targets')\n",
    "learnrate=0.01\n",
    "# Create neural network model (depending on first command line parameter)\n",
    "print(\"Building model and compiling functions...\")\n",
    "network = build_cnn(input_var)\n",
    "reconstructed = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.squared_error(reconstructed, target_var)\n",
    "loss = loss.mean()\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "# updates = lasagne.updates.nesterov_momentum(\n",
    "#     loss, params, learning_rate=learnrate, momentum=0.975)\n",
    "updates = lasagne.updates.rmsprop(loss, params, learning_rate=learnrate)\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates,on_unused_input='warn')\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(train_face, train_face_out, 200, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    # Optionally, you could now dump the network weights to a file like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_func = theano.function([input_var], [lasagne.layers.get_output(network)])\n",
    "data_pred = np.zeros((200,4096))\n",
    "i = 0\n",
    "for batch in iterate_minibatches(train_face, train_face_out, 200, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    data_pred[1000*i:1000*(i+1),:] = output_func(inputs)[0]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAMo0lEQVR4nM3byZLjyHIF0BNAACCT\n2dWDBnsLLfSp+lTtNJmsX1UlSczQIjzIbP1AMmjMmUnEDR+uX3ekfzM4GZz0Bp1Oq9VqNJIkKev4\nf4/d4UCS4m/FM2nimeKVO3a7zWxy82H03WJ2aHzxyklrcPamd9LJGm3s6vPVHdjjq/rguePytUAt\nffr5ZwQ6ncbhiN/tL4BAo9Ub4tk9zm53WONzPd8DjYMHAumx64pIWeV3uxT7hUar02vtVhyWl0Ag\nxZV1sk7+ZPf1XHcJjScC9ZEetpLi+2oDZRVvKLZTMDk0NoNBcthtL4AAjSxrZQ3W8PFivclm0zhi\nn1mn0YRlH+EbyW6LV+7xjwtarUFDYJglvbOk8ZvG+BIIpIh69aw2u83usNmw2rTIWi2BhthtOfVy\nmpvDZrXb7VarRuNdjnfIaOzIzi4WnfkFEMhh/eIUxwcCi8lmMmlszk5O3pwjX1DOuXj0anI3GX2Y\nzRaLRafzh3O8ZtDJLlqt3rvV95dAoIkzOqwWm1tY9Obmw+pu1Fq9e/fmm1nvpJcUj9mwW9x9uLv6\n4W4yma2y3ujiV4PBWR9+MCDrI/N88cptxIBiuYspEFjc/HR3dZfs7kbvNpve6qR/2MFhsZmMrj58\nd7NY7OiIHe5qLCjRJTmZndxfAIHi3zX/HeEPrQ5L+C2bXtYq/r9awreLBxx2s9lottklWSN71/tF\nb4iz3m0me0SUbIg4+aUrF/5X8lej14c9cHOymKyS2aALzlDyXTbIWK1S7Klwh6yRvDn7TecP2Vmj\nvs8mWdDK+pdBoLKdFMygZLI3F6vZajM6JK2sk2TZoNcrCBwWnf7xX3q9i3f/qPO7Vq+NnR4mh01y\nBBN7BQSeLDhrdbLdIceTw+weWbHUD28GWRcRbZOMWpPvvvuw653sePNm1unjPUrcpHCDxttL5IJq\nnSn4LuxWu8nf3VxNJjebXevszbvsEhzi+MQKd4vR3Wo22dzN3tyd/B6YFaT3B29qXiQXNJ+idOEE\nk0l29x/+9MNif/DDk282nV8lhxSYrRazXfeIqVtExs5PZ6PfXZx1wbHbBz/sXwKBejKFE01mV6PO\nd//uv/10OHkzxCmXaO/BAo8401LrprCMReNutRqc/It/9odfXQxoDRBx5esRyI/avnCcu9GHNTjS\nZJF0fvVNNmtcvEemZOZRRTQG3+yyBQz+1w+j1ebqQxOZotc7ybLN8hI20Dyqvcpyd63ObwY/Lc6S\nf/BPLhqTzje/eXMxBGfYHQaNX61aSTI5NM7WiPZnf/ObsxM2s2wLu2sfOeILV86yageHHNbd+92q\n9e5q9Yt3g92bN99cIrrnYIXJSec3vLk4ubnLfnE2m1xc/KtBo9XotaFDZHvoJV+8ghUXtpfCojed\ns83vmNx12rCSZ/Q+PukIWTIYQlUpXLhz8Y5f9P6IPNA+PpZ3fB0bKLVe0iFLEf2XULV6OXJB46LX\n2Mxai2Q36YI79A5nZ4t3q9aok1yCRw2hQJLCIlLUZF+8clVFj9Bzap24WrXOepNEaBlVRWseuFVG\n1RskKX624aq1yQ6jIyqFqrdVLal5AQSqJlz1wEbrCG3oglbvWdclnd5qiCh+hOrZOttDRcoGs02S\nLDprKExNoFs16Kopf/HKf72GciolTrV2jdbqcNg0UfW1Uf2U1zXxyHpbqKuNU1Q+i9akDdsp63i8\nU3oRBMquSwVfo1Vy6KPOWYMpbpa43i3iQuHPR1hNsZzFbHYY7JKTWn13D29ro/7Y5ZdAYP+k/Aqe\nWzXuwg82m9lit0cFUOx/s9lMWq1Na7XaHGajLaJp1tsjBhSrKVn3eHz+egTqjsvVJc/+xhq7Pxxm\ni8OutcW5l6+3yJVJ6xYVzxYq4WLWOYXXFNW17PuZAV4BgV1V/qmRbX9USWvEseK77aPGnRVVeXfV\nhXZwC+xGk1FjM+osoQ5WFf1ZgR5hVV+8cu0HVKs8HruvVVDx882htZjM7niTsfvuHDHkpkS5H6Zg\nl4u7u8HuElkixf9KUU+9AgL7o7IRrKewlhRxvXZBVnvEuburxuEkGd3tdtnhFsxytkhRJwubWaJa\nqL2DRu20fT0Cq1WOXJ4eJ7VKoWgWq1iMFim0g6tWMtpd/XA2OTvc9dhDJz9ij7XmXDWhsFV0XqVr\nVuJdscaaFZ81r/iueHqx9tov3C1u7opqWur+FN2mZ7+07rRa0x4xYY/OzNcjsETMr5l5s9nNJkuo\nxKvN/FBDiqJeonnpL3xmd51FY/ZhsrhrnHVGAwZ7MM/CkZewjC9eeQ9+U/u7pfs3RhxsH/5RGcKk\nsSg9YHqDk5NOo3UyGLRYHt23VbJGj6l0GhpbsO7tZbygdor3+FiYTWE9uya4PgKB1SyrunjvYpB0\nOieHVrKacHOEgni4xOk3Gn0gsL4EAttjt0fkvsKRSjbbjcHq16if6hRA9ZwhukkFp4JLUchTVItF\nNdjCA57ssHjV1yMwuXvTSQ5LRO7Dqok++hz7XtQZiuIfWzCEFqsZ2c0k290Cn+4T26ox8ogYyGJ5\niTiwBu9pH0zgiFNbzRZXm5PkpLejsaJqxbViEDXDKmvdLQ70UVM9u1HdQy87Hj72xStPJosteMCh\ntUecO4LL7LaobUrl24TFL8GadxP2mJDIuogjjU7Nq038ps7pHIHvK9hA6fiWaF30nTLntEQ+3KNW\naj4xwyPiXemilHmrUlmX6D9GzZisao+0D/9vIq9sUXd8PQJ3yVX/4DCdPeq62W52s5oCn81s10QX\nuWS9bLUS9lNi3k+zRTYEUkVlWy1E13T10+z6EvMDk+Qefb0jTihb5Yhjs6ukjamg0inPVruru8kP\nhd91ErLV5Go0RX/5Taf/5A/Vjzaz0fQCCIx2N2/BamrcbmKfh9VHsKI+eHHRhld3t5gGSxFDktVi\njcqolQ3OTs7eXCIK5MiQd7eXsYGiixeNtNRCpZ/X6mx+uFkkZ0NYRauN2ataR2yxr2yXYt5uRheT\nmqXn3oc+VmrG1ej2EgjczT68eTfoNFG5VNUwyd40On9z1j/q+hTPzhGKXxOaQNY6dKZHL3WLuHLI\nlmDHV//pw/eH8viVCMySOSqfGrOqdphkv8QJ/h67LHWTyPzFvzd1rmbVafVOVqOrxfyI/7VjVPLA\n6Ob2ErlgxN3NbH5UeGUaqugXZxfvet8ihpeYvlncTcGFJpPBWePkojHIRodR8ZD2wbOKXjL58N3P\niDBfvPLscI8u97M+KBVdmZL6HMt3de56dHdV5k03BbXSQSp2VDlD7REWv2oCxzJtMr6ERjTZ/dT7\n0+bi5CyZTcEK7jqHbIpdbpj96e6HD3ec7FatTWPAGBXA3eRq9dNZlo0uTrLRzZ/+x38ZjS/RN1yj\nai8aYDmxwl3WiPsppiiaiA+bOlVTFaXnNHmrdhP2qDPqtFjtERQvKlx7fQkbKLx+jEqtXFWJ3FvE\nw8KEy05KnB8wy6GFb6j3ZRxhK8+ZjF2dUiudhNVPf/fdaDS/xBRNnaWd9NHnq1OBGYLrFyZYT7pD\nrzDcPrhE1sV8TKn89qikdyeNk2w3x/zxT9eoi7wAAqXzdXWNWZJFNltix0Uhr1aegvmWGcKqijw1\n5vxglcVXlvCu1uQeleaHj5g7fBUEqLV+nfPbVeWsfp8/IdCE8nGKOqePaiKHpdcZ6qIJFPVxD6b8\nvPvgeZ/K1yOQIvstZosJTUT+0hM6BcPPgUO2Occ02d2qtT8sYJCjkq6vrpyQUZmlnML+K+96BQSe\nd8o9tdIy/VrvQcph2Z+vtokaorC80nFaQy1oMGKMXtGOWR+v26Mj++xVfenKbfTFl8iGVdErODR2\nnfFxzkmvqOCTxWgMvy9zJkUDLnjMWmOobbvNGJx69eFqtKgzK1+88lO3LRa7Sya1a55sMSH07B/W\nyYFaP9WuUArdqOTBTbmbbgndaFU783+dU/h6BHJcd536LudUZ2crZ8smnaqi7+pMLU8Mjrgj4wjm\nmyxWs3r/Vr2H4Tm79RrTdF1UqEX77rSR/baHxknplGy28P0nBk99rHxVFfU6dTuHarypMyrt41F1\niC9eeYjO4HedQfrU9S0xfAplZ4nrvsQuJ2UKu/REm+gPPO89LfM0mznYcZ0lLJO1N6tW6xXusLgQ\nHf/VHCpQI33ihkL/rdOnJfbN0XEt3YPCfrdAoHCJakN1YqwJflVybadz9vYCCHyTjVZDaBclw3VK\nZV/vla0Rbnd7qFw1qz0n04/HX3lExDbQpPpHG9NVvTe/vAACb1q9SftQNBqf75k+Hruru6qxvE6h\nPtdf70qvkyjJ8Zffl87pEXe8fzkC/weuozDDlCfgVQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic_array = get_picture_array(data_pred[5]*255)\n",
    "image = Image.fromarray(pic_array)\n",
    "image.save('temp.png', format=\"PNG\")  \n",
    "IPImage('temp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('CAE_face.npz', *lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1 of 500 took 0.752s\n",
      "  training loss:\t\t0.003627\n",
      "Epoch 2 of 500 took 0.758s\n",
      "  training loss:\t\t0.003627\n",
      "Epoch 3 of 500 took 0.770s\n",
      "  training loss:\t\t0.003627\n",
      "Epoch 4 of 500 took 0.790s\n",
      "  training loss:\t\t0.003627\n",
      "Epoch 5 of 500 took 0.768s\n",
      "  training loss:\t\t0.003627\n",
      "Epoch 6 of 500 took 0.761s\n",
      "  training loss:\t\t0.003627\n",
      "Epoch 7 of 500 took 0.777s\n",
      "  training loss:\t\t0.003627\n",
      "Epoch 8 of 500 took 0.704s\n",
      "  training loss:\t\t0.003627\n",
      "Epoch 9 of 500 took 0.704s\n",
      "  training loss:\t\t0.003627\n",
      "Epoch 10 of 500 took 0.703s\n",
      "  training loss:\t\t0.003627\n",
      "Epoch 11 of 500 took 0.701s\n",
      "  training loss:\t\t0.003627\n",
      "Epoch 12 of 500 took 0.737s\n",
      "  training loss:\t\t0.003627\n",
      "Epoch 13 of 500 took 0.704s\n",
      "  training loss:\t\t0.003626\n",
      "Epoch 14 of 500 took 0.705s\n",
      "  training loss:\t\t0.003626\n",
      "Epoch 15 of 500 took 0.705s\n",
      "  training loss:\t\t0.003626\n",
      "Epoch 16 of 500 took 0.705s\n",
      "  training loss:\t\t0.003626\n",
      "Epoch 17 of 500 took 0.748s\n",
      "  training loss:\t\t0.003626\n",
      "Epoch 18 of 500 took 0.704s\n",
      "  training loss:\t\t0.003626\n",
      "Epoch 19 of 500 took 0.704s\n",
      "  training loss:\t\t0.003626\n",
      "Epoch 20 of 500 took 0.706s\n",
      "  training loss:\t\t0.003626\n",
      "Epoch 21 of 500 took 0.703s\n",
      "  training loss:\t\t0.003626\n",
      "Epoch 22 of 500 took 0.705s\n",
      "  training loss:\t\t0.003626\n",
      "Epoch 23 of 500 took 0.703s\n",
      "  training loss:\t\t0.003626\n",
      "Epoch 24 of 500 took 0.702s\n",
      "  training loss:\t\t0.003626\n",
      "Epoch 25 of 500 took 0.703s\n",
      "  training loss:\t\t0.003626\n",
      "Epoch 26 of 500 took 0.703s\n",
      "  training loss:\t\t0.003625\n",
      "Epoch 27 of 500 took 0.703s\n",
      "  training loss:\t\t0.003625\n",
      "Epoch 28 of 500 took 0.704s\n",
      "  training loss:\t\t0.003625\n",
      "Epoch 29 of 500 took 0.704s\n",
      "  training loss:\t\t0.003625\n",
      "Epoch 30 of 500 took 0.702s\n",
      "  training loss:\t\t0.003625\n",
      "Epoch 31 of 500 took 0.703s\n",
      "  training loss:\t\t0.003625\n",
      "Epoch 32 of 500 took 0.704s\n",
      "  training loss:\t\t0.003625\n",
      "Epoch 33 of 500 took 0.702s\n",
      "  training loss:\t\t0.003625\n",
      "Epoch 34 of 500 took 0.701s\n",
      "  training loss:\t\t0.003625\n",
      "Epoch 35 of 500 took 0.702s\n",
      "  training loss:\t\t0.003624\n",
      "Epoch 36 of 500 took 0.701s\n",
      "  training loss:\t\t0.003624\n",
      "Epoch 37 of 500 took 0.704s\n",
      "  training loss:\t\t0.003624\n",
      "Epoch 38 of 500 took 0.739s\n",
      "  training loss:\t\t0.003624\n",
      "Epoch 39 of 500 took 0.723s\n",
      "  training loss:\t\t0.003624\n",
      "Epoch 40 of 500 took 0.725s\n",
      "  training loss:\t\t0.003624\n",
      "Epoch 41 of 500 took 0.713s\n",
      "  training loss:\t\t0.003624\n",
      "Epoch 42 of 500 took 0.703s\n",
      "  training loss:\t\t0.003623\n",
      "Epoch 43 of 500 took 0.709s\n",
      "  training loss:\t\t0.003623\n",
      "Epoch 44 of 500 took 0.757s\n",
      "  training loss:\t\t0.003623\n",
      "Epoch 45 of 500 took 0.741s\n",
      "  training loss:\t\t0.003623\n",
      "Epoch 46 of 500 took 0.702s\n",
      "  training loss:\t\t0.003623\n",
      "Epoch 47 of 500 took 0.733s\n",
      "  training loss:\t\t0.003623\n",
      "Epoch 48 of 500 took 0.715s\n",
      "  training loss:\t\t0.003623\n",
      "Epoch 49 of 500 took 0.711s\n",
      "  training loss:\t\t0.003622\n",
      "Epoch 50 of 500 took 0.749s\n",
      "  training loss:\t\t0.003622\n",
      "Epoch 51 of 500 took 0.728s\n",
      "  training loss:\t\t0.003622\n",
      "Epoch 52 of 500 took 0.701s\n",
      "  training loss:\t\t0.003622\n",
      "Epoch 53 of 500 took 0.701s\n",
      "  training loss:\t\t0.003622\n",
      "Epoch 54 of 500 took 0.701s\n",
      "  training loss:\t\t0.003622\n",
      "Epoch 55 of 500 took 0.705s\n",
      "  training loss:\t\t0.003622\n",
      "Epoch 56 of 500 took 0.786s\n",
      "  training loss:\t\t0.003621\n",
      "Epoch 57 of 500 took 0.764s\n",
      "  training loss:\t\t0.003621\n",
      "Epoch 58 of 500 took 0.776s\n",
      "  training loss:\t\t0.003621\n",
      "Epoch 59 of 500 took 0.775s\n",
      "  training loss:\t\t0.003621\n",
      "Epoch 60 of 500 took 0.758s\n",
      "  training loss:\t\t0.003621\n",
      "Epoch 61 of 500 took 0.707s\n",
      "  training loss:\t\t0.003621\n",
      "Epoch 62 of 500 took 0.739s\n",
      "  training loss:\t\t0.003620\n",
      "Epoch 63 of 500 took 0.733s\n",
      "  training loss:\t\t0.003620\n",
      "Epoch 64 of 500 took 0.744s\n",
      "  training loss:\t\t0.003620\n",
      "Epoch 65 of 500 took 0.726s\n",
      "  training loss:\t\t0.003620\n",
      "Epoch 66 of 500 took 0.717s\n",
      "  training loss:\t\t0.003620\n",
      "Epoch 67 of 500 took 0.743s\n",
      "  training loss:\t\t0.003620\n",
      "Epoch 68 of 500 took 0.748s\n",
      "  training loss:\t\t0.003619\n",
      "Epoch 69 of 500 took 0.746s\n",
      "  training loss:\t\t0.003619\n",
      "Epoch 70 of 500 took 0.725s\n",
      "  training loss:\t\t0.003619\n",
      "Epoch 71 of 500 took 0.719s\n",
      "  training loss:\t\t0.003619\n",
      "Epoch 72 of 500 took 0.746s\n",
      "  training loss:\t\t0.003619\n",
      "Epoch 73 of 500 took 0.751s\n",
      "  training loss:\t\t0.003619\n",
      "Epoch 74 of 500 took 0.728s\n",
      "  training loss:\t\t0.003618\n",
      "Epoch 75 of 500 took 0.748s\n",
      "  training loss:\t\t0.003618\n",
      "Epoch 76 of 500 took 0.713s\n",
      "  training loss:\t\t0.003618\n",
      "Epoch 77 of 500 took 0.718s\n",
      "  training loss:\t\t0.003618\n",
      "Epoch 78 of 500 took 0.729s\n",
      "  training loss:\t\t0.003618\n",
      "Epoch 79 of 500 took 0.745s\n",
      "  training loss:\t\t0.003617\n",
      "Epoch 80 of 500 took 0.744s\n",
      "  training loss:\t\t0.003617\n",
      "Epoch 81 of 500 took 0.758s\n",
      "  training loss:\t\t0.003617\n",
      "Epoch 82 of 500 took 0.745s\n",
      "  training loss:\t\t0.003617\n",
      "Epoch 83 of 500 took 0.738s\n",
      "  training loss:\t\t0.003617\n",
      "Epoch 84 of 500 took 0.743s\n",
      "  training loss:\t\t0.003617\n",
      "Epoch 85 of 500 took 0.708s\n",
      "  training loss:\t\t0.003616\n",
      "Epoch 86 of 500 took 0.711s\n",
      "  training loss:\t\t0.003616\n",
      "Epoch 87 of 500 took 0.707s\n",
      "  training loss:\t\t0.003616\n",
      "Epoch 88 of 500 took 0.735s\n",
      "  training loss:\t\t0.003616\n",
      "Epoch 89 of 500 took 0.724s\n",
      "  training loss:\t\t0.003616\n",
      "Epoch 90 of 500 took 0.718s\n",
      "  training loss:\t\t0.003615\n",
      "Epoch 91 of 500 took 0.708s\n",
      "  training loss:\t\t0.003615\n",
      "Epoch 92 of 500 took 0.704s\n",
      "  training loss:\t\t0.003615\n",
      "Epoch 93 of 500 took 0.722s\n",
      "  training loss:\t\t0.003615\n",
      "Epoch 94 of 500 took 0.739s\n",
      "  training loss:\t\t0.003615\n",
      "Epoch 95 of 500 took 0.755s\n",
      "  training loss:\t\t0.003615\n",
      "Epoch 96 of 500 took 0.754s\n",
      "  training loss:\t\t0.003614\n",
      "Epoch 97 of 500 took 0.753s\n",
      "  training loss:\t\t0.003614\n",
      "Epoch 98 of 500 took 0.765s\n",
      "  training loss:\t\t0.003614\n",
      "Epoch 99 of 500 took 0.773s\n",
      "  training loss:\t\t0.003614\n",
      "Epoch 100 of 500 took 0.775s\n",
      "  training loss:\t\t0.003614\n",
      "Epoch 101 of 500 took 0.758s\n",
      "  training loss:\t\t0.003613\n",
      "Epoch 102 of 500 took 0.766s\n",
      "  training loss:\t\t0.003613\n",
      "Epoch 103 of 500 took 0.765s\n",
      "  training loss:\t\t0.003613\n",
      "Epoch 104 of 500 took 0.746s\n",
      "  training loss:\t\t0.003613\n",
      "Epoch 105 of 500 took 0.750s\n",
      "  training loss:\t\t0.003613\n",
      "Epoch 106 of 500 took 0.751s\n",
      "  training loss:\t\t0.003612\n",
      "Epoch 107 of 500 took 0.776s\n",
      "  training loss:\t\t0.003612\n",
      "Epoch 108 of 500 took 0.765s\n",
      "  training loss:\t\t0.003612\n",
      "Epoch 109 of 500 took 0.770s\n",
      "  training loss:\t\t0.003612\n",
      "Epoch 110 of 500 took 0.773s\n",
      "  training loss:\t\t0.003612\n",
      "Epoch 111 of 500 took 0.773s\n",
      "  training loss:\t\t0.003612\n",
      "Epoch 112 of 500 took 0.771s\n",
      "  training loss:\t\t0.003611\n",
      "Epoch 113 of 500 took 0.773s\n",
      "  training loss:\t\t0.003611\n",
      "Epoch 114 of 500 took 0.774s\n",
      "  training loss:\t\t0.003611\n",
      "Epoch 115 of 500 took 0.770s\n",
      "  training loss:\t\t0.003611\n",
      "Epoch 116 of 500 took 0.772s\n",
      "  training loss:\t\t0.003611\n",
      "Epoch 117 of 500 took 0.772s\n",
      "  training loss:\t\t0.003610\n",
      "Epoch 118 of 500 took 0.775s\n",
      "  training loss:\t\t0.003610\n",
      "Epoch 119 of 500 took 0.772s\n",
      "  training loss:\t\t0.003610\n",
      "Epoch 120 of 500 took 0.774s\n",
      "  training loss:\t\t0.003610\n",
      "Epoch 121 of 500 took 0.777s\n",
      "  training loss:\t\t0.003610\n",
      "Epoch 122 of 500 took 0.776s\n",
      "  training loss:\t\t0.003609\n",
      "Epoch 123 of 500 took 0.773s\n",
      "  training loss:\t\t0.003609\n",
      "Epoch 124 of 500 took 0.774s\n",
      "  training loss:\t\t0.003609\n",
      "Epoch 125 of 500 took 0.774s\n",
      "  training loss:\t\t0.003609\n",
      "Epoch 126 of 500 took 0.773s\n",
      "  training loss:\t\t0.003609\n",
      "Epoch 127 of 500 took 0.773s\n",
      "  training loss:\t\t0.003608\n",
      "Epoch 128 of 500 took 0.773s\n",
      "  training loss:\t\t0.003608\n",
      "Epoch 129 of 500 took 0.773s\n",
      "  training loss:\t\t0.003608\n",
      "Epoch 130 of 500 took 0.773s\n",
      "  training loss:\t\t0.003608\n",
      "Epoch 131 of 500 took 0.771s\n",
      "  training loss:\t\t0.003608\n",
      "Epoch 132 of 500 took 0.765s\n",
      "  training loss:\t\t0.003607\n",
      "Epoch 133 of 500 took 0.773s\n",
      "  training loss:\t\t0.003607\n",
      "Epoch 134 of 500 took 0.764s\n",
      "  training loss:\t\t0.003607\n",
      "Epoch 135 of 500 took 0.750s\n",
      "  training loss:\t\t0.003607\n",
      "Epoch 136 of 500 took 0.742s\n",
      "  training loss:\t\t0.003607\n",
      "Epoch 137 of 500 took 0.764s\n",
      "  training loss:\t\t0.003607\n",
      "Epoch 138 of 500 took 0.773s\n",
      "  training loss:\t\t0.003606\n",
      "Epoch 139 of 500 took 0.768s\n",
      "  training loss:\t\t0.003606\n",
      "Epoch 140 of 500 took 0.748s\n",
      "  training loss:\t\t0.003606\n",
      "Epoch 141 of 500 took 0.718s\n",
      "  training loss:\t\t0.003606\n",
      "Epoch 142 of 500 took 0.767s\n",
      "  training loss:\t\t0.003606\n",
      "Epoch 143 of 500 took 0.749s\n",
      "  training loss:\t\t0.003605\n",
      "Epoch 144 of 500 took 0.762s\n",
      "  training loss:\t\t0.003605\n",
      "Epoch 145 of 500 took 0.740s\n",
      "  training loss:\t\t0.003605\n",
      "Epoch 146 of 500 took 0.703s\n",
      "  training loss:\t\t0.003605\n",
      "Epoch 147 of 500 took 0.708s\n",
      "  training loss:\t\t0.003605\n",
      "Epoch 148 of 500 took 0.722s\n",
      "  training loss:\t\t0.003604\n",
      "Epoch 149 of 500 took 0.784s\n",
      "  training loss:\t\t0.003604\n",
      "Epoch 150 of 500 took 0.769s\n",
      "  training loss:\t\t0.003604\n",
      "Epoch 151 of 500 took 0.774s\n",
      "  training loss:\t\t0.003604\n",
      "Epoch 152 of 500 took 0.801s\n",
      "  training loss:\t\t0.003604\n",
      "Epoch 153 of 500 took 0.760s\n",
      "  training loss:\t\t0.003603\n",
      "Epoch 154 of 500 took 0.764s\n",
      "  training loss:\t\t0.003603\n",
      "Epoch 155 of 500 took 0.699s\n",
      "  training loss:\t\t0.003603\n",
      "Epoch 156 of 500 took 0.704s\n",
      "  training loss:\t\t0.003603\n",
      "Epoch 157 of 500 took 0.767s\n",
      "  training loss:\t\t0.003603\n",
      "Epoch 158 of 500 took 0.771s\n",
      "  training loss:\t\t0.003602\n",
      "Epoch 159 of 500 took 0.750s\n",
      "  training loss:\t\t0.003602\n",
      "Epoch 160 of 500 took 0.726s\n",
      "  training loss:\t\t0.003602\n",
      "Epoch 161 of 500 took 0.716s\n",
      "  training loss:\t\t0.003602\n",
      "Epoch 162 of 500 took 0.703s\n",
      "  training loss:\t\t0.003602\n",
      "Epoch 163 of 500 took 0.730s\n",
      "  training loss:\t\t0.003602\n",
      "Epoch 164 of 500 took 0.701s\n",
      "  training loss:\t\t0.003601\n",
      "Epoch 165 of 500 took 0.701s\n",
      "  training loss:\t\t0.003601\n",
      "Epoch 166 of 500 took 0.730s\n",
      "  training loss:\t\t0.003601\n",
      "Epoch 167 of 500 took 0.700s\n",
      "  training loss:\t\t0.003601\n",
      "Epoch 168 of 500 took 0.705s\n",
      "  training loss:\t\t0.003601\n",
      "Epoch 169 of 500 took 0.703s\n",
      "  training loss:\t\t0.003600\n",
      "Epoch 170 of 500 took 0.703s\n",
      "  training loss:\t\t0.003600\n",
      "Epoch 171 of 500 took 0.699s\n",
      "  training loss:\t\t0.003600\n",
      "Epoch 172 of 500 took 0.703s\n",
      "  training loss:\t\t0.003600\n",
      "Epoch 173 of 500 took 0.702s\n",
      "  training loss:\t\t0.003600\n",
      "Epoch 174 of 500 took 0.705s\n",
      "  training loss:\t\t0.003599\n",
      "Epoch 175 of 500 took 0.700s\n",
      "  training loss:\t\t0.003599\n",
      "Epoch 176 of 500 took 0.703s\n",
      "  training loss:\t\t0.003599\n",
      "Epoch 177 of 500 took 0.736s\n",
      "  training loss:\t\t0.003599\n",
      "Epoch 178 of 500 took 0.745s\n",
      "  training loss:\t\t0.003599\n",
      "Epoch 179 of 500 took 0.730s\n",
      "  training loss:\t\t0.003598\n",
      "Epoch 180 of 500 took 0.772s\n",
      "  training loss:\t\t0.003598\n",
      "Epoch 181 of 500 took 0.744s\n",
      "  training loss:\t\t0.003598\n",
      "Epoch 182 of 500 took 0.731s\n",
      "  training loss:\t\t0.003598\n",
      "Epoch 183 of 500 took 0.703s\n",
      "  training loss:\t\t0.003598\n",
      "Epoch 184 of 500 took 0.701s\n",
      "  training loss:\t\t0.003597\n",
      "Epoch 185 of 500 took 0.741s\n",
      "  training loss:\t\t0.003597\n",
      "Epoch 186 of 500 took 0.745s\n",
      "  training loss:\t\t0.003597\n",
      "Epoch 187 of 500 took 0.702s\n",
      "  training loss:\t\t0.003597\n",
      "Epoch 188 of 500 took 0.703s\n",
      "  training loss:\t\t0.003597\n",
      "Epoch 189 of 500 took 0.703s\n",
      "  training loss:\t\t0.003597\n",
      "Epoch 190 of 500 took 0.722s\n",
      "  training loss:\t\t0.003596\n",
      "Epoch 191 of 500 took 0.716s\n",
      "  training loss:\t\t0.003596\n",
      "Epoch 192 of 500 took 0.709s\n",
      "  training loss:\t\t0.003596\n",
      "Epoch 193 of 500 took 0.700s\n",
      "  training loss:\t\t0.003596\n",
      "Epoch 194 of 500 took 0.706s\n",
      "  training loss:\t\t0.003596\n",
      "Epoch 195 of 500 took 0.765s\n",
      "  training loss:\t\t0.003595\n",
      "Epoch 196 of 500 took 0.752s\n",
      "  training loss:\t\t0.003595\n",
      "Epoch 197 of 500 took 0.733s\n",
      "  training loss:\t\t0.003595\n",
      "Epoch 198 of 500 took 0.706s\n",
      "  training loss:\t\t0.003595\n",
      "Epoch 199 of 500 took 0.699s\n",
      "  training loss:\t\t0.003595\n",
      "Epoch 200 of 500 took 0.703s\n",
      "  training loss:\t\t0.003594\n",
      "Epoch 201 of 500 took 0.734s\n",
      "  training loss:\t\t0.003594\n",
      "Epoch 202 of 500 took 0.739s\n",
      "  training loss:\t\t0.003594\n",
      "Epoch 203 of 500 took 0.736s\n",
      "  training loss:\t\t0.003594\n",
      "Epoch 204 of 500 took 0.741s\n",
      "  training loss:\t\t0.003594\n",
      "Epoch 205 of 500 took 0.768s\n",
      "  training loss:\t\t0.003593\n",
      "Epoch 206 of 500 took 0.742s\n",
      "  training loss:\t\t0.003593\n",
      "Epoch 207 of 500 took 0.703s\n",
      "  training loss:\t\t0.003593\n",
      "Epoch 208 of 500 took 0.714s\n",
      "  training loss:\t\t0.003593\n",
      "Epoch 209 of 500 took 0.706s\n",
      "  training loss:\t\t0.003593\n",
      "Epoch 210 of 500 took 0.704s\n",
      "  training loss:\t\t0.003593\n",
      "Epoch 211 of 500 took 0.704s\n",
      "  training loss:\t\t0.003592\n",
      "Epoch 212 of 500 took 0.706s\n",
      "  training loss:\t\t0.003592\n",
      "Epoch 213 of 500 took 0.705s\n",
      "  training loss:\t\t0.003592\n",
      "Epoch 214 of 500 took 0.707s\n",
      "  training loss:\t\t0.003592\n",
      "Epoch 215 of 500 took 0.708s\n",
      "  training loss:\t\t0.003592\n",
      "Epoch 216 of 500 took 0.756s\n",
      "  training loss:\t\t0.003591\n",
      "Epoch 217 of 500 took 0.724s\n",
      "  training loss:\t\t0.003591\n",
      "Epoch 218 of 500 took 0.704s\n",
      "  training loss:\t\t0.003591\n",
      "Epoch 219 of 500 took 0.747s\n",
      "  training loss:\t\t0.003591\n",
      "Epoch 220 of 500 took 0.739s\n",
      "  training loss:\t\t0.003591\n",
      "Epoch 221 of 500 took 0.724s\n",
      "  training loss:\t\t0.003590\n",
      "Epoch 222 of 500 took 0.753s\n",
      "  training loss:\t\t0.003590\n",
      "Epoch 223 of 500 took 0.727s\n",
      "  training loss:\t\t0.003590\n",
      "Epoch 224 of 500 took 0.746s\n",
      "  training loss:\t\t0.003590\n",
      "Epoch 225 of 500 took 0.762s\n",
      "  training loss:\t\t0.003590\n",
      "Epoch 226 of 500 took 0.764s\n",
      "  training loss:\t\t0.003590\n",
      "Epoch 227 of 500 took 0.750s\n",
      "  training loss:\t\t0.003589\n",
      "Epoch 228 of 500 took 0.726s\n",
      "  training loss:\t\t0.003589\n",
      "Epoch 229 of 500 took 0.744s\n",
      "  training loss:\t\t0.003589\n",
      "Epoch 230 of 500 took 0.741s\n",
      "  training loss:\t\t0.003589\n",
      "Epoch 231 of 500 took 0.767s\n",
      "  training loss:\t\t0.003589\n",
      "Epoch 232 of 500 took 0.773s\n",
      "  training loss:\t\t0.003588\n",
      "Epoch 233 of 500 took 0.775s\n",
      "  training loss:\t\t0.003588\n",
      "Epoch 234 of 500 took 0.774s\n",
      "  training loss:\t\t0.003588\n",
      "Epoch 235 of 500 took 0.771s\n",
      "  training loss:\t\t0.003588\n",
      "Epoch 236 of 500 took 0.770s\n",
      "  training loss:\t\t0.003588\n",
      "Epoch 237 of 500 took 0.772s\n",
      "  training loss:\t\t0.003587\n",
      "Epoch 238 of 500 took 0.774s\n",
      "  training loss:\t\t0.003587\n",
      "Epoch 239 of 500 took 0.771s\n",
      "  training loss:\t\t0.003587\n",
      "Epoch 240 of 500 took 0.771s\n",
      "  training loss:\t\t0.003587\n",
      "Epoch 241 of 500 took 0.770s\n",
      "  training loss:\t\t0.003587\n",
      "Epoch 242 of 500 took 0.771s\n",
      "  training loss:\t\t0.003587\n",
      "Epoch 243 of 500 took 0.770s\n",
      "  training loss:\t\t0.003586\n",
      "Epoch 244 of 500 took 0.770s\n",
      "  training loss:\t\t0.003586\n",
      "Epoch 245 of 500 took 0.770s\n",
      "  training loss:\t\t0.003586\n",
      "Epoch 246 of 500 took 0.770s\n",
      "  training loss:\t\t0.003586\n",
      "Epoch 247 of 500 took 0.774s\n",
      "  training loss:\t\t0.003586\n",
      "Epoch 248 of 500 took 0.773s\n",
      "  training loss:\t\t0.003585\n",
      "Epoch 249 of 500 took 0.773s\n",
      "  training loss:\t\t0.003585\n",
      "Epoch 250 of 500 took 0.780s\n",
      "  training loss:\t\t0.003585\n",
      "Epoch 251 of 500 took 0.776s\n",
      "  training loss:\t\t0.003585\n",
      "Epoch 252 of 500 took 0.776s\n",
      "  training loss:\t\t0.003585\n",
      "Epoch 253 of 500 took 0.773s\n",
      "  training loss:\t\t0.003584\n",
      "Epoch 254 of 500 took 0.769s\n",
      "  training loss:\t\t0.003584\n",
      "Epoch 255 of 500 took 0.768s\n",
      "  training loss:\t\t0.003584\n",
      "Epoch 256 of 500 took 0.767s\n",
      "  training loss:\t\t0.003584\n",
      "Epoch 257 of 500 took 0.771s\n",
      "  training loss:\t\t0.003584\n",
      "Epoch 258 of 500 took 0.772s\n",
      "  training loss:\t\t0.003584\n",
      "Epoch 259 of 500 took 0.772s\n",
      "  training loss:\t\t0.003583\n",
      "Epoch 260 of 500 took 0.775s\n",
      "  training loss:\t\t0.003583\n",
      "Epoch 261 of 500 took 0.780s\n",
      "  training loss:\t\t0.003583\n",
      "Epoch 262 of 500 took 0.772s\n",
      "  training loss:\t\t0.003583\n",
      "Epoch 263 of 500 took 0.775s\n",
      "  training loss:\t\t0.003583\n",
      "Epoch 264 of 500 took 0.773s\n",
      "  training loss:\t\t0.003582\n",
      "Epoch 265 of 500 took 0.771s\n",
      "  training loss:\t\t0.003582\n",
      "Epoch 266 of 500 took 0.773s\n",
      "  training loss:\t\t0.003582\n",
      "Epoch 267 of 500 took 0.773s\n",
      "  training loss:\t\t0.003582\n",
      "Epoch 268 of 500 took 0.773s\n",
      "  training loss:\t\t0.003582\n",
      "Epoch 269 of 500 took 0.773s\n",
      "  training loss:\t\t0.003582\n",
      "Epoch 270 of 500 took 0.776s\n",
      "  training loss:\t\t0.003581\n",
      "Epoch 271 of 500 took 0.775s\n",
      "  training loss:\t\t0.003581\n",
      "Epoch 272 of 500 took 0.777s\n",
      "  training loss:\t\t0.003581\n",
      "Epoch 273 of 500 took 0.776s\n",
      "  training loss:\t\t0.003581\n",
      "Epoch 274 of 500 took 0.776s\n",
      "  training loss:\t\t0.003581\n",
      "Epoch 275 of 500 took 0.775s\n",
      "  training loss:\t\t0.003580\n",
      "Epoch 276 of 500 took 0.774s\n",
      "  training loss:\t\t0.003580\n",
      "Epoch 277 of 500 took 0.772s\n",
      "  training loss:\t\t0.003580\n",
      "Epoch 278 of 500 took 0.768s\n",
      "  training loss:\t\t0.003580\n",
      "Epoch 279 of 500 took 0.773s\n",
      "  training loss:\t\t0.003580\n",
      "Epoch 280 of 500 took 0.773s\n",
      "  training loss:\t\t0.003580\n",
      "Epoch 281 of 500 took 0.771s\n",
      "  training loss:\t\t0.003579\n",
      "Epoch 282 of 500 took 0.769s\n",
      "  training loss:\t\t0.003579\n",
      "Epoch 283 of 500 took 0.775s\n",
      "  training loss:\t\t0.003579\n",
      "Epoch 284 of 500 took 0.777s\n",
      "  training loss:\t\t0.003579\n",
      "Epoch 285 of 500 took 0.770s\n",
      "  training loss:\t\t0.003579\n",
      "Epoch 286 of 500 took 0.767s\n",
      "  training loss:\t\t0.003578\n",
      "Epoch 287 of 500 took 0.773s\n",
      "  training loss:\t\t0.003578\n",
      "Epoch 288 of 500 took 0.771s\n",
      "  training loss:\t\t0.003578\n",
      "Epoch 289 of 500 took 0.767s\n",
      "  training loss:\t\t0.003578\n",
      "Epoch 290 of 500 took 0.766s\n",
      "  training loss:\t\t0.003578\n",
      "Epoch 291 of 500 took 0.767s\n",
      "  training loss:\t\t0.003578\n",
      "Epoch 292 of 500 took 0.767s\n",
      "  training loss:\t\t0.003577\n",
      "Epoch 293 of 500 took 0.767s\n",
      "  training loss:\t\t0.003577\n",
      "Epoch 294 of 500 took 0.767s\n",
      "  training loss:\t\t0.003577\n",
      "Epoch 295 of 500 took 0.768s\n",
      "  training loss:\t\t0.003577\n",
      "Epoch 296 of 500 took 0.770s\n",
      "  training loss:\t\t0.003577\n",
      "Epoch 297 of 500 took 0.770s\n",
      "  training loss:\t\t0.003576\n",
      "Epoch 298 of 500 took 0.770s\n",
      "  training loss:\t\t0.003576\n",
      "Epoch 299 of 500 took 0.767s\n",
      "  training loss:\t\t0.003576\n",
      "Epoch 300 of 500 took 0.768s\n",
      "  training loss:\t\t0.003576\n",
      "Epoch 301 of 500 took 0.768s\n",
      "  training loss:\t\t0.003576\n",
      "Epoch 302 of 500 took 0.766s\n",
      "  training loss:\t\t0.003576\n",
      "Epoch 303 of 500 took 0.766s\n",
      "  training loss:\t\t0.003575\n",
      "Epoch 304 of 500 took 0.767s\n",
      "  training loss:\t\t0.003575\n",
      "Epoch 305 of 500 took 0.769s\n",
      "  training loss:\t\t0.003575\n",
      "Epoch 306 of 500 took 0.769s\n",
      "  training loss:\t\t0.003575\n",
      "Epoch 307 of 500 took 0.769s\n",
      "  training loss:\t\t0.003575\n",
      "Epoch 308 of 500 took 0.771s\n",
      "  training loss:\t\t0.003574\n",
      "Epoch 309 of 500 took 0.769s\n",
      "  training loss:\t\t0.003574\n",
      "Epoch 310 of 500 took 0.766s\n",
      "  training loss:\t\t0.003574\n",
      "Epoch 311 of 500 took 0.769s\n",
      "  training loss:\t\t0.003574\n",
      "Epoch 312 of 500 took 0.768s\n",
      "  training loss:\t\t0.003574\n",
      "Epoch 313 of 500 took 0.768s\n",
      "  training loss:\t\t0.003574\n",
      "Epoch 314 of 500 took 0.767s\n",
      "  training loss:\t\t0.003573\n",
      "Epoch 315 of 500 took 0.768s\n",
      "  training loss:\t\t0.003573\n",
      "Epoch 316 of 500 took 0.769s\n",
      "  training loss:\t\t0.003573\n",
      "Epoch 317 of 500 took 0.768s\n",
      "  training loss:\t\t0.003573\n",
      "Epoch 318 of 500 took 0.771s\n",
      "  training loss:\t\t0.003573\n",
      "Epoch 319 of 500 took 0.769s\n",
      "  training loss:\t\t0.003572\n",
      "Epoch 320 of 500 took 0.767s\n",
      "  training loss:\t\t0.003572\n",
      "Epoch 321 of 500 took 0.768s\n",
      "  training loss:\t\t0.003572\n",
      "Epoch 322 of 500 took 0.767s\n",
      "  training loss:\t\t0.003572\n",
      "Epoch 323 of 500 took 0.768s\n",
      "  training loss:\t\t0.003572\n",
      "Epoch 324 of 500 took 0.768s\n",
      "  training loss:\t\t0.003572\n",
      "Epoch 325 of 500 took 0.768s\n",
      "  training loss:\t\t0.003571\n",
      "Epoch 326 of 500 took 0.768s\n",
      "  training loss:\t\t0.003571\n",
      "Epoch 327 of 500 took 0.769s\n",
      "  training loss:\t\t0.003571\n",
      "Epoch 328 of 500 took 0.771s\n",
      "  training loss:\t\t0.003571\n",
      "Epoch 329 of 500 took 0.771s\n",
      "  training loss:\t\t0.003571\n",
      "Epoch 330 of 500 took 0.771s\n",
      "  training loss:\t\t0.003570\n",
      "Epoch 331 of 500 took 0.770s\n",
      "  training loss:\t\t0.003570\n",
      "Epoch 332 of 500 took 0.755s\n",
      "  training loss:\t\t0.003570\n",
      "Epoch 333 of 500 took 0.747s\n",
      "  training loss:\t\t0.003570\n",
      "Epoch 334 of 500 took 0.744s\n",
      "  training loss:\t\t0.003570\n",
      "Epoch 335 of 500 took 0.746s\n",
      "  training loss:\t\t0.003570\n",
      "Epoch 336 of 500 took 0.750s\n",
      "  training loss:\t\t0.003569\n",
      "Epoch 337 of 500 took 0.771s\n",
      "  training loss:\t\t0.003569\n",
      "Epoch 338 of 500 took 0.773s\n",
      "  training loss:\t\t0.003569\n",
      "Epoch 339 of 500 took 0.774s\n",
      "  training loss:\t\t0.003569\n",
      "Epoch 340 of 500 took 0.771s\n",
      "  training loss:\t\t0.003569\n",
      "Epoch 341 of 500 took 0.770s\n",
      "  training loss:\t\t0.003569\n",
      "Epoch 342 of 500 took 0.770s\n",
      "  training loss:\t\t0.003568\n",
      "Epoch 343 of 500 took 0.771s\n",
      "  training loss:\t\t0.003568\n",
      "Epoch 344 of 500 took 0.774s\n",
      "  training loss:\t\t0.003568\n",
      "Epoch 345 of 500 took 0.770s\n",
      "  training loss:\t\t0.003568\n",
      "Epoch 346 of 500 took 0.775s\n",
      "  training loss:\t\t0.003568\n",
      "Epoch 347 of 500 took 0.770s\n",
      "  training loss:\t\t0.003567\n",
      "Epoch 348 of 500 took 0.771s\n",
      "  training loss:\t\t0.003567\n",
      "Epoch 349 of 500 took 0.770s\n",
      "  training loss:\t\t0.003567\n",
      "Epoch 350 of 500 took 0.773s\n",
      "  training loss:\t\t0.003567\n",
      "Epoch 351 of 500 took 0.772s\n",
      "  training loss:\t\t0.003567\n",
      "Epoch 352 of 500 took 0.771s\n",
      "  training loss:\t\t0.003567\n",
      "Epoch 353 of 500 took 0.772s\n",
      "  training loss:\t\t0.003566\n",
      "Epoch 354 of 500 took 0.772s\n",
      "  training loss:\t\t0.003566\n",
      "Epoch 355 of 500 took 0.769s\n",
      "  training loss:\t\t0.003566\n",
      "Epoch 356 of 500 took 0.775s\n",
      "  training loss:\t\t0.003566\n",
      "Epoch 357 of 500 took 0.772s\n",
      "  training loss:\t\t0.003566\n",
      "Epoch 358 of 500 took 0.769s\n",
      "  training loss:\t\t0.003566\n",
      "Epoch 359 of 500 took 0.766s\n",
      "  training loss:\t\t0.003565\n",
      "Epoch 360 of 500 took 0.767s\n",
      "  training loss:\t\t0.003565\n",
      "Epoch 361 of 500 took 0.751s\n",
      "  training loss:\t\t0.003565\n",
      "Epoch 362 of 500 took 0.748s\n",
      "  training loss:\t\t0.003565\n",
      "Epoch 363 of 500 took 0.744s\n",
      "  training loss:\t\t0.003565\n",
      "Epoch 364 of 500 took 0.748s\n",
      "  training loss:\t\t0.003564\n",
      "Epoch 365 of 500 took 0.743s\n",
      "  training loss:\t\t0.003564\n",
      "Epoch 366 of 500 took 0.752s\n",
      "  training loss:\t\t0.003564\n",
      "Epoch 367 of 500 took 0.752s\n",
      "  training loss:\t\t0.003564\n",
      "Epoch 368 of 500 took 0.745s\n",
      "  training loss:\t\t0.003564\n",
      "Epoch 369 of 500 took 0.754s\n",
      "  training loss:\t\t0.003564\n",
      "Epoch 370 of 500 took 0.746s\n",
      "  training loss:\t\t0.003563\n",
      "Epoch 371 of 500 took 0.748s\n",
      "  training loss:\t\t0.003563\n",
      "Epoch 372 of 500 took 0.748s\n",
      "  training loss:\t\t0.003563\n",
      "Epoch 373 of 500 took 0.752s\n",
      "  training loss:\t\t0.003563\n",
      "Epoch 374 of 500 took 0.770s\n",
      "  training loss:\t\t0.003563\n",
      "Epoch 375 of 500 took 0.749s\n",
      "  training loss:\t\t0.003563\n",
      "Epoch 376 of 500 took 0.750s\n",
      "  training loss:\t\t0.003562\n",
      "Epoch 377 of 500 took 0.761s\n",
      "  training loss:\t\t0.003562\n",
      "Epoch 378 of 500 took 0.770s\n",
      "  training loss:\t\t0.003562\n",
      "Epoch 379 of 500 took 0.768s\n",
      "  training loss:\t\t0.003562\n",
      "Epoch 380 of 500 took 0.721s\n",
      "  training loss:\t\t0.003562\n",
      "Epoch 381 of 500 took 0.727s\n",
      "  training loss:\t\t0.003562\n",
      "Epoch 382 of 500 took 0.735s\n",
      "  training loss:\t\t0.003561\n",
      "Epoch 383 of 500 took 0.772s\n",
      "  training loss:\t\t0.003561\n",
      "Epoch 384 of 500 took 0.744s\n",
      "  training loss:\t\t0.003561\n",
      "Epoch 385 of 500 took 0.761s\n",
      "  training loss:\t\t0.003561\n",
      "Epoch 386 of 500 took 0.750s\n",
      "  training loss:\t\t0.003561\n",
      "Epoch 387 of 500 took 0.751s\n",
      "  training loss:\t\t0.003560\n",
      "Epoch 388 of 500 took 0.735s\n",
      "  training loss:\t\t0.003560\n",
      "Epoch 389 of 500 took 0.751s\n",
      "  training loss:\t\t0.003560\n",
      "Epoch 390 of 500 took 0.726s\n",
      "  training loss:\t\t0.003560\n",
      "Epoch 391 of 500 took 0.704s\n",
      "  training loss:\t\t0.003560\n",
      "Epoch 392 of 500 took 0.703s\n",
      "  training loss:\t\t0.003560\n",
      "Epoch 393 of 500 took 0.709s\n",
      "  training loss:\t\t0.003559\n",
      "Epoch 394 of 500 took 0.744s\n",
      "  training loss:\t\t0.003559\n",
      "Epoch 395 of 500 took 0.724s\n",
      "  training loss:\t\t0.003559\n",
      "Epoch 396 of 500 took 0.702s\n",
      "  training loss:\t\t0.003559\n",
      "Epoch 397 of 500 took 0.744s\n",
      "  training loss:\t\t0.003559\n",
      "Epoch 398 of 500 took 0.726s\n",
      "  training loss:\t\t0.003559\n",
      "Epoch 399 of 500 took 0.739s\n",
      "  training loss:\t\t0.003558\n",
      "Epoch 400 of 500 took 0.765s\n",
      "  training loss:\t\t0.003558\n",
      "Epoch 401 of 500 took 0.754s\n",
      "  training loss:\t\t0.003558\n",
      "Epoch 402 of 500 took 0.733s\n",
      "  training loss:\t\t0.003558\n",
      "Epoch 403 of 500 took 0.759s\n",
      "  training loss:\t\t0.003558\n",
      "Epoch 404 of 500 took 0.765s\n",
      "  training loss:\t\t0.003558\n",
      "Epoch 405 of 500 took 0.756s\n",
      "  training loss:\t\t0.003557\n",
      "Epoch 406 of 500 took 0.754s\n",
      "  training loss:\t\t0.003557\n",
      "Epoch 407 of 500 took 0.738s\n",
      "  training loss:\t\t0.003557\n",
      "Epoch 408 of 500 took 0.741s\n",
      "  training loss:\t\t0.003557\n",
      "Epoch 409 of 500 took 0.735s\n",
      "  training loss:\t\t0.003557\n",
      "Epoch 410 of 500 took 0.733s\n",
      "  training loss:\t\t0.003556\n",
      "Epoch 411 of 500 took 0.735s\n",
      "  training loss:\t\t0.003556\n",
      "Epoch 412 of 500 took 0.736s\n",
      "  training loss:\t\t0.003556\n",
      "Epoch 413 of 500 took 0.737s\n",
      "  training loss:\t\t0.003556\n",
      "Epoch 414 of 500 took 0.732s\n",
      "  training loss:\t\t0.003556\n",
      "Epoch 415 of 500 took 0.737s\n",
      "  training loss:\t\t0.003556\n",
      "Epoch 416 of 500 took 0.737s\n",
      "  training loss:\t\t0.003555\n",
      "Epoch 417 of 500 took 0.742s\n",
      "  training loss:\t\t0.003555\n",
      "Epoch 418 of 500 took 0.753s\n",
      "  training loss:\t\t0.003555\n",
      "Epoch 419 of 500 took 0.742s\n",
      "  training loss:\t\t0.003555\n",
      "Epoch 420 of 500 took 0.741s\n",
      "  training loss:\t\t0.003555\n",
      "Epoch 421 of 500 took 0.739s\n",
      "  training loss:\t\t0.003555\n",
      "Epoch 422 of 500 took 0.750s\n",
      "  training loss:\t\t0.003554\n",
      "Epoch 423 of 500 took 0.749s\n",
      "  training loss:\t\t0.003554\n",
      "Epoch 424 of 500 took 0.751s\n",
      "  training loss:\t\t0.003554\n",
      "Epoch 425 of 500 took 0.756s\n",
      "  training loss:\t\t0.003554\n",
      "Epoch 426 of 500 took 0.754s\n",
      "  training loss:\t\t0.003554\n",
      "Epoch 427 of 500 took 0.745s\n",
      "  training loss:\t\t0.003554\n",
      "Epoch 428 of 500 took 0.746s\n",
      "  training loss:\t\t0.003553\n",
      "Epoch 429 of 500 took 0.744s\n",
      "  training loss:\t\t0.003553\n",
      "Epoch 430 of 500 took 0.746s\n",
      "  training loss:\t\t0.003553\n",
      "Epoch 431 of 500 took 0.747s\n",
      "  training loss:\t\t0.003553\n",
      "Epoch 432 of 500 took 0.745s\n",
      "  training loss:\t\t0.003553\n",
      "Epoch 433 of 500 took 0.740s\n",
      "  training loss:\t\t0.003553\n",
      "Epoch 434 of 500 took 0.740s\n",
      "  training loss:\t\t0.003552\n",
      "Epoch 435 of 500 took 0.746s\n",
      "  training loss:\t\t0.003552\n",
      "Epoch 436 of 500 took 0.758s\n",
      "  training loss:\t\t0.003552\n",
      "Epoch 437 of 500 took 0.737s\n",
      "  training loss:\t\t0.003552\n",
      "Epoch 438 of 500 took 0.737s\n",
      "  training loss:\t\t0.003552\n",
      "Epoch 439 of 500 took 0.750s\n",
      "  training loss:\t\t0.003552\n",
      "Epoch 440 of 500 took 0.758s\n",
      "  training loss:\t\t0.003551\n",
      "Epoch 441 of 500 took 0.773s\n",
      "  training loss:\t\t0.003551\n",
      "Epoch 442 of 500 took 0.764s\n",
      "  training loss:\t\t0.003551\n",
      "Epoch 443 of 500 took 0.758s\n",
      "  training loss:\t\t0.003551\n",
      "Epoch 444 of 500 took 0.749s\n",
      "  training loss:\t\t0.003551\n",
      "Epoch 445 of 500 took 0.738s\n",
      "  training loss:\t\t0.003551\n",
      "Epoch 446 of 500 took 0.739s\n",
      "  training loss:\t\t0.003550\n",
      "Epoch 447 of 500 took 0.739s\n",
      "  training loss:\t\t0.003550\n",
      "Epoch 448 of 500 took 0.739s\n",
      "  training loss:\t\t0.003550\n",
      "Epoch 449 of 500 took 0.735s\n",
      "  training loss:\t\t0.003550\n",
      "Epoch 450 of 500 took 0.738s\n",
      "  training loss:\t\t0.003550\n",
      "Epoch 451 of 500 took 0.738s\n",
      "  training loss:\t\t0.003549\n",
      "Epoch 452 of 500 took 0.737s\n",
      "  training loss:\t\t0.003549\n",
      "Epoch 453 of 500 took 0.745s\n",
      "  training loss:\t\t0.003549\n",
      "Epoch 454 of 500 took 0.772s\n",
      "  training loss:\t\t0.003549\n",
      "Epoch 455 of 500 took 0.770s\n",
      "  training loss:\t\t0.003549\n",
      "Epoch 456 of 500 took 0.745s\n",
      "  training loss:\t\t0.003549\n",
      "Epoch 457 of 500 took 0.718s\n",
      "  training loss:\t\t0.003548\n",
      "Epoch 458 of 500 took 0.735s\n",
      "  training loss:\t\t0.003548\n",
      "Epoch 459 of 500 took 0.768s\n",
      "  training loss:\t\t0.003548\n",
      "Epoch 460 of 500 took 0.706s\n",
      "  training loss:\t\t0.003548\n",
      "Epoch 461 of 500 took 0.704s\n",
      "  training loss:\t\t0.003548\n",
      "Epoch 462 of 500 took 0.705s\n",
      "  training loss:\t\t0.003548\n",
      "Epoch 463 of 500 took 0.756s\n",
      "  training loss:\t\t0.003547\n",
      "Epoch 464 of 500 took 0.710s\n",
      "  training loss:\t\t0.003547\n",
      "Epoch 465 of 500 took 0.703s\n",
      "  training loss:\t\t0.003547\n",
      "Epoch 466 of 500 took 0.705s\n",
      "  training loss:\t\t0.003547\n",
      "Epoch 467 of 500 took 0.764s\n",
      "  training loss:\t\t0.003547\n",
      "Epoch 468 of 500 took 0.766s\n",
      "  training loss:\t\t0.003547\n",
      "Epoch 469 of 500 took 0.707s\n",
      "  training loss:\t\t0.003546\n",
      "Epoch 470 of 500 took 0.704s\n",
      "  training loss:\t\t0.003546\n",
      "Epoch 471 of 500 took 0.705s\n",
      "  training loss:\t\t0.003546\n",
      "Epoch 472 of 500 took 0.707s\n",
      "  training loss:\t\t0.003546\n",
      "Epoch 473 of 500 took 0.705s\n",
      "  training loss:\t\t0.003546\n",
      "Epoch 474 of 500 took 0.706s\n",
      "  training loss:\t\t0.003546\n",
      "Epoch 475 of 500 took 0.703s\n",
      "  training loss:\t\t0.003545\n",
      "Epoch 476 of 500 took 0.707s\n",
      "  training loss:\t\t0.003545\n",
      "Epoch 477 of 500 took 0.703s\n",
      "  training loss:\t\t0.003545\n",
      "Epoch 478 of 500 took 0.706s\n",
      "  training loss:\t\t0.003545\n",
      "Epoch 479 of 500 took 0.750s\n",
      "  training loss:\t\t0.003545\n",
      "Epoch 480 of 500 took 0.744s\n",
      "  training loss:\t\t0.003545\n",
      "Epoch 481 of 500 took 0.766s\n",
      "  training loss:\t\t0.003544\n",
      "Epoch 482 of 500 took 0.736s\n",
      "  training loss:\t\t0.003544\n",
      "Epoch 483 of 500 took 0.757s\n",
      "  training loss:\t\t0.003544\n",
      "Epoch 484 of 500 took 0.713s\n",
      "  training loss:\t\t0.003544\n",
      "Epoch 485 of 500 took 0.706s\n",
      "  training loss:\t\t0.003544\n",
      "Epoch 486 of 500 took 0.706s\n",
      "  training loss:\t\t0.003544\n",
      "Epoch 487 of 500 took 0.755s\n",
      "  training loss:\t\t0.003543\n",
      "Epoch 488 of 500 took 0.731s\n",
      "  training loss:\t\t0.003543\n",
      "Epoch 489 of 500 took 0.757s\n",
      "  training loss:\t\t0.003543\n",
      "Epoch 490 of 500 took 0.760s\n",
      "  training loss:\t\t0.003543\n",
      "Epoch 491 of 500 took 0.757s\n",
      "  training loss:\t\t0.003543\n",
      "Epoch 492 of 500 took 0.767s\n",
      "  training loss:\t\t0.003543\n",
      "Epoch 493 of 500 took 0.764s\n",
      "  training loss:\t\t0.003542\n",
      "Epoch 494 of 500 took 0.748s\n",
      "  training loss:\t\t0.003542\n",
      "Epoch 495 of 500 took 0.754s\n",
      "  training loss:\t\t0.003542\n",
      "Epoch 496 of 500 took 0.762s\n",
      "  training loss:\t\t0.003542\n",
      "Epoch 497 of 500 took 0.766s\n",
      "  training loss:\t\t0.003542\n",
      "Epoch 498 of 500 took 0.770s\n",
      "  training loss:\t\t0.003542\n",
      "Epoch 499 of 500 took 0.765s\n",
      "  training loss:\t\t0.003541\n",
      "Epoch 500 of 500 took 0.760s\n",
      "  training loss:\t\t0.003541\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "    loss, params, learning_rate=0.02, momentum=0.975)\n",
    "# updates = lasagne.updates.rmsprop(\n",
    "#     loss, params[10:12], learning_rate=0.01)\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates,on_unused_input='warn')\n",
    "print(\"Starting training...\")\n",
    "best_err = 0.003620\n",
    "for epoch in range(num_epochs):\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(train_face, train_face_out, 200, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "        # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    if train_err / train_batches < best_err:\n",
    "        best_err = train_err / train_batches\n",
    "        np.savez('CAE_face.npz', *lasagne.layers.get_all_param_values(network))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
