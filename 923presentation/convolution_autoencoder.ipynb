{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convolutional autoencoder with MNIST\n",
    "###  inspired by the [Swarbrick's blog](https://swarbrickjones.wordpress.com/2015/04/29/convolutional-autoencoders-in-pythontheanolasagne/) and Professor [G.E.Hinton's paper](http://science.sciencemag.org/content/313/5786/504)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda_convnet (faster)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 750 Ti (CNMeM is disabled, CuDNN 3007)\n",
      "/home/rui/.pyenv/versions/3.5.0/envs/cae/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import os, sys, urllib, gzip\n",
    "sys.path.append('/home/rui/pylearn2')\n",
    "from __future__ import print_function\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "import numpy as np\n",
    "from lasagne.layers import get_output, InputLayer, DenseLayer, Upscale2DLayer, ReshapeLayer\n",
    "from lasagne.nonlinearities import rectify, leaky_rectify, tanh\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from lasagne.objectives import categorical_crossentropy\n",
    "import pylearn2\n",
    "from lasagne.layers.cuda_convnet import Conv2DCCLayer as Conv2DLayerFast\n",
    "from lasagne.regularization import regularize_layer_params, l2, l1\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import lasagne\n",
    "from lasagne.layers import Conv2DLayer as Conv2DLayerSlow\n",
    "from lasagne.layers import MaxPool2DLayer as MaxPool2DLayerSlow\n",
    "try:\n",
    "    from lasagne.layers.cuda_convnet import Conv2DCCLayer as Conv2DLayerFast\n",
    "    from lasagne.layers.cuda_convnet import MaxPool2DCCLayer as MaxPool2DLayerFast\n",
    "    print('Using cuda_convnet (faster)')\n",
    "except ImportError:\n",
    "    from lasagne.layers import Conv2DLayer as Conv2DLayerFast\n",
    "    from lasagne.layers import MaxPool2DLayer as MaxPool2DLayerFast\n",
    "    print('Using lasagne.layers (slower)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = gzip.open('/home/rui/Downloads/mnist.pkl.gz', 'rb')\n",
    "try:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "except:\n",
    "    train_set, valid_set, test_set = pickle.load(f)\n",
    "f.close()\n",
    "X, y = train_set\n",
    "X = np.reshape(X, (-1, 1, 28, 28))\n",
    "X_out = X.reshape((X.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_num_filters = 16\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "encode_size = 16\n",
    "dense_mid_size = 128\n",
    "pad_in = 'valid'    \n",
    "pad_out = 'full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn(input_var=None):\n",
    "    \n",
    "    network = InputLayer(shape=(None,  X.shape[1], X.shape[2], X.shape[3]),input_var=input_var)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "\n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "\n",
    "    network = MaxPool2DLayerFast(network, pool_size=pool_size)\n",
    "\n",
    "    network = Conv2DLayerFast(network, num_filters=2*conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "\n",
    "    network = MaxPool2DLayerFast(network, pool_size=pool_size)\n",
    "    \n",
    "    network = ReshapeLayer(network, shape =(([0], -1)))\n",
    "    \n",
    "    network = DenseLayer(network, num_units= dense_mid_size, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    encode_layer = DenseLayer(network, name= 'encode', num_units= encode_size, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "    network = DenseLayer(encode_layer, num_units= dense_mid_size, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    network = DenseLayer(network, num_units= 800, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    network = ReshapeLayer(network, shape =(([0], 2*conv_num_filters, 5, 5)))\n",
    "    \n",
    "    network = Upscale2DLayer(network, scale_factor = pool_size)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_out)\n",
    "\n",
    "    network = Upscale2DLayer(network, scale_factor = pool_size)\n",
    "    \n",
    "    network = Conv2DLayerSlow(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_out)\n",
    "\n",
    "    network = Conv2DLayerSlow(network, num_filters=1, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.sigmoid, filter_size=filter_size, pad=pad_out)\n",
    "    \n",
    "    network = ReshapeLayer(network, shape =(([0], -1)))\n",
    "\n",
    "    return network, encode_layer\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 30 took 24.963s\n",
      "  training loss:\t\t0.086922\n",
      "Epoch 2 of 30 took 13.797s\n",
      "  training loss:\t\t0.041286\n",
      "Epoch 3 of 30 took 13.801s\n",
      "  training loss:\t\t0.028758\n",
      "Epoch 4 of 30 took 13.826s\n",
      "  training loss:\t\t0.025219\n",
      "Epoch 5 of 30 took 13.810s\n",
      "  training loss:\t\t0.023327\n",
      "Epoch 6 of 30 took 17.382s\n",
      "  training loss:\t\t0.022095\n",
      "Epoch 7 of 30 took 13.828s\n",
      "  training loss:\t\t0.021484\n",
      "Epoch 8 of 30 took 13.839s\n",
      "  training loss:\t\t0.021024\n",
      "Epoch 9 of 30 took 13.835s\n",
      "  training loss:\t\t0.020552\n",
      "Epoch 10 of 30 took 13.832s\n",
      "  training loss:\t\t0.020142\n",
      "Epoch 11 of 30 took 17.407s\n",
      "  training loss:\t\t0.019707\n",
      "Epoch 12 of 30 took 13.848s\n",
      "  training loss:\t\t0.019458\n",
      "Epoch 13 of 30 took 13.843s\n",
      "  training loss:\t\t0.019248\n",
      "Epoch 14 of 30 took 13.847s\n",
      "  training loss:\t\t0.019057\n",
      "Epoch 15 of 30 took 13.843s\n",
      "  training loss:\t\t0.018876\n",
      "Epoch 16 of 30 took 17.495s\n",
      "  training loss:\t\t0.018654\n",
      "Epoch 17 of 30 took 13.846s\n",
      "  training loss:\t\t0.018521\n",
      "Epoch 18 of 30 took 13.844s\n",
      "  training loss:\t\t0.018399\n",
      "Epoch 19 of 30 took 13.848s\n",
      "  training loss:\t\t0.018286\n",
      "Epoch 20 of 30 took 13.847s\n",
      "  training loss:\t\t0.018181\n",
      "Epoch 21 of 30 took 17.384s\n",
      "  training loss:\t\t0.018058\n",
      "Epoch 22 of 30 took 13.859s\n",
      "  training loss:\t\t0.017976\n",
      "Epoch 23 of 30 took 13.847s\n",
      "  training loss:\t\t0.017897\n",
      "Epoch 24 of 30 took 13.846s\n",
      "  training loss:\t\t0.017821\n",
      "Epoch 25 of 30 took 13.866s\n",
      "  training loss:\t\t0.017747\n",
      "Epoch 26 of 30 took 17.397s\n",
      "  training loss:\t\t0.017671\n",
      "Epoch 27 of 30 took 13.860s\n",
      "  training loss:\t\t0.017615\n",
      "Epoch 28 of 30 took 13.861s\n",
      "  training loss:\t\t0.017560\n",
      "Epoch 29 of 30 took 13.870s\n",
      "  training loss:\t\t0.017506\n",
      "Epoch 30 of 30 took 13.866s\n",
      "  training loss:\t\t0.017453\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.matrix('targets')\n",
    "learnrate=0.2\n",
    "# Create neural network model (depending on first command line parameter)\n",
    "print(\"Building model and compiling functions...\")\n",
    "network, encode_layer = build_cnn(input_var)\n",
    "l2_penalty = regularize_layer_params(network, l2)\n",
    "l1_penalty = regularize_layer_params(network, l1)\n",
    "reconstructed = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.squared_error(reconstructed, target_var)\n",
    "loss = loss.mean()\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    if epoch % 5 == 0:\n",
    "        learnrate = learnrate * 0.8\n",
    "        updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=learnrate, momentum=0.9)\n",
    "        train_fn = theano.function([input_var, target_var], loss, updates=updates,on_unused_input='warn')\n",
    "        \n",
    "    for batch in iterate_minibatches(X, X_out, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "\n",
    "\n",
    "\n",
    "    # Optionally, you could now dump the network weights to a file like this:\n",
    "np.savez('CAE_MNIST.npz', *lasagne.layers.get_all_param_values(network))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CAE_predict = theano.function([input_var], [lasagne.layers.get_output(network),lasagne.layers.get_output(encode_layer)])\n",
    "X_pred = np.zeros((50000,784))\n",
    "X_encode = np.zeros((50000,encode_size))\n",
    "i = 0\n",
    "for batch in iterate_minibatches(X, X_out, 1000, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    a, b = CAE_predict(inputs)\n",
    "    X_pred[1000*i:1000*(i+1)] = a\n",
    "    X_encode[1000*i:1000*(i+1)] = b\n",
    "    i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savez('CAE_MNIST_learned_feature.npz', X_pred,X_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313600112"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
