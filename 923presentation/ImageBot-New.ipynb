{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 750 Ti (CNMeM is disabled, cuDNN Version is too old. Update to v5, was 3007.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda_convnet (faster)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, urllib, gzip\n",
    "sys.path.append('/home/rui/pylearn2')\n",
    "from __future__ import print_function\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "import numpy as np\n",
    "from lasagne.layers import get_output, InputLayer, DenseLayer, Upscale2DLayer, ReshapeLayer\n",
    "from lasagne.nonlinearities import rectify, leaky_rectify, tanh\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from lasagne.objectives import categorical_crossentropy\n",
    "import pylearn2\n",
    "from lasagne.layers.cuda_convnet import Conv2DCCLayer as Conv2DLayerFast\n",
    "from lasagne.regularization import regularize_layer_params, l2, l1\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import lasagne\n",
    "from lasagne.layers import Conv2DLayer as Conv2DLayerSlow\n",
    "from lasagne.layers import MaxPool2DLayer as MaxPool2DLayerSlow\n",
    "try:\n",
    "    from lasagne.layers.cuda_convnet import Conv2DCCLayer as Conv2DLayerFast\n",
    "    from lasagne.layers.cuda_convnet import MaxPool2DCCLayer as MaxPool2DLayerFast\n",
    "    print('Using cuda_convnet (faster)')\n",
    "except ImportError:\n",
    "    from lasagne.layers import Conv2DLayer as Conv2DLayerFast\n",
    "    from lasagne.layers import MaxPool2DLayer as MaxPool2DLayerFast\n",
    "    print('Using lasagne.layers (slower)')\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # Change matplotlib backend, in case we have no X server running..\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from IPython.display import Image as IPImage\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load('output/mars.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 = data['arr_0']\n",
    "data_out1 = data['arr_1']\n",
    "data2 = data['arr_2']\n",
    "data_out2 = data['arr_3']\n",
    "data3 = data['arr_4']\n",
    "data_out3 = data['arr_5']\n",
    "data4 = data['arr_6']\n",
    "data_out4 = data['arr_7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((data1,data2,data3,data4))\n",
    "X = np.asarray(X, dtype = np.float32)\n",
    "X = np.reshape(X, (200, 1, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X / 255\n",
    "X_out = X.reshape((X.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_picture_array(X, rescale=2):\n",
    "    array = X.reshape(64,64)\n",
    "    array = np.clip(array, a_min = 0, a_max = 255)\n",
    "    return  array.repeat(rescale, axis = 0).repeat(rescale, axis = 1).astype(np.uint8())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAP8ElEQVR4nM3byY4k25Wd4c96M2+j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\ncaN0b+NO6/ESquH/B9DOtdJfA3GzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic_array = get_picture_array(X[2]*255)\n",
    "image = Image.fromarray(pic_array)\n",
    "image.save('./output/temp.png', format=\"PNG\")  \n",
    "IPImage('./output/temp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_num_filters = 16\n",
    "filter_size = 5\n",
    "pool_size = 2\n",
    "encode_size = 64\n",
    "pad_in = 'valid'    \n",
    "pad_out = 'full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn(input_var=None):\n",
    "    \n",
    "    network = InputLayer(shape=(None,  X.shape[1], X.shape[2], X.shape[3]),input_var=input_var)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "\n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "\n",
    "    network = MaxPool2DLayerFast(network, pool_size=pool_size)\n",
    "\n",
    "    network = Conv2DLayerFast(network, num_filters=2*conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=2*conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "\n",
    "    network = MaxPool2DLayerFast(network, pool_size=pool_size)\n",
    "    \n",
    "    network = ReshapeLayer(network, shape =(([0], -1)))\n",
    "    \n",
    "    \n",
    "    encode_layer = DenseLayer(network, name= 'encode', num_units= encode_size, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    \n",
    "    network = DenseLayer(encode_layer, num_units= 3200, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    network = ReshapeLayer(network, shape =(([0], 2*conv_num_filters, 10, 10)))\n",
    "    \n",
    "    network = Upscale2DLayer(network, scale_factor = pool_size)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=2 * conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_out)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_out)\n",
    "\n",
    "    network = Upscale2DLayer(network, scale_factor = pool_size)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_out)\n",
    "\n",
    "    network = Conv2DLayerSlow(network, num_filters=1, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.sigmoid, filter_size=filter_size, pad=pad_out)\n",
    "    \n",
    "    network = ReshapeLayer(network, shape =(([0], -1)))\n",
    "\n",
    "    return network\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('./output/autoencoder_mars.npz', *lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 1000 took 0.717s\n",
      "training loss:\t\t0.027666\n",
      "Epoch 2 of 1000 took 0.702s\n",
      "training loss:\t\t0.027666\n",
      "Epoch 3 of 1000 took 0.706s\n",
      "training loss:\t\t0.027666\n",
      "Epoch 4 of 1000 took 0.705s\n",
      "training loss:\t\t0.027666\n",
      "Epoch 5 of 1000 took 0.704s\n",
      "training loss:\t\t0.027665\n",
      "Epoch 6 of 1000 took 0.703s\n",
      "training loss:\t\t0.027665\n",
      "Epoch 7 of 1000 took 0.704s\n",
      "training loss:\t\t0.027665\n",
      "Epoch 8 of 1000 took 0.702s\n",
      "training loss:\t\t0.027665\n",
      "Epoch 9 of 1000 took 0.704s\n",
      "training loss:\t\t0.027665\n",
      "Epoch 10 of 1000 took 0.703s\n",
      "training loss:\t\t0.027664\n",
      "Epoch 11 of 1000 took 0.703s\n",
      "training loss:\t\t0.027664\n",
      "Epoch 12 of 1000 took 0.703s\n",
      "training loss:\t\t0.027664\n",
      "Epoch 13 of 1000 took 0.704s\n",
      "training loss:\t\t0.027664\n",
      "Epoch 14 of 1000 took 0.704s\n",
      "training loss:\t\t0.027663\n",
      "Epoch 15 of 1000 took 0.703s\n",
      "training loss:\t\t0.027663\n",
      "Epoch 16 of 1000 took 0.703s\n",
      "training loss:\t\t0.027662\n",
      "Epoch 17 of 1000 took 0.704s\n",
      "training loss:\t\t0.027662\n",
      "Epoch 18 of 1000 took 0.706s\n",
      "training loss:\t\t0.027662\n",
      "Epoch 19 of 1000 took 0.705s\n",
      "training loss:\t\t0.027661\n",
      "Epoch 20 of 1000 took 0.704s\n",
      "training loss:\t\t0.027661\n",
      "Epoch 21 of 1000 took 0.703s\n",
      "training loss:\t\t0.027660\n",
      "Epoch 22 of 1000 took 0.705s\n",
      "training loss:\t\t0.027660\n",
      "Epoch 23 of 1000 took 0.706s\n",
      "training loss:\t\t0.027660\n",
      "Epoch 24 of 1000 took 0.704s\n",
      "training loss:\t\t0.027659\n",
      "Epoch 25 of 1000 took 0.705s\n",
      "training loss:\t\t0.027659\n",
      "Epoch 26 of 1000 took 0.705s\n",
      "training loss:\t\t0.027658\n",
      "Epoch 27 of 1000 took 0.707s\n",
      "training loss:\t\t0.027658\n",
      "Epoch 28 of 1000 took 0.704s\n",
      "training loss:\t\t0.027657\n",
      "Epoch 29 of 1000 took 0.703s\n",
      "training loss:\t\t0.027657\n",
      "Epoch 30 of 1000 took 0.705s\n",
      "training loss:\t\t0.027656\n",
      "Epoch 31 of 1000 took 0.704s\n",
      "training loss:\t\t0.027656\n",
      "Epoch 32 of 1000 took 0.704s\n",
      "training loss:\t\t0.027655\n",
      "Epoch 33 of 1000 took 0.703s\n",
      "training loss:\t\t0.027654\n",
      "Epoch 34 of 1000 took 0.703s\n",
      "training loss:\t\t0.027654\n",
      "Epoch 35 of 1000 took 0.704s\n",
      "training loss:\t\t0.027653\n",
      "Epoch 36 of 1000 took 0.705s\n",
      "training loss:\t\t0.027653\n",
      "Epoch 37 of 1000 took 0.703s\n",
      "training loss:\t\t0.027652\n",
      "Epoch 38 of 1000 took 0.704s\n",
      "training loss:\t\t0.027651\n",
      "Epoch 39 of 1000 took 0.704s\n",
      "training loss:\t\t0.027651\n",
      "Epoch 40 of 1000 took 0.702s\n",
      "training loss:\t\t0.027650\n",
      "Epoch 41 of 1000 took 0.706s\n",
      "training loss:\t\t0.027650\n",
      "Epoch 42 of 1000 took 0.705s\n",
      "training loss:\t\t0.027649\n",
      "Epoch 43 of 1000 took 0.705s\n",
      "training loss:\t\t0.027648\n",
      "Epoch 44 of 1000 took 0.705s\n",
      "training loss:\t\t0.027648\n",
      "Epoch 45 of 1000 took 0.706s\n",
      "training loss:\t\t0.027647\n",
      "Epoch 46 of 1000 took 0.706s\n",
      "training loss:\t\t0.027646\n",
      "Epoch 47 of 1000 took 0.704s\n",
      "training loss:\t\t0.027646\n",
      "Epoch 48 of 1000 took 0.703s\n",
      "training loss:\t\t0.027645\n",
      "Epoch 49 of 1000 took 0.706s\n",
      "training loss:\t\t0.027644\n",
      "Epoch 50 of 1000 took 0.705s\n",
      "training loss:\t\t0.027644\n",
      "Epoch 51 of 1000 took 0.704s\n",
      "training loss:\t\t0.027643\n",
      "Epoch 52 of 1000 took 0.705s\n",
      "training loss:\t\t0.027642\n",
      "Epoch 53 of 1000 took 0.705s\n",
      "training loss:\t\t0.027642\n",
      "Epoch 54 of 1000 took 0.703s\n",
      "training loss:\t\t0.027641\n",
      "Epoch 55 of 1000 took 0.703s\n",
      "training loss:\t\t0.027640\n",
      "Epoch 56 of 1000 took 0.704s\n",
      "training loss:\t\t0.027640\n",
      "Epoch 57 of 1000 took 0.703s\n",
      "training loss:\t\t0.027639\n",
      "Epoch 58 of 1000 took 0.708s\n",
      "training loss:\t\t0.027638\n",
      "Epoch 59 of 1000 took 0.707s\n",
      "training loss:\t\t0.027637\n",
      "Epoch 60 of 1000 took 0.707s\n",
      "training loss:\t\t0.027637\n",
      "Epoch 61 of 1000 took 0.701s\n",
      "training loss:\t\t0.027636\n",
      "Epoch 62 of 1000 took 0.702s\n",
      "training loss:\t\t0.027635\n",
      "Epoch 63 of 1000 took 0.703s\n",
      "training loss:\t\t0.027634\n",
      "Epoch 64 of 1000 took 0.705s\n",
      "training loss:\t\t0.027634\n",
      "Epoch 65 of 1000 took 0.704s\n",
      "training loss:\t\t0.027633\n",
      "Epoch 66 of 1000 took 0.704s\n",
      "training loss:\t\t0.027632\n",
      "Epoch 67 of 1000 took 0.704s\n",
      "training loss:\t\t0.027631\n",
      "Epoch 68 of 1000 took 0.707s\n",
      "training loss:\t\t0.027631\n",
      "Epoch 69 of 1000 took 0.702s\n",
      "training loss:\t\t0.027630\n",
      "Epoch 70 of 1000 took 0.702s\n",
      "training loss:\t\t0.027629\n",
      "Epoch 71 of 1000 took 0.701s\n",
      "training loss:\t\t0.027628\n",
      "Epoch 72 of 1000 took 0.704s\n",
      "training loss:\t\t0.027628\n",
      "Epoch 73 of 1000 took 0.706s\n",
      "training loss:\t\t0.027627\n",
      "Epoch 74 of 1000 took 0.705s\n",
      "training loss:\t\t0.027626\n",
      "Epoch 75 of 1000 took 0.704s\n",
      "training loss:\t\t0.027625\n",
      "Epoch 76 of 1000 took 0.704s\n",
      "training loss:\t\t0.027625\n",
      "Epoch 77 of 1000 took 0.703s\n",
      "training loss:\t\t0.027624\n",
      "Epoch 78 of 1000 took 0.704s\n",
      "training loss:\t\t0.027623\n",
      "Epoch 79 of 1000 took 0.702s\n",
      "training loss:\t\t0.027622\n",
      "Epoch 80 of 1000 took 0.702s\n",
      "training loss:\t\t0.027621\n",
      "Epoch 81 of 1000 took 0.703s\n",
      "training loss:\t\t0.027621\n",
      "Epoch 82 of 1000 took 0.705s\n",
      "training loss:\t\t0.027620\n",
      "Epoch 83 of 1000 took 0.704s\n",
      "training loss:\t\t0.027619\n",
      "Epoch 84 of 1000 took 0.703s\n",
      "training loss:\t\t0.027618\n",
      "Epoch 85 of 1000 took 0.704s\n",
      "training loss:\t\t0.027617\n",
      "Epoch 86 of 1000 took 0.704s\n",
      "training loss:\t\t0.027617\n",
      "Epoch 87 of 1000 took 0.749s\n",
      "training loss:\t\t0.027616\n",
      "Epoch 88 of 1000 took 0.741s\n",
      "training loss:\t\t0.027615\n",
      "Epoch 89 of 1000 took 0.740s\n",
      "training loss:\t\t0.027614\n",
      "Epoch 90 of 1000 took 0.730s\n",
      "training loss:\t\t0.027613\n",
      "Epoch 91 of 1000 took 0.745s\n",
      "training loss:\t\t0.027613\n",
      "Epoch 92 of 1000 took 0.734s\n",
      "training loss:\t\t0.027612\n",
      "Epoch 93 of 1000 took 0.732s\n",
      "training loss:\t\t0.027611\n",
      "Epoch 94 of 1000 took 0.730s\n",
      "training loss:\t\t0.027610\n",
      "Epoch 95 of 1000 took 0.731s\n",
      "training loss:\t\t0.027609\n",
      "Epoch 96 of 1000 took 0.704s\n",
      "training loss:\t\t0.027608\n",
      "Epoch 97 of 1000 took 0.734s\n",
      "training loss:\t\t0.027608\n",
      "Epoch 98 of 1000 took 0.701s\n",
      "training loss:\t\t0.027607\n",
      "Epoch 99 of 1000 took 0.701s\n",
      "training loss:\t\t0.027606\n",
      "Epoch 100 of 1000 took 0.701s\n",
      "training loss:\t\t0.027605\n",
      "Epoch 101 of 1000 took 0.701s\n",
      "training loss:\t\t0.027604\n",
      "Epoch 102 of 1000 took 0.728s\n",
      "training loss:\t\t0.027603\n",
      "Epoch 103 of 1000 took 0.758s\n",
      "training loss:\t\t0.027603\n",
      "Epoch 104 of 1000 took 0.704s\n",
      "training loss:\t\t0.027602\n",
      "Epoch 105 of 1000 took 0.715s\n",
      "training loss:\t\t0.027601\n",
      "Epoch 106 of 1000 took 0.752s\n",
      "training loss:\t\t0.027600\n",
      "Epoch 107 of 1000 took 0.728s\n",
      "training loss:\t\t0.027599\n",
      "Epoch 108 of 1000 took 0.707s\n",
      "training loss:\t\t0.027598\n",
      "Epoch 109 of 1000 took 0.732s\n",
      "training loss:\t\t0.027597\n",
      "Epoch 110 of 1000 took 0.706s\n",
      "training loss:\t\t0.027596\n",
      "Epoch 111 of 1000 took 0.705s\n",
      "training loss:\t\t0.027596\n",
      "Epoch 112 of 1000 took 0.720s\n",
      "training loss:\t\t0.027595\n",
      "Epoch 113 of 1000 took 0.704s\n",
      "training loss:\t\t0.027594\n",
      "Epoch 114 of 1000 took 0.708s\n",
      "training loss:\t\t0.027593\n",
      "Epoch 115 of 1000 took 0.738s\n",
      "training loss:\t\t0.027592\n",
      "Epoch 116 of 1000 took 0.706s\n",
      "training loss:\t\t0.027591\n",
      "Epoch 117 of 1000 took 0.707s\n",
      "training loss:\t\t0.027590\n",
      "Epoch 118 of 1000 took 0.707s\n",
      "training loss:\t\t0.027589\n",
      "Epoch 119 of 1000 took 0.781s\n",
      "training loss:\t\t0.027588\n",
      "Epoch 120 of 1000 took 0.719s\n",
      "training loss:\t\t0.027587\n",
      "Epoch 121 of 1000 took 0.708s\n",
      "training loss:\t\t0.027586\n",
      "Epoch 122 of 1000 took 0.734s\n",
      "training loss:\t\t0.027585\n",
      "Epoch 123 of 1000 took 0.715s\n",
      "training loss:\t\t0.027584\n",
      "Epoch 124 of 1000 took 0.716s\n",
      "training loss:\t\t0.027584\n",
      "Epoch 125 of 1000 took 0.704s\n",
      "training loss:\t\t0.027583\n",
      "Epoch 126 of 1000 took 0.706s\n",
      "training loss:\t\t0.027582\n",
      "Epoch 127 of 1000 took 0.705s\n",
      "training loss:\t\t0.027581\n",
      "Epoch 128 of 1000 took 0.705s\n",
      "training loss:\t\t0.027580\n",
      "Epoch 129 of 1000 took 0.704s\n",
      "training loss:\t\t0.027579\n",
      "Epoch 130 of 1000 took 0.704s\n",
      "training loss:\t\t0.027578\n",
      "Epoch 131 of 1000 took 0.703s\n",
      "training loss:\t\t0.027577\n",
      "Epoch 132 of 1000 took 0.705s\n",
      "training loss:\t\t0.027576\n",
      "Epoch 133 of 1000 took 0.705s\n",
      "training loss:\t\t0.027575\n",
      "Epoch 134 of 1000 took 0.704s\n",
      "training loss:\t\t0.027573\n",
      "Epoch 135 of 1000 took 0.704s\n",
      "training loss:\t\t0.027572\n",
      "Epoch 136 of 1000 took 0.705s\n",
      "training loss:\t\t0.027571\n",
      "Epoch 137 of 1000 took 0.705s\n",
      "training loss:\t\t0.027570\n",
      "Epoch 138 of 1000 took 0.702s\n",
      "training loss:\t\t0.027569\n",
      "Epoch 139 of 1000 took 0.701s\n",
      "training loss:\t\t0.027568\n",
      "Epoch 140 of 1000 took 0.702s\n",
      "training loss:\t\t0.027567\n",
      "Epoch 141 of 1000 took 0.704s\n",
      "training loss:\t\t0.027566\n",
      "Epoch 142 of 1000 took 0.706s\n",
      "training loss:\t\t0.027565\n",
      "Epoch 143 of 1000 took 0.705s\n",
      "training loss:\t\t0.027564\n",
      "Epoch 144 of 1000 took 0.705s\n",
      "training loss:\t\t0.027563\n",
      "Epoch 145 of 1000 took 0.706s\n",
      "training loss:\t\t0.027561\n",
      "Epoch 146 of 1000 took 0.704s\n",
      "training loss:\t\t0.027560\n",
      "Epoch 147 of 1000 took 0.702s\n",
      "training loss:\t\t0.027559\n",
      "Epoch 148 of 1000 took 0.701s\n",
      "training loss:\t\t0.027558\n",
      "Epoch 149 of 1000 took 0.702s\n",
      "training loss:\t\t0.027557\n",
      "Epoch 150 of 1000 took 0.704s\n",
      "training loss:\t\t0.027556\n",
      "Epoch 151 of 1000 took 0.703s\n",
      "training loss:\t\t0.027554\n",
      "Epoch 152 of 1000 took 0.705s\n",
      "training loss:\t\t0.027553\n",
      "Epoch 153 of 1000 took 0.702s\n",
      "training loss:\t\t0.027552\n",
      "Epoch 154 of 1000 took 0.704s\n",
      "training loss:\t\t0.027551\n",
      "Epoch 155 of 1000 took 0.711s\n",
      "training loss:\t\t0.027549\n",
      "Epoch 156 of 1000 took 0.704s\n",
      "training loss:\t\t0.027548\n",
      "Epoch 157 of 1000 took 0.703s\n",
      "training loss:\t\t0.027547\n",
      "Epoch 158 of 1000 took 0.704s\n",
      "training loss:\t\t0.027545\n",
      "Epoch 159 of 1000 took 0.705s\n",
      "training loss:\t\t0.027544\n",
      "Epoch 160 of 1000 took 0.706s\n",
      "training loss:\t\t0.027543\n",
      "Epoch 161 of 1000 took 0.705s\n",
      "training loss:\t\t0.027541\n",
      "Epoch 162 of 1000 took 0.701s\n",
      "training loss:\t\t0.027540\n",
      "Epoch 163 of 1000 took 0.703s\n",
      "training loss:\t\t0.027539\n",
      "Epoch 164 of 1000 took 0.704s\n",
      "training loss:\t\t0.027537\n",
      "Epoch 165 of 1000 took 0.706s\n",
      "training loss:\t\t0.027536\n",
      "Epoch 166 of 1000 took 0.703s\n",
      "training loss:\t\t0.027534\n",
      "Epoch 167 of 1000 took 0.705s\n",
      "training loss:\t\t0.027533\n",
      "Epoch 168 of 1000 took 0.707s\n",
      "training loss:\t\t0.027531\n",
      "Epoch 169 of 1000 took 0.704s\n",
      "training loss:\t\t0.027530\n",
      "Epoch 170 of 1000 took 0.701s\n",
      "training loss:\t\t0.027528\n",
      "Epoch 171 of 1000 took 0.701s\n",
      "training loss:\t\t0.027527\n",
      "Epoch 172 of 1000 took 0.701s\n",
      "training loss:\t\t0.027525\n",
      "Epoch 173 of 1000 took 0.704s\n",
      "training loss:\t\t0.027524\n",
      "Epoch 174 of 1000 took 0.707s\n",
      "training loss:\t\t0.027522\n",
      "Epoch 175 of 1000 took 0.704s\n",
      "training loss:\t\t0.027520\n",
      "Epoch 176 of 1000 took 0.703s\n",
      "training loss:\t\t0.027519\n",
      "Epoch 177 of 1000 took 0.704s\n",
      "training loss:\t\t0.027517\n",
      "Epoch 178 of 1000 took 0.707s\n",
      "training loss:\t\t0.027515\n",
      "Epoch 179 of 1000 took 0.703s\n",
      "training loss:\t\t0.027514\n",
      "Epoch 180 of 1000 took 0.703s\n",
      "training loss:\t\t0.027512\n",
      "Epoch 181 of 1000 took 0.703s\n",
      "training loss:\t\t0.027510\n",
      "Epoch 182 of 1000 took 0.707s\n",
      "training loss:\t\t0.027508\n",
      "Epoch 183 of 1000 took 0.704s\n",
      "training loss:\t\t0.027507\n",
      "Epoch 184 of 1000 took 0.704s\n",
      "training loss:\t\t0.027505\n",
      "Epoch 185 of 1000 took 0.703s\n",
      "training loss:\t\t0.027503\n",
      "Epoch 186 of 1000 took 0.704s\n",
      "training loss:\t\t0.027501\n",
      "Epoch 187 of 1000 took 0.704s\n",
      "training loss:\t\t0.027499\n",
      "Epoch 188 of 1000 took 0.706s\n",
      "training loss:\t\t0.027497\n",
      "Epoch 189 of 1000 took 0.703s\n",
      "training loss:\t\t0.027495\n",
      "Epoch 190 of 1000 took 0.703s\n",
      "training loss:\t\t0.027493\n",
      "Epoch 191 of 1000 took 0.706s\n",
      "training loss:\t\t0.027491\n",
      "Epoch 192 of 1000 took 0.705s\n",
      "training loss:\t\t0.027489\n",
      "Epoch 193 of 1000 took 0.705s\n",
      "training loss:\t\t0.027487\n",
      "Epoch 194 of 1000 took 0.704s\n",
      "training loss:\t\t0.027485\n",
      "Epoch 195 of 1000 took 0.701s\n",
      "training loss:\t\t0.027482\n",
      "Epoch 196 of 1000 took 0.705s\n",
      "training loss:\t\t0.027480\n",
      "Epoch 197 of 1000 took 0.705s\n",
      "training loss:\t\t0.027478\n",
      "Epoch 198 of 1000 took 0.704s\n",
      "training loss:\t\t0.027476\n",
      "Epoch 199 of 1000 took 0.722s\n",
      "training loss:\t\t0.027473\n",
      "Epoch 200 of 1000 took 0.755s\n",
      "training loss:\t\t0.027471\n",
      "Epoch 201 of 1000 took 0.757s\n",
      "training loss:\t\t0.027469\n",
      "Epoch 202 of 1000 took 0.710s\n",
      "training loss:\t\t0.027466\n",
      "Epoch 203 of 1000 took 0.731s\n",
      "training loss:\t\t0.027464\n",
      "Epoch 204 of 1000 took 0.732s\n",
      "training loss:\t\t0.027461\n",
      "Epoch 205 of 1000 took 0.757s\n",
      "training loss:\t\t0.027459\n",
      "Epoch 206 of 1000 took 0.767s\n",
      "training loss:\t\t0.027456\n",
      "Epoch 207 of 1000 took 0.701s\n",
      "training loss:\t\t0.027453\n",
      "Epoch 208 of 1000 took 0.739s\n",
      "training loss:\t\t0.027451\n",
      "Epoch 209 of 1000 took 0.702s\n",
      "training loss:\t\t0.027448\n",
      "Epoch 210 of 1000 took 0.747s\n",
      "training loss:\t\t0.027445\n",
      "Epoch 211 of 1000 took 0.710s\n",
      "training loss:\t\t0.027442\n",
      "Epoch 212 of 1000 took 0.741s\n",
      "training loss:\t\t0.027439\n",
      "Epoch 213 of 1000 took 0.727s\n",
      "training loss:\t\t0.027436\n",
      "Epoch 214 of 1000 took 0.715s\n",
      "training loss:\t\t0.027433\n",
      "Epoch 215 of 1000 took 0.701s\n",
      "training loss:\t\t0.027430\n",
      "Epoch 216 of 1000 took 0.710s\n",
      "training loss:\t\t0.027427\n",
      "Epoch 217 of 1000 took 0.741s\n",
      "training loss:\t\t0.027424\n",
      "Epoch 218 of 1000 took 0.703s\n",
      "training loss:\t\t0.027421\n",
      "Epoch 219 of 1000 took 0.702s\n",
      "training loss:\t\t0.027418\n",
      "Epoch 220 of 1000 took 0.710s\n",
      "training loss:\t\t0.027414\n",
      "Epoch 221 of 1000 took 0.722s\n",
      "training loss:\t\t0.027411\n",
      "Epoch 222 of 1000 took 0.702s\n",
      "training loss:\t\t0.027407\n",
      "Epoch 223 of 1000 took 0.702s\n",
      "training loss:\t\t0.027404\n",
      "Epoch 224 of 1000 took 0.702s\n",
      "training loss:\t\t0.027400\n",
      "Epoch 225 of 1000 took 0.702s\n",
      "training loss:\t\t0.027397\n",
      "Epoch 226 of 1000 took 0.702s\n",
      "training loss:\t\t0.027393\n",
      "Epoch 227 of 1000 took 0.704s\n",
      "training loss:\t\t0.027389\n",
      "Epoch 228 of 1000 took 0.699s\n",
      "training loss:\t\t0.027385\n",
      "Epoch 229 of 1000 took 0.699s\n",
      "training loss:\t\t0.027381\n",
      "Epoch 230 of 1000 took 0.700s\n",
      "training loss:\t\t0.027377\n",
      "Epoch 231 of 1000 took 0.702s\n",
      "training loss:\t\t0.027373\n",
      "Epoch 232 of 1000 took 0.701s\n",
      "training loss:\t\t0.027369\n",
      "Epoch 233 of 1000 took 0.700s\n",
      "training loss:\t\t0.027365\n",
      "Epoch 234 of 1000 took 0.699s\n",
      "training loss:\t\t0.027361\n",
      "Epoch 235 of 1000 took 0.708s\n",
      "training loss:\t\t0.027356\n",
      "Epoch 236 of 1000 took 0.735s\n",
      "training loss:\t\t0.027352\n",
      "Epoch 237 of 1000 took 0.702s\n",
      "training loss:\t\t0.027347\n",
      "Epoch 238 of 1000 took 0.703s\n",
      "training loss:\t\t0.027342\n",
      "Epoch 239 of 1000 took 0.702s\n",
      "training loss:\t\t0.027338\n",
      "Epoch 240 of 1000 took 0.703s\n",
      "training loss:\t\t0.027333\n",
      "Epoch 241 of 1000 took 0.703s\n",
      "training loss:\t\t0.027328\n",
      "Epoch 242 of 1000 took 0.702s\n",
      "training loss:\t\t0.027323\n",
      "Epoch 243 of 1000 took 0.701s\n",
      "training loss:\t\t0.027318\n",
      "Epoch 244 of 1000 took 0.738s\n",
      "training loss:\t\t0.027312\n",
      "Epoch 245 of 1000 took 0.699s\n",
      "training loss:\t\t0.027307\n",
      "Epoch 246 of 1000 took 0.702s\n",
      "training loss:\t\t0.027301\n",
      "Epoch 247 of 1000 took 0.699s\n",
      "training loss:\t\t0.027296\n",
      "Epoch 248 of 1000 took 0.702s\n",
      "training loss:\t\t0.027290\n",
      "Epoch 249 of 1000 took 0.703s\n",
      "training loss:\t\t0.027284\n",
      "Epoch 250 of 1000 took 0.736s\n",
      "training loss:\t\t0.027278\n",
      "Epoch 251 of 1000 took 0.743s\n",
      "training loss:\t\t0.027272\n",
      "Epoch 252 of 1000 took 0.743s\n",
      "training loss:\t\t0.027266\n",
      "Epoch 253 of 1000 took 0.745s\n",
      "training loss:\t\t0.027259\n",
      "Epoch 254 of 1000 took 0.738s\n",
      "training loss:\t\t0.027253\n",
      "Epoch 255 of 1000 took 0.746s\n",
      "training loss:\t\t0.027246\n",
      "Epoch 256 of 1000 took 0.703s\n",
      "training loss:\t\t0.027240\n",
      "Epoch 257 of 1000 took 0.702s\n",
      "training loss:\t\t0.027233\n",
      "Epoch 258 of 1000 took 0.702s\n",
      "training loss:\t\t0.027225\n",
      "Epoch 259 of 1000 took 0.702s\n",
      "training loss:\t\t0.027218\n",
      "Epoch 260 of 1000 took 0.734s\n",
      "training loss:\t\t0.027211\n",
      "Epoch 261 of 1000 took 0.764s\n",
      "training loss:\t\t0.027203\n",
      "Epoch 262 of 1000 took 0.727s\n",
      "training loss:\t\t0.027195\n",
      "Epoch 263 of 1000 took 0.735s\n",
      "training loss:\t\t0.027188\n",
      "Epoch 264 of 1000 took 0.703s\n",
      "training loss:\t\t0.027179\n",
      "Epoch 265 of 1000 took 0.731s\n",
      "training loss:\t\t0.027171\n",
      "Epoch 266 of 1000 took 0.704s\n",
      "training loss:\t\t0.027163\n",
      "Epoch 267 of 1000 took 0.704s\n",
      "training loss:\t\t0.027154\n",
      "Epoch 268 of 1000 took 0.737s\n",
      "training loss:\t\t0.027145\n",
      "Epoch 269 of 1000 took 0.731s\n",
      "training loss:\t\t0.027136\n",
      "Epoch 270 of 1000 took 0.703s\n",
      "training loss:\t\t0.027126\n",
      "Epoch 271 of 1000 took 0.709s\n",
      "training loss:\t\t0.027117\n",
      "Epoch 272 of 1000 took 0.768s\n",
      "training loss:\t\t0.027107\n",
      "Epoch 273 of 1000 took 0.701s\n",
      "training loss:\t\t0.027097\n",
      "Epoch 274 of 1000 took 0.702s\n",
      "training loss:\t\t0.027087\n",
      "Epoch 275 of 1000 took 0.702s\n",
      "training loss:\t\t0.027076\n",
      "Epoch 276 of 1000 took 0.737s\n",
      "training loss:\t\t0.027065\n",
      "Epoch 277 of 1000 took 0.732s\n",
      "training loss:\t\t0.027054\n",
      "Epoch 278 of 1000 took 0.703s\n",
      "training loss:\t\t0.027043\n",
      "Epoch 279 of 1000 took 0.704s\n",
      "training loss:\t\t0.027031\n",
      "Epoch 280 of 1000 took 0.699s\n",
      "training loss:\t\t0.027019\n",
      "Epoch 281 of 1000 took 0.702s\n",
      "training loss:\t\t0.027007\n",
      "Epoch 282 of 1000 took 0.700s\n",
      "training loss:\t\t0.026994\n",
      "Epoch 283 of 1000 took 0.703s\n",
      "training loss:\t\t0.026981\n",
      "Epoch 284 of 1000 took 0.702s\n",
      "training loss:\t\t0.026968\n",
      "Epoch 285 of 1000 took 0.701s\n",
      "training loss:\t\t0.026954\n",
      "Epoch 286 of 1000 took 0.704s\n",
      "training loss:\t\t0.026940\n",
      "Epoch 287 of 1000 took 0.704s\n",
      "training loss:\t\t0.026926\n",
      "Epoch 288 of 1000 took 0.732s\n",
      "training loss:\t\t0.026911\n",
      "Epoch 289 of 1000 took 0.753s\n",
      "training loss:\t\t0.026895\n",
      "Epoch 290 of 1000 took 0.700s\n",
      "training loss:\t\t0.026880\n",
      "Epoch 291 of 1000 took 0.748s\n",
      "training loss:\t\t0.026863\n",
      "Epoch 292 of 1000 took 0.734s\n",
      "training loss:\t\t0.026847\n",
      "Epoch 293 of 1000 took 0.726s\n",
      "training loss:\t\t0.026829\n",
      "Epoch 294 of 1000 took 0.712s\n",
      "training loss:\t\t0.026812\n",
      "Epoch 295 of 1000 took 0.708s\n",
      "training loss:\t\t0.026793\n",
      "Epoch 296 of 1000 took 0.747s\n",
      "training loss:\t\t0.026774\n",
      "Epoch 297 of 1000 took 0.703s\n",
      "training loss:\t\t0.026755\n",
      "Epoch 298 of 1000 took 0.722s\n",
      "training loss:\t\t0.026735\n",
      "Epoch 299 of 1000 took 0.704s\n",
      "training loss:\t\t0.026714\n",
      "Epoch 300 of 1000 took 0.703s\n",
      "training loss:\t\t0.026693\n",
      "Epoch 301 of 1000 took 0.707s\n",
      "training loss:\t\t0.026671\n",
      "Epoch 302 of 1000 took 0.747s\n",
      "training loss:\t\t0.026648\n",
      "Epoch 303 of 1000 took 0.722s\n",
      "training loss:\t\t0.026625\n",
      "Epoch 304 of 1000 took 0.705s\n",
      "training loss:\t\t0.026600\n",
      "Epoch 305 of 1000 took 0.711s\n",
      "training loss:\t\t0.026575\n",
      "Epoch 306 of 1000 took 0.722s\n",
      "training loss:\t\t0.026549\n",
      "Epoch 307 of 1000 took 0.735s\n",
      "training loss:\t\t0.026522\n",
      "Epoch 308 of 1000 took 0.763s\n",
      "training loss:\t\t0.026494\n",
      "Epoch 309 of 1000 took 0.746s\n",
      "training loss:\t\t0.026465\n",
      "Epoch 310 of 1000 took 0.717s\n",
      "training loss:\t\t0.026435\n",
      "Epoch 311 of 1000 took 0.714s\n",
      "training loss:\t\t0.026404\n",
      "Epoch 312 of 1000 took 0.758s\n",
      "training loss:\t\t0.026372\n",
      "Epoch 313 of 1000 took 0.707s\n",
      "training loss:\t\t0.026339\n",
      "Epoch 314 of 1000 took 0.704s\n",
      "training loss:\t\t0.026304\n",
      "Epoch 315 of 1000 took 0.729s\n",
      "training loss:\t\t0.026268\n",
      "Epoch 316 of 1000 took 0.704s\n",
      "training loss:\t\t0.026231\n",
      "Epoch 317 of 1000 took 0.706s\n",
      "training loss:\t\t0.026192\n",
      "Epoch 318 of 1000 took 0.705s\n",
      "training loss:\t\t0.026152\n",
      "Epoch 319 of 1000 took 0.703s\n",
      "training loss:\t\t0.026110\n",
      "Epoch 320 of 1000 took 0.704s\n",
      "training loss:\t\t0.026066\n",
      "Epoch 321 of 1000 took 0.716s\n",
      "training loss:\t\t0.026021\n",
      "Epoch 322 of 1000 took 0.708s\n",
      "training loss:\t\t0.025973\n",
      "Epoch 323 of 1000 took 0.704s\n",
      "training loss:\t\t0.025924\n",
      "Epoch 324 of 1000 took 0.705s\n",
      "training loss:\t\t0.025873\n",
      "Epoch 325 of 1000 took 0.705s\n",
      "training loss:\t\t0.025819\n",
      "Epoch 326 of 1000 took 0.710s\n",
      "training loss:\t\t0.025763\n",
      "Epoch 327 of 1000 took 0.707s\n",
      "training loss:\t\t0.025705\n",
      "Epoch 328 of 1000 took 0.763s\n",
      "training loss:\t\t0.025644\n",
      "Epoch 329 of 1000 took 0.723s\n",
      "training loss:\t\t0.025580\n",
      "Epoch 330 of 1000 took 0.728s\n",
      "training loss:\t\t0.025514\n",
      "Epoch 331 of 1000 took 0.707s\n",
      "training loss:\t\t0.025444\n",
      "Epoch 332 of 1000 took 0.703s\n",
      "training loss:\t\t0.025371\n",
      "Epoch 333 of 1000 took 0.706s\n",
      "training loss:\t\t0.025295\n",
      "Epoch 334 of 1000 took 0.710s\n",
      "training loss:\t\t0.025216\n",
      "Epoch 335 of 1000 took 0.709s\n",
      "training loss:\t\t0.025132\n",
      "Epoch 336 of 1000 took 0.706s\n",
      "training loss:\t\t0.025045\n",
      "Epoch 337 of 1000 took 0.703s\n",
      "training loss:\t\t0.024953\n",
      "Epoch 338 of 1000 took 0.726s\n",
      "training loss:\t\t0.024857\n",
      "Epoch 339 of 1000 took 0.724s\n",
      "training loss:\t\t0.024757\n",
      "Epoch 340 of 1000 took 0.704s\n",
      "training loss:\t\t0.024651\n",
      "Epoch 341 of 1000 took 0.723s\n",
      "training loss:\t\t0.024541\n",
      "Epoch 342 of 1000 took 0.707s\n",
      "training loss:\t\t0.024425\n",
      "Epoch 343 of 1000 took 0.704s\n",
      "training loss:\t\t0.024304\n",
      "Epoch 344 of 1000 took 0.704s\n",
      "training loss:\t\t0.024176\n",
      "Epoch 345 of 1000 took 0.704s\n",
      "training loss:\t\t0.024043\n",
      "Epoch 346 of 1000 took 0.701s\n",
      "training loss:\t\t0.023904\n",
      "Epoch 347 of 1000 took 0.701s\n",
      "training loss:\t\t0.023758\n",
      "Epoch 348 of 1000 took 0.705s\n",
      "training loss:\t\t0.023605\n",
      "Epoch 349 of 1000 took 0.707s\n",
      "training loss:\t\t0.023446\n",
      "Epoch 350 of 1000 took 0.703s\n",
      "training loss:\t\t0.023280\n",
      "Epoch 351 of 1000 took 0.704s\n",
      "training loss:\t\t0.023107\n",
      "Epoch 352 of 1000 took 0.704s\n",
      "training loss:\t\t0.022927\n",
      "Epoch 353 of 1000 took 0.703s\n",
      "training loss:\t\t0.022741\n",
      "Epoch 354 of 1000 took 0.701s\n",
      "training loss:\t\t0.022549\n",
      "Epoch 355 of 1000 took 0.702s\n",
      "training loss:\t\t0.022351\n",
      "Epoch 356 of 1000 took 0.702s\n",
      "training loss:\t\t0.022149\n",
      "Epoch 357 of 1000 took 0.702s\n",
      "training loss:\t\t0.021942\n",
      "Epoch 358 of 1000 took 0.705s\n",
      "training loss:\t\t0.021732\n",
      "Epoch 359 of 1000 took 0.704s\n",
      "training loss:\t\t0.021521\n",
      "Epoch 360 of 1000 took 0.703s\n",
      "training loss:\t\t0.021311\n",
      "Epoch 361 of 1000 took 0.704s\n",
      "training loss:\t\t0.021103\n",
      "Epoch 362 of 1000 took 0.704s\n",
      "training loss:\t\t0.020901\n",
      "Epoch 363 of 1000 took 0.704s\n",
      "training loss:\t\t0.020705\n",
      "Epoch 364 of 1000 took 0.703s\n",
      "training loss:\t\t0.020521\n",
      "Epoch 365 of 1000 took 0.701s\n",
      "training loss:\t\t0.020350\n",
      "Epoch 366 of 1000 took 0.702s\n",
      "training loss:\t\t0.020197\n",
      "Epoch 367 of 1000 took 0.702s\n",
      "training loss:\t\t0.020063\n",
      "Epoch 368 of 1000 took 0.706s\n",
      "training loss:\t\t0.019953\n",
      "Epoch 369 of 1000 took 0.704s\n",
      "training loss:\t\t0.019868\n",
      "Epoch 370 of 1000 took 0.705s\n",
      "training loss:\t\t0.019810\n",
      "Epoch 371 of 1000 took 0.704s\n",
      "training loss:\t\t0.019781\n",
      "Epoch 372 of 1000 took 0.709s\n",
      "training loss:\t\t0.019779\n",
      "Epoch 373 of 1000 took 0.705s\n",
      "training loss:\t\t0.019805\n",
      "Epoch 374 of 1000 took 0.703s\n",
      "training loss:\t\t0.019854\n",
      "Epoch 375 of 1000 took 0.704s\n",
      "training loss:\t\t0.019924\n",
      "Epoch 376 of 1000 took 0.704s\n",
      "training loss:\t\t0.020010\n",
      "Epoch 377 of 1000 took 0.703s\n",
      "training loss:\t\t0.020106\n",
      "Epoch 378 of 1000 took 0.704s\n",
      "training loss:\t\t0.020207\n",
      "Epoch 379 of 1000 took 0.704s\n",
      "training loss:\t\t0.020305\n",
      "Epoch 380 of 1000 took 0.703s\n",
      "training loss:\t\t0.020396\n",
      "Epoch 381 of 1000 took 0.705s\n",
      "training loss:\t\t0.020473\n",
      "Epoch 382 of 1000 took 0.703s\n",
      "training loss:\t\t0.020532\n",
      "Epoch 383 of 1000 took 0.705s\n",
      "training loss:\t\t0.020569\n",
      "Epoch 384 of 1000 took 0.704s\n",
      "training loss:\t\t0.020582\n",
      "Epoch 385 of 1000 took 0.705s\n",
      "training loss:\t\t0.020570\n",
      "Epoch 386 of 1000 took 0.705s\n",
      "training loss:\t\t0.020533\n",
      "Epoch 387 of 1000 took 0.706s\n",
      "training loss:\t\t0.020473\n",
      "Epoch 388 of 1000 took 0.700s\n",
      "training loss:\t\t0.020392\n",
      "Epoch 389 of 1000 took 0.703s\n",
      "training loss:\t\t0.020295\n",
      "Epoch 390 of 1000 took 0.703s\n",
      "training loss:\t\t0.020184\n",
      "Epoch 391 of 1000 took 0.706s\n",
      "training loss:\t\t0.020064\n",
      "Epoch 392 of 1000 took 0.704s\n",
      "training loss:\t\t0.019940\n",
      "Epoch 393 of 1000 took 0.704s\n",
      "training loss:\t\t0.019816\n",
      "Epoch 394 of 1000 took 0.704s\n",
      "training loss:\t\t0.019695\n",
      "Epoch 395 of 1000 took 0.706s\n",
      "training loss:\t\t0.019581\n",
      "Epoch 396 of 1000 took 0.706s\n",
      "training loss:\t\t0.019475\n",
      "Epoch 397 of 1000 took 0.703s\n",
      "training loss:\t\t0.019381\n",
      "Epoch 398 of 1000 took 0.702s\n",
      "training loss:\t\t0.019298\n",
      "Epoch 399 of 1000 took 0.708s\n",
      "training loss:\t\t0.019228\n",
      "Epoch 400 of 1000 took 0.704s\n",
      "training loss:\t\t0.019169\n",
      "Epoch 401 of 1000 took 0.704s\n",
      "training loss:\t\t0.019122\n",
      "Epoch 402 of 1000 took 0.704s\n",
      "training loss:\t\t0.019085\n",
      "Epoch 403 of 1000 took 0.704s\n",
      "training loss:\t\t0.019057\n",
      "Epoch 404 of 1000 took 0.703s\n",
      "training loss:\t\t0.019037\n",
      "Epoch 405 of 1000 took 0.702s\n",
      "training loss:\t\t0.019022\n",
      "Epoch 406 of 1000 took 0.702s\n",
      "training loss:\t\t0.019012\n",
      "Epoch 407 of 1000 took 0.701s\n",
      "training loss:\t\t0.019006\n",
      "Epoch 408 of 1000 took 0.704s\n",
      "training loss:\t\t0.019001\n",
      "Epoch 409 of 1000 took 0.706s\n",
      "training loss:\t\t0.018997\n",
      "Epoch 410 of 1000 took 0.705s\n",
      "training loss:\t\t0.018993\n",
      "Epoch 411 of 1000 took 0.702s\n",
      "training loss:\t\t0.018988\n",
      "Epoch 412 of 1000 took 0.703s\n",
      "training loss:\t\t0.018981\n",
      "Epoch 413 of 1000 took 0.702s\n",
      "training loss:\t\t0.018972\n",
      "Epoch 414 of 1000 took 0.703s\n",
      "training loss:\t\t0.018960\n",
      "Epoch 415 of 1000 took 0.702s\n",
      "training loss:\t\t0.018946\n",
      "Epoch 416 of 1000 took 0.703s\n",
      "training loss:\t\t0.018928\n",
      "Epoch 417 of 1000 took 0.701s\n",
      "training loss:\t\t0.018907\n",
      "Epoch 418 of 1000 took 0.703s\n",
      "training loss:\t\t0.018883\n",
      "Epoch 419 of 1000 took 0.706s\n",
      "training loss:\t\t0.018855\n",
      "Epoch 420 of 1000 took 0.704s\n",
      "training loss:\t\t0.018825\n",
      "Epoch 421 of 1000 took 0.703s\n",
      "training loss:\t\t0.018793\n",
      "Epoch 422 of 1000 took 0.703s\n",
      "training loss:\t\t0.018758\n",
      "Epoch 423 of 1000 took 0.704s\n",
      "training loss:\t\t0.018722\n",
      "Epoch 424 of 1000 took 0.705s\n",
      "training loss:\t\t0.018684\n",
      "Epoch 425 of 1000 took 0.705s\n",
      "training loss:\t\t0.018644\n",
      "Epoch 426 of 1000 took 0.704s\n",
      "training loss:\t\t0.018604\n",
      "Epoch 427 of 1000 took 0.705s\n",
      "training loss:\t\t0.018564\n",
      "Epoch 428 of 1000 took 0.704s\n",
      "training loss:\t\t0.018524\n",
      "Epoch 429 of 1000 took 0.704s\n",
      "training loss:\t\t0.018485\n",
      "Epoch 430 of 1000 took 0.704s\n",
      "training loss:\t\t0.018446\n",
      "Epoch 431 of 1000 took 0.704s\n",
      "training loss:\t\t0.018409\n",
      "Epoch 432 of 1000 took 0.704s\n",
      "training loss:\t\t0.018374\n",
      "Epoch 433 of 1000 took 0.702s\n",
      "training loss:\t\t0.018341\n",
      "Epoch 434 of 1000 took 0.704s\n",
      "training loss:\t\t0.018310\n",
      "Epoch 435 of 1000 took 0.705s\n",
      "training loss:\t\t0.018281\n",
      "Epoch 436 of 1000 took 0.704s\n",
      "training loss:\t\t0.018255\n",
      "Epoch 437 of 1000 took 0.705s\n",
      "training loss:\t\t0.018232\n",
      "Epoch 438 of 1000 took 0.705s\n",
      "training loss:\t\t0.018211\n",
      "Epoch 439 of 1000 took 0.706s\n",
      "training loss:\t\t0.018192\n",
      "Epoch 440 of 1000 took 0.701s\n",
      "training loss:\t\t0.018176\n",
      "Epoch 441 of 1000 took 0.703s\n",
      "training loss:\t\t0.018162\n",
      "Epoch 442 of 1000 took 0.705s\n",
      "training loss:\t\t0.018149\n",
      "Epoch 443 of 1000 took 0.705s\n",
      "training loss:\t\t0.018137\n",
      "Epoch 444 of 1000 took 0.703s\n",
      "training loss:\t\t0.018127\n",
      "Epoch 445 of 1000 took 0.703s\n",
      "training loss:\t\t0.018116\n",
      "Epoch 446 of 1000 took 0.703s\n",
      "training loss:\t\t0.018106\n",
      "Epoch 447 of 1000 took 0.705s\n",
      "training loss:\t\t0.018095\n",
      "Epoch 448 of 1000 took 0.703s\n",
      "training loss:\t\t0.018084\n",
      "Epoch 449 of 1000 took 0.704s\n",
      "training loss:\t\t0.018071\n",
      "Epoch 450 of 1000 took 0.701s\n",
      "training loss:\t\t0.018057\n",
      "Epoch 451 of 1000 took 0.703s\n",
      "training loss:\t\t0.018041\n",
      "Epoch 452 of 1000 took 0.705s\n",
      "training loss:\t\t0.018023\n",
      "Epoch 453 of 1000 took 0.705s\n",
      "training loss:\t\t0.018004\n",
      "Epoch 454 of 1000 took 0.703s\n",
      "training loss:\t\t0.017983\n",
      "Epoch 455 of 1000 took 0.705s\n",
      "training loss:\t\t0.017960\n",
      "Epoch 456 of 1000 took 0.705s\n",
      "training loss:\t\t0.017936\n",
      "Epoch 457 of 1000 took 0.703s\n",
      "training loss:\t\t0.017911\n",
      "Epoch 458 of 1000 took 0.701s\n",
      "training loss:\t\t0.017884\n",
      "Epoch 459 of 1000 took 0.700s\n",
      "training loss:\t\t0.017857\n",
      "Epoch 460 of 1000 took 0.701s\n",
      "training loss:\t\t0.017830\n",
      "Epoch 461 of 1000 took 0.704s\n",
      "training loss:\t\t0.017803\n",
      "Epoch 462 of 1000 took 0.704s\n",
      "training loss:\t\t0.017776\n",
      "Epoch 463 of 1000 took 0.701s\n",
      "training loss:\t\t0.017750\n",
      "Epoch 464 of 1000 took 0.702s\n",
      "training loss:\t\t0.017724\n",
      "Epoch 465 of 1000 took 0.704s\n",
      "training loss:\t\t0.017700\n",
      "Epoch 466 of 1000 took 0.706s\n",
      "training loss:\t\t0.017676\n",
      "Epoch 467 of 1000 took 0.705s\n",
      "training loss:\t\t0.017654\n",
      "Epoch 468 of 1000 took 0.704s\n",
      "training loss:\t\t0.017633\n",
      "Epoch 469 of 1000 took 0.702s\n",
      "training loss:\t\t0.017614\n",
      "Epoch 470 of 1000 took 0.703s\n",
      "training loss:\t\t0.017595\n",
      "Epoch 471 of 1000 took 0.705s\n",
      "training loss:\t\t0.017578\n",
      "Epoch 472 of 1000 took 0.705s\n",
      "training loss:\t\t0.017562\n",
      "Epoch 473 of 1000 took 0.703s\n",
      "training loss:\t\t0.017547\n",
      "Epoch 474 of 1000 took 0.704s\n",
      "training loss:\t\t0.017532\n",
      "Epoch 475 of 1000 took 0.703s\n",
      "training loss:\t\t0.017518\n",
      "Epoch 476 of 1000 took 0.704s\n",
      "training loss:\t\t0.017505\n",
      "Epoch 477 of 1000 took 0.707s\n",
      "training loss:\t\t0.017492\n",
      "Epoch 478 of 1000 took 0.703s\n",
      "training loss:\t\t0.017479\n",
      "Epoch 479 of 1000 took 0.705s\n",
      "training loss:\t\t0.017466\n",
      "Epoch 480 of 1000 took 0.704s\n",
      "training loss:\t\t0.017452\n",
      "Epoch 481 of 1000 took 0.704s\n",
      "training loss:\t\t0.017439\n",
      "Epoch 482 of 1000 took 0.707s\n",
      "training loss:\t\t0.017425\n",
      "Epoch 483 of 1000 took 0.703s\n",
      "training loss:\t\t0.017411\n",
      "Epoch 484 of 1000 took 0.702s\n",
      "training loss:\t\t0.017396\n",
      "Epoch 485 of 1000 took 0.701s\n",
      "training loss:\t\t0.017380\n",
      "Epoch 486 of 1000 took 0.704s\n",
      "training loss:\t\t0.017364\n",
      "Epoch 487 of 1000 took 0.722s\n",
      "training loss:\t\t0.017348\n",
      "Epoch 488 of 1000 took 0.744s\n",
      "training loss:\t\t0.017331\n",
      "Epoch 489 of 1000 took 0.735s\n",
      "training loss:\t\t0.017314\n",
      "Epoch 490 of 1000 took 0.750s\n",
      "training loss:\t\t0.017296\n",
      "Epoch 491 of 1000 took 0.719s\n",
      "training loss:\t\t0.017279\n",
      "Epoch 492 of 1000 took 0.707s\n",
      "training loss:\t\t0.017261\n",
      "Epoch 493 of 1000 took 0.749s\n",
      "training loss:\t\t0.017242\n",
      "Epoch 494 of 1000 took 0.704s\n",
      "training loss:\t\t0.017224\n",
      "Epoch 495 of 1000 took 0.738s\n",
      "training loss:\t\t0.017206\n",
      "Epoch 496 of 1000 took 0.706s\n",
      "training loss:\t\t0.017188\n",
      "Epoch 497 of 1000 took 0.705s\n",
      "training loss:\t\t0.017170\n",
      "Epoch 498 of 1000 took 0.704s\n",
      "training loss:\t\t0.017153\n",
      "Epoch 499 of 1000 took 0.704s\n",
      "training loss:\t\t0.017135\n",
      "Epoch 500 of 1000 took 0.705s\n",
      "training loss:\t\t0.017118\n",
      "Epoch 501 of 1000 took 0.704s\n",
      "training loss:\t\t0.017102\n",
      "Epoch 502 of 1000 took 0.729s\n",
      "training loss:\t\t0.017085\n",
      "Epoch 503 of 1000 took 0.738s\n",
      "training loss:\t\t0.017069\n",
      "Epoch 504 of 1000 took 0.763s\n",
      "training loss:\t\t0.017054\n",
      "Epoch 505 of 1000 took 0.744s\n",
      "training loss:\t\t0.017038\n",
      "Epoch 506 of 1000 took 0.704s\n",
      "training loss:\t\t0.017023\n",
      "Epoch 507 of 1000 took 0.704s\n",
      "training loss:\t\t0.017007\n",
      "Epoch 508 of 1000 took 0.704s\n",
      "training loss:\t\t0.016992\n",
      "Epoch 509 of 1000 took 0.704s\n",
      "training loss:\t\t0.016977\n",
      "Epoch 510 of 1000 took 0.707s\n",
      "training loss:\t\t0.016962\n",
      "Epoch 511 of 1000 took 0.702s\n",
      "training loss:\t\t0.016947\n",
      "Epoch 512 of 1000 took 0.737s\n",
      "training loss:\t\t0.016931\n",
      "Epoch 513 of 1000 took 0.751s\n",
      "training loss:\t\t0.016916\n",
      "Epoch 514 of 1000 took 0.744s\n",
      "training loss:\t\t0.016900\n",
      "Epoch 515 of 1000 took 0.762s\n",
      "training loss:\t\t0.016884\n",
      "Epoch 516 of 1000 took 0.729s\n",
      "training loss:\t\t0.016868\n",
      "Epoch 517 of 1000 took 0.761s\n",
      "training loss:\t\t0.016852\n",
      "Epoch 518 of 1000 took 0.711s\n",
      "training loss:\t\t0.016835\n",
      "Epoch 519 of 1000 took 0.719s\n",
      "training loss:\t\t0.016819\n",
      "Epoch 520 of 1000 took 0.733s\n",
      "training loss:\t\t0.016802\n",
      "Epoch 521 of 1000 took 0.747s\n",
      "training loss:\t\t0.016786\n",
      "Epoch 522 of 1000 took 0.744s\n",
      "training loss:\t\t0.016769\n",
      "Epoch 523 of 1000 took 0.723s\n",
      "training loss:\t\t0.016752\n",
      "Epoch 524 of 1000 took 0.702s\n",
      "training loss:\t\t0.016736\n",
      "Epoch 525 of 1000 took 0.748s\n",
      "training loss:\t\t0.016719\n",
      "Epoch 526 of 1000 took 0.734s\n",
      "training loss:\t\t0.016703\n",
      "Epoch 527 of 1000 took 0.702s\n",
      "training loss:\t\t0.016687\n",
      "Epoch 528 of 1000 took 0.738s\n",
      "training loss:\t\t0.016671\n",
      "Epoch 529 of 1000 took 0.748s\n",
      "training loss:\t\t0.016655\n",
      "Epoch 530 of 1000 took 0.740s\n",
      "training loss:\t\t0.016640\n",
      "Epoch 531 of 1000 took 0.705s\n",
      "training loss:\t\t0.016624\n",
      "Epoch 532 of 1000 took 0.706s\n",
      "training loss:\t\t0.016609\n",
      "Epoch 533 of 1000 took 0.742s\n",
      "training loss:\t\t0.016594\n",
      "Epoch 534 of 1000 took 0.702s\n",
      "training loss:\t\t0.016579\n",
      "Epoch 535 of 1000 took 0.703s\n",
      "training loss:\t\t0.016564\n",
      "Epoch 536 of 1000 took 0.702s\n",
      "training loss:\t\t0.016549\n",
      "Epoch 537 of 1000 took 0.728s\n",
      "training loss:\t\t0.016535\n",
      "Epoch 538 of 1000 took 0.731s\n",
      "training loss:\t\t0.016520\n",
      "Epoch 539 of 1000 took 0.717s\n",
      "training loss:\t\t0.016506\n",
      "Epoch 540 of 1000 took 0.707s\n",
      "training loss:\t\t0.016491\n",
      "Epoch 541 of 1000 took 0.704s\n",
      "training loss:\t\t0.016477\n",
      "Epoch 542 of 1000 took 0.707s\n",
      "training loss:\t\t0.016462\n",
      "Epoch 543 of 1000 took 0.704s\n",
      "training loss:\t\t0.016448\n",
      "Epoch 544 of 1000 took 0.701s\n",
      "training loss:\t\t0.016433\n",
      "Epoch 545 of 1000 took 0.700s\n",
      "training loss:\t\t0.016419\n",
      "Epoch 546 of 1000 took 0.700s\n",
      "training loss:\t\t0.016404\n",
      "Epoch 547 of 1000 took 0.705s\n",
      "training loss:\t\t0.016389\n",
      "Epoch 548 of 1000 took 0.704s\n",
      "training loss:\t\t0.016375\n",
      "Epoch 549 of 1000 took 0.703s\n",
      "training loss:\t\t0.016360\n",
      "Epoch 550 of 1000 took 0.704s\n",
      "training loss:\t\t0.016345\n",
      "Epoch 551 of 1000 took 0.708s\n",
      "training loss:\t\t0.016331\n",
      "Epoch 552 of 1000 took 0.740s\n",
      "training loss:\t\t0.016316\n",
      "Epoch 553 of 1000 took 0.743s\n",
      "training loss:\t\t0.016301\n",
      "Epoch 554 of 1000 took 0.702s\n",
      "training loss:\t\t0.016287\n",
      "Epoch 555 of 1000 took 0.736s\n",
      "training loss:\t\t0.016272\n",
      "Epoch 556 of 1000 took 0.707s\n",
      "training loss:\t\t0.016258\n",
      "Epoch 557 of 1000 took 0.737s\n",
      "training loss:\t\t0.016244\n",
      "Epoch 558 of 1000 took 0.720s\n",
      "training loss:\t\t0.016230\n",
      "Epoch 559 of 1000 took 0.704s\n",
      "training loss:\t\t0.016215\n",
      "Epoch 560 of 1000 took 0.702s\n",
      "training loss:\t\t0.016201\n",
      "Epoch 561 of 1000 took 0.706s\n",
      "training loss:\t\t0.016188\n",
      "Epoch 562 of 1000 took 0.706s\n",
      "training loss:\t\t0.016174\n",
      "Epoch 563 of 1000 took 0.707s\n",
      "training loss:\t\t0.016160\n",
      "Epoch 564 of 1000 took 0.706s\n",
      "training loss:\t\t0.016146\n",
      "Epoch 565 of 1000 took 0.707s\n",
      "training loss:\t\t0.016133\n",
      "Epoch 566 of 1000 took 0.708s\n",
      "training loss:\t\t0.016119\n",
      "Epoch 567 of 1000 took 0.719s\n",
      "training loss:\t\t0.016105\n",
      "Epoch 568 of 1000 took 0.718s\n",
      "training loss:\t\t0.016092\n",
      "Epoch 569 of 1000 took 0.711s\n",
      "training loss:\t\t0.016079\n",
      "Epoch 570 of 1000 took 0.706s\n",
      "training loss:\t\t0.016065\n",
      "Epoch 571 of 1000 took 0.706s\n",
      "training loss:\t\t0.016052\n",
      "Epoch 572 of 1000 took 0.705s\n",
      "training loss:\t\t0.016038\n",
      "Epoch 573 of 1000 took 0.709s\n",
      "training loss:\t\t0.016025\n",
      "Epoch 574 of 1000 took 0.747s\n",
      "training loss:\t\t0.016012\n",
      "Epoch 575 of 1000 took 0.707s\n",
      "training loss:\t\t0.015998\n",
      "Epoch 576 of 1000 took 0.710s\n",
      "training loss:\t\t0.015985\n",
      "Epoch 577 of 1000 took 0.701s\n",
      "training loss:\t\t0.015972\n",
      "Epoch 578 of 1000 took 0.703s\n",
      "training loss:\t\t0.015959\n",
      "Epoch 579 of 1000 took 0.704s\n",
      "training loss:\t\t0.015946\n",
      "Epoch 580 of 1000 took 0.705s\n",
      "training loss:\t\t0.015933\n",
      "Epoch 581 of 1000 took 0.705s\n",
      "training loss:\t\t0.015920\n",
      "Epoch 582 of 1000 took 0.708s\n",
      "training loss:\t\t0.015907\n",
      "Epoch 583 of 1000 took 0.706s\n",
      "training loss:\t\t0.015894\n",
      "Epoch 584 of 1000 took 0.706s\n",
      "training loss:\t\t0.015881\n",
      "Epoch 585 of 1000 took 0.708s\n",
      "training loss:\t\t0.015869\n",
      "Epoch 586 of 1000 took 0.719s\n",
      "training loss:\t\t0.015856\n",
      "Epoch 587 of 1000 took 0.722s\n",
      "training loss:\t\t0.015844\n",
      "Epoch 588 of 1000 took 0.706s\n",
      "training loss:\t\t0.015831\n",
      "Epoch 589 of 1000 took 0.708s\n",
      "training loss:\t\t0.015819\n",
      "Epoch 590 of 1000 took 0.706s\n",
      "training loss:\t\t0.015807\n",
      "Epoch 591 of 1000 took 0.710s\n",
      "training loss:\t\t0.015795\n",
      "Epoch 592 of 1000 took 0.731s\n",
      "training loss:\t\t0.015783\n",
      "Epoch 593 of 1000 took 0.748s\n",
      "training loss:\t\t0.015771\n",
      "Epoch 594 of 1000 took 0.746s\n",
      "training loss:\t\t0.015759\n",
      "Epoch 595 of 1000 took 0.754s\n",
      "training loss:\t\t0.015747\n",
      "Epoch 596 of 1000 took 0.710s\n",
      "training loss:\t\t0.015735\n",
      "Epoch 597 of 1000 took 0.707s\n",
      "training loss:\t\t0.015723\n",
      "Epoch 598 of 1000 took 0.706s\n",
      "training loss:\t\t0.015711\n",
      "Epoch 599 of 1000 took 0.707s\n",
      "training loss:\t\t0.015700\n",
      "Epoch 600 of 1000 took 0.709s\n",
      "training loss:\t\t0.015688\n",
      "Epoch 601 of 1000 took 0.705s\n",
      "training loss:\t\t0.015676\n",
      "Epoch 602 of 1000 took 0.741s\n",
      "training loss:\t\t0.015665\n",
      "Epoch 603 of 1000 took 0.720s\n",
      "training loss:\t\t0.015653\n",
      "Epoch 604 of 1000 took 0.708s\n",
      "training loss:\t\t0.015642\n",
      "Epoch 605 of 1000 took 0.706s\n",
      "training loss:\t\t0.015630\n",
      "Epoch 606 of 1000 took 0.705s\n",
      "training loss:\t\t0.015619\n",
      "Epoch 607 of 1000 took 0.704s\n",
      "training loss:\t\t0.015608\n",
      "Epoch 608 of 1000 took 0.705s\n",
      "training loss:\t\t0.015596\n",
      "Epoch 609 of 1000 took 0.706s\n",
      "training loss:\t\t0.015585\n",
      "Epoch 610 of 1000 took 0.709s\n",
      "training loss:\t\t0.015574\n",
      "Epoch 611 of 1000 took 0.771s\n",
      "training loss:\t\t0.015563\n",
      "Epoch 612 of 1000 took 0.731s\n",
      "training loss:\t\t0.015552\n",
      "Epoch 613 of 1000 took 0.706s\n",
      "training loss:\t\t0.015541\n",
      "Epoch 614 of 1000 took 0.703s\n",
      "training loss:\t\t0.015531\n",
      "Epoch 615 of 1000 took 0.702s\n",
      "training loss:\t\t0.015520\n",
      "Epoch 616 of 1000 took 0.705s\n",
      "training loss:\t\t0.015509\n",
      "Epoch 617 of 1000 took 0.701s\n",
      "training loss:\t\t0.015498\n",
      "Epoch 618 of 1000 took 0.705s\n",
      "training loss:\t\t0.015488\n",
      "Epoch 619 of 1000 took 0.701s\n",
      "training loss:\t\t0.015477\n",
      "Epoch 620 of 1000 took 0.709s\n",
      "training loss:\t\t0.015467\n",
      "Epoch 621 of 1000 took 0.704s\n",
      "training loss:\t\t0.015456\n",
      "Epoch 622 of 1000 took 0.705s\n",
      "training loss:\t\t0.015446\n",
      "Epoch 623 of 1000 took 0.707s\n",
      "training loss:\t\t0.015435\n",
      "Epoch 624 of 1000 took 0.702s\n",
      "training loss:\t\t0.015425\n",
      "Epoch 625 of 1000 took 0.707s\n",
      "training loss:\t\t0.015415\n",
      "Epoch 626 of 1000 took 0.707s\n",
      "training loss:\t\t0.015404\n",
      "Epoch 627 of 1000 took 0.740s\n",
      "training loss:\t\t0.015394\n",
      "Epoch 628 of 1000 took 0.767s\n",
      "training loss:\t\t0.015384\n",
      "Epoch 629 of 1000 took 0.730s\n",
      "training loss:\t\t0.015374\n",
      "Epoch 630 of 1000 took 0.748s\n",
      "training loss:\t\t0.015364\n",
      "Epoch 631 of 1000 took 0.748s\n",
      "training loss:\t\t0.015354\n",
      "Epoch 632 of 1000 took 0.749s\n",
      "training loss:\t\t0.015343\n",
      "Epoch 633 of 1000 took 0.752s\n",
      "training loss:\t\t0.015333\n",
      "Epoch 634 of 1000 took 0.727s\n",
      "training loss:\t\t0.015324\n",
      "Epoch 635 of 1000 took 0.705s\n",
      "training loss:\t\t0.015314\n",
      "Epoch 636 of 1000 took 0.707s\n",
      "training loss:\t\t0.015304\n",
      "Epoch 637 of 1000 took 0.706s\n",
      "training loss:\t\t0.015294\n",
      "Epoch 638 of 1000 took 0.707s\n",
      "training loss:\t\t0.015284\n",
      "Epoch 639 of 1000 took 0.701s\n",
      "training loss:\t\t0.015274\n",
      "Epoch 640 of 1000 took 0.703s\n",
      "training loss:\t\t0.015265\n",
      "Epoch 641 of 1000 took 0.705s\n",
      "training loss:\t\t0.015255\n",
      "Epoch 642 of 1000 took 0.703s\n",
      "training loss:\t\t0.015245\n",
      "Epoch 643 of 1000 took 0.704s\n",
      "training loss:\t\t0.015236\n",
      "Epoch 644 of 1000 took 0.704s\n",
      "training loss:\t\t0.015226\n",
      "Epoch 645 of 1000 took 0.704s\n",
      "training loss:\t\t0.015217\n",
      "Epoch 646 of 1000 took 0.704s\n",
      "training loss:\t\t0.015207\n",
      "Epoch 647 of 1000 took 0.702s\n",
      "training loss:\t\t0.015198\n",
      "Epoch 648 of 1000 took 0.701s\n",
      "training loss:\t\t0.015188\n",
      "Epoch 649 of 1000 took 0.702s\n",
      "training loss:\t\t0.015179\n",
      "Epoch 650 of 1000 took 0.702s\n",
      "training loss:\t\t0.015170\n",
      "Epoch 651 of 1000 took 0.703s\n",
      "training loss:\t\t0.015160\n",
      "Epoch 652 of 1000 took 0.703s\n",
      "training loss:\t\t0.015151\n",
      "Epoch 653 of 1000 took 0.703s\n",
      "training loss:\t\t0.015142\n",
      "Epoch 654 of 1000 took 0.704s\n",
      "training loss:\t\t0.015133\n",
      "Epoch 655 of 1000 took 0.706s\n",
      "training loss:\t\t0.015123\n",
      "Epoch 656 of 1000 took 0.705s\n",
      "training loss:\t\t0.015114\n",
      "Epoch 657 of 1000 took 0.702s\n",
      "training loss:\t\t0.015105\n",
      "Epoch 658 of 1000 took 0.701s\n",
      "training loss:\t\t0.015096\n",
      "Epoch 659 of 1000 took 0.701s\n",
      "training loss:\t\t0.015087\n",
      "Epoch 660 of 1000 took 0.701s\n",
      "training loss:\t\t0.015078\n",
      "Epoch 661 of 1000 took 0.702s\n",
      "training loss:\t\t0.015069\n",
      "Epoch 662 of 1000 took 0.705s\n",
      "training loss:\t\t0.015060\n",
      "Epoch 663 of 1000 took 0.703s\n",
      "training loss:\t\t0.015052\n",
      "Epoch 664 of 1000 took 0.704s\n",
      "training loss:\t\t0.015043\n",
      "Epoch 665 of 1000 took 0.706s\n",
      "training loss:\t\t0.015034\n",
      "Epoch 666 of 1000 took 0.704s\n",
      "training loss:\t\t0.015025\n",
      "Epoch 667 of 1000 took 0.705s\n",
      "training loss:\t\t0.015017\n",
      "Epoch 668 of 1000 took 0.700s\n",
      "training loss:\t\t0.015008\n",
      "Epoch 669 of 1000 took 0.703s\n",
      "training loss:\t\t0.014999\n",
      "Epoch 670 of 1000 took 0.702s\n",
      "training loss:\t\t0.014991\n",
      "Epoch 671 of 1000 took 0.703s\n",
      "training loss:\t\t0.014982\n",
      "Epoch 672 of 1000 took 0.702s\n",
      "training loss:\t\t0.014974\n",
      "Epoch 673 of 1000 took 0.705s\n",
      "training loss:\t\t0.014965\n",
      "Epoch 674 of 1000 took 0.704s\n",
      "training loss:\t\t0.014957\n",
      "Epoch 675 of 1000 took 0.704s\n",
      "training loss:\t\t0.014949\n",
      "Epoch 676 of 1000 took 0.704s\n",
      "training loss:\t\t0.014940\n",
      "Epoch 677 of 1000 took 0.704s\n",
      "training loss:\t\t0.014932\n",
      "Epoch 678 of 1000 took 0.703s\n",
      "training loss:\t\t0.014924\n",
      "Epoch 679 of 1000 took 0.702s\n",
      "training loss:\t\t0.014916\n",
      "Epoch 680 of 1000 took 0.705s\n",
      "training loss:\t\t0.014908\n",
      "Epoch 681 of 1000 took 0.706s\n",
      "training loss:\t\t0.014899\n",
      "Epoch 682 of 1000 took 0.706s\n",
      "training loss:\t\t0.014891\n",
      "Epoch 683 of 1000 took 0.705s\n",
      "training loss:\t\t0.014883\n",
      "Epoch 684 of 1000 took 0.703s\n",
      "training loss:\t\t0.014875\n",
      "Epoch 685 of 1000 took 0.704s\n",
      "training loss:\t\t0.014867\n",
      "Epoch 686 of 1000 took 0.702s\n",
      "training loss:\t\t0.014860\n",
      "Epoch 687 of 1000 took 0.701s\n",
      "training loss:\t\t0.014852\n",
      "Epoch 688 of 1000 took 0.701s\n",
      "training loss:\t\t0.014844\n",
      "Epoch 689 of 1000 took 0.703s\n",
      "training loss:\t\t0.014836\n",
      "Epoch 690 of 1000 took 0.707s\n",
      "training loss:\t\t0.014828\n",
      "Epoch 691 of 1000 took 0.726s\n",
      "training loss:\t\t0.014821\n",
      "Epoch 692 of 1000 took 0.716s\n",
      "training loss:\t\t0.014813\n",
      "Epoch 693 of 1000 took 0.706s\n",
      "training loss:\t\t0.014806\n",
      "Epoch 694 of 1000 took 0.703s\n",
      "training loss:\t\t0.014798\n",
      "Epoch 695 of 1000 took 0.716s\n",
      "training loss:\t\t0.014791\n",
      "Epoch 696 of 1000 took 0.710s\n",
      "training loss:\t\t0.014783\n",
      "Epoch 697 of 1000 took 0.716s\n",
      "training loss:\t\t0.014776\n",
      "Epoch 698 of 1000 took 0.711s\n",
      "training loss:\t\t0.014768\n",
      "Epoch 699 of 1000 took 0.713s\n",
      "training loss:\t\t0.014761\n",
      "Epoch 700 of 1000 took 0.704s\n",
      "training loss:\t\t0.014754\n",
      "Epoch 701 of 1000 took 0.709s\n",
      "training loss:\t\t0.014746\n",
      "Epoch 702 of 1000 took 0.703s\n",
      "training loss:\t\t0.014739\n",
      "Epoch 703 of 1000 took 0.709s\n",
      "training loss:\t\t0.014732\n",
      "Epoch 704 of 1000 took 0.706s\n",
      "training loss:\t\t0.014725\n",
      "Epoch 705 of 1000 took 0.704s\n",
      "training loss:\t\t0.014718\n",
      "Epoch 706 of 1000 took 0.700s\n",
      "training loss:\t\t0.014711\n",
      "Epoch 707 of 1000 took 0.704s\n",
      "training loss:\t\t0.014704\n",
      "Epoch 708 of 1000 took 0.704s\n",
      "training loss:\t\t0.014697\n",
      "Epoch 709 of 1000 took 0.709s\n",
      "training loss:\t\t0.014690\n",
      "Epoch 710 of 1000 took 0.704s\n",
      "training loss:\t\t0.014683\n",
      "Epoch 711 of 1000 took 0.706s\n",
      "training loss:\t\t0.014676\n",
      "Epoch 712 of 1000 took 0.715s\n",
      "training loss:\t\t0.014669\n",
      "Epoch 713 of 1000 took 0.719s\n",
      "training loss:\t\t0.014663\n",
      "Epoch 714 of 1000 took 0.704s\n",
      "training loss:\t\t0.014656\n",
      "Epoch 715 of 1000 took 0.711s\n",
      "training loss:\t\t0.014649\n",
      "Epoch 716 of 1000 took 0.705s\n",
      "training loss:\t\t0.014642\n",
      "Epoch 717 of 1000 took 0.707s\n",
      "training loss:\t\t0.014636\n",
      "Epoch 718 of 1000 took 0.712s\n",
      "training loss:\t\t0.014629\n",
      "Epoch 719 of 1000 took 0.704s\n",
      "training loss:\t\t0.014623\n",
      "Epoch 720 of 1000 took 0.705s\n",
      "training loss:\t\t0.014616\n",
      "Epoch 721 of 1000 took 0.707s\n",
      "training loss:\t\t0.014610\n",
      "Epoch 722 of 1000 took 0.708s\n",
      "training loss:\t\t0.014603\n",
      "Epoch 723 of 1000 took 0.706s\n",
      "training loss:\t\t0.014597\n",
      "Epoch 724 of 1000 took 0.702s\n",
      "training loss:\t\t0.014590\n",
      "Epoch 725 of 1000 took 0.701s\n",
      "training loss:\t\t0.014584\n",
      "Epoch 726 of 1000 took 0.701s\n",
      "training loss:\t\t0.014578\n",
      "Epoch 727 of 1000 took 0.711s\n",
      "training loss:\t\t0.014571\n",
      "Epoch 728 of 1000 took 0.707s\n",
      "training loss:\t\t0.014565\n",
      "Epoch 729 of 1000 took 0.714s\n",
      "training loss:\t\t0.014559\n",
      "Epoch 730 of 1000 took 0.703s\n",
      "training loss:\t\t0.014553\n",
      "Epoch 731 of 1000 took 0.701s\n",
      "training loss:\t\t0.014546\n",
      "Epoch 732 of 1000 took 0.710s\n",
      "training loss:\t\t0.014540\n",
      "Epoch 733 of 1000 took 0.704s\n",
      "training loss:\t\t0.014534\n",
      "Epoch 734 of 1000 took 0.737s\n",
      "training loss:\t\t0.014528\n",
      "Epoch 735 of 1000 took 0.743s\n",
      "training loss:\t\t0.014522\n",
      "Epoch 736 of 1000 took 0.710s\n",
      "training loss:\t\t0.014516\n",
      "Epoch 737 of 1000 took 0.735s\n",
      "training loss:\t\t0.014510\n",
      "Epoch 738 of 1000 took 0.702s\n",
      "training loss:\t\t0.014504\n",
      "Epoch 739 of 1000 took 0.733s\n",
      "training loss:\t\t0.014498\n",
      "Epoch 740 of 1000 took 0.728s\n",
      "training loss:\t\t0.014492\n",
      "Epoch 741 of 1000 took 0.712s\n",
      "training loss:\t\t0.014486\n",
      "Epoch 742 of 1000 took 0.752s\n",
      "training loss:\t\t0.014480\n",
      "Epoch 743 of 1000 took 0.750s\n",
      "training loss:\t\t0.014474\n",
      "Epoch 744 of 1000 took 0.700s\n",
      "training loss:\t\t0.014469\n",
      "Epoch 745 of 1000 took 0.742s\n",
      "training loss:\t\t0.014463\n",
      "Epoch 746 of 1000 took 0.702s\n",
      "training loss:\t\t0.014457\n",
      "Epoch 747 of 1000 took 0.732s\n",
      "training loss:\t\t0.014451\n",
      "Epoch 748 of 1000 took 0.731s\n",
      "training loss:\t\t0.014445\n",
      "Epoch 749 of 1000 took 0.738s\n",
      "training loss:\t\t0.014440\n",
      "Epoch 750 of 1000 took 0.699s\n",
      "training loss:\t\t0.014434\n",
      "Epoch 751 of 1000 took 0.701s\n",
      "training loss:\t\t0.014428\n",
      "Epoch 752 of 1000 took 0.700s\n",
      "training loss:\t\t0.014422\n",
      "Epoch 753 of 1000 took 0.701s\n",
      "training loss:\t\t0.014417\n",
      "Epoch 754 of 1000 took 0.724s\n",
      "training loss:\t\t0.014411\n",
      "Epoch 755 of 1000 took 0.742s\n",
      "training loss:\t\t0.014405\n",
      "Epoch 756 of 1000 took 0.701s\n",
      "training loss:\t\t0.014400\n",
      "Epoch 757 of 1000 took 0.701s\n",
      "training loss:\t\t0.014394\n",
      "Epoch 758 of 1000 took 0.702s\n",
      "training loss:\t\t0.014389\n",
      "Epoch 759 of 1000 took 0.701s\n",
      "training loss:\t\t0.014383\n",
      "Epoch 760 of 1000 took 0.701s\n",
      "training loss:\t\t0.014378\n",
      "Epoch 761 of 1000 took 0.703s\n",
      "training loss:\t\t0.014372\n",
      "Epoch 762 of 1000 took 0.709s\n",
      "training loss:\t\t0.014367\n",
      "Epoch 763 of 1000 took 0.702s\n",
      "training loss:\t\t0.014361\n",
      "Epoch 764 of 1000 took 0.703s\n",
      "training loss:\t\t0.014356\n",
      "Epoch 765 of 1000 took 0.703s\n",
      "training loss:\t\t0.014350\n",
      "Epoch 766 of 1000 took 0.702s\n",
      "training loss:\t\t0.014345\n",
      "Epoch 767 of 1000 took 0.700s\n",
      "training loss:\t\t0.014339\n",
      "Epoch 768 of 1000 took 0.702s\n",
      "training loss:\t\t0.014334\n",
      "Epoch 769 of 1000 took 0.701s\n",
      "training loss:\t\t0.014328\n",
      "Epoch 770 of 1000 took 0.700s\n",
      "training loss:\t\t0.014323\n",
      "Epoch 771 of 1000 took 0.698s\n",
      "training loss:\t\t0.014318\n",
      "Epoch 772 of 1000 took 0.699s\n",
      "training loss:\t\t0.014312\n",
      "Epoch 773 of 1000 took 0.700s\n",
      "training loss:\t\t0.014307\n",
      "Epoch 774 of 1000 took 0.700s\n",
      "training loss:\t\t0.014301\n",
      "Epoch 775 of 1000 took 0.700s\n",
      "training loss:\t\t0.014296\n",
      "Epoch 776 of 1000 took 0.701s\n",
      "training loss:\t\t0.014291\n",
      "Epoch 777 of 1000 took 0.729s\n",
      "training loss:\t\t0.014286\n",
      "Epoch 778 of 1000 took 0.771s\n",
      "training loss:\t\t0.014280\n",
      "Epoch 779 of 1000 took 0.747s\n",
      "training loss:\t\t0.014275\n",
      "Epoch 780 of 1000 took 0.726s\n",
      "training loss:\t\t0.014270\n",
      "Epoch 781 of 1000 took 0.729s\n",
      "training loss:\t\t0.014264\n",
      "Epoch 782 of 1000 took 0.714s\n",
      "training loss:\t\t0.014259\n",
      "Epoch 783 of 1000 took 0.702s\n",
      "training loss:\t\t0.014254\n",
      "Epoch 784 of 1000 took 0.704s\n",
      "training loss:\t\t0.014249\n",
      "Epoch 785 of 1000 took 0.707s\n",
      "training loss:\t\t0.014243\n",
      "Epoch 786 of 1000 took 0.703s\n",
      "training loss:\t\t0.014238\n",
      "Epoch 787 of 1000 took 0.701s\n",
      "training loss:\t\t0.014233\n",
      "Epoch 788 of 1000 took 0.699s\n",
      "training loss:\t\t0.014228\n",
      "Epoch 789 of 1000 took 0.713s\n",
      "training loss:\t\t0.014223\n",
      "Epoch 790 of 1000 took 0.742s\n",
      "training loss:\t\t0.014218\n",
      "Epoch 791 of 1000 took 0.780s\n",
      "training loss:\t\t0.014212\n",
      "Epoch 792 of 1000 took 0.700s\n",
      "training loss:\t\t0.014207\n",
      "Epoch 793 of 1000 took 0.766s\n",
      "training loss:\t\t0.014202\n",
      "Epoch 794 of 1000 took 0.700s\n",
      "training loss:\t\t0.014197\n",
      "Epoch 795 of 1000 took 0.715s\n",
      "training loss:\t\t0.014192\n",
      "Epoch 796 of 1000 took 0.764s\n",
      "training loss:\t\t0.014187\n",
      "Epoch 797 of 1000 took 0.701s\n",
      "training loss:\t\t0.014182\n",
      "Epoch 798 of 1000 took 0.745s\n",
      "training loss:\t\t0.014177\n",
      "Epoch 799 of 1000 took 0.703s\n",
      "training loss:\t\t0.014172\n",
      "Epoch 800 of 1000 took 0.772s\n",
      "training loss:\t\t0.014167\n",
      "Epoch 801 of 1000 took 0.699s\n",
      "training loss:\t\t0.014162\n",
      "Epoch 802 of 1000 took 0.699s\n",
      "training loss:\t\t0.014157\n",
      "Epoch 803 of 1000 took 0.700s\n",
      "training loss:\t\t0.014152\n",
      "Epoch 804 of 1000 took 0.731s\n",
      "training loss:\t\t0.014147\n",
      "Epoch 805 of 1000 took 0.700s\n",
      "training loss:\t\t0.014142\n",
      "Epoch 806 of 1000 took 0.700s\n",
      "training loss:\t\t0.014137\n",
      "Epoch 807 of 1000 took 0.702s\n",
      "training loss:\t\t0.014132\n",
      "Epoch 808 of 1000 took 0.700s\n",
      "training loss:\t\t0.014127\n",
      "Epoch 809 of 1000 took 0.703s\n",
      "training loss:\t\t0.014122\n",
      "Epoch 810 of 1000 took 0.745s\n",
      "training loss:\t\t0.014117\n",
      "Epoch 811 of 1000 took 0.704s\n",
      "training loss:\t\t0.014112\n",
      "Epoch 812 of 1000 took 0.732s\n",
      "training loss:\t\t0.014107\n",
      "Epoch 813 of 1000 took 0.789s\n",
      "training loss:\t\t0.014102\n",
      "Epoch 814 of 1000 took 0.757s\n",
      "training loss:\t\t0.014097\n",
      "Epoch 815 of 1000 took 0.702s\n",
      "training loss:\t\t0.014093\n",
      "Epoch 816 of 1000 took 0.730s\n",
      "training loss:\t\t0.014088\n",
      "Epoch 817 of 1000 took 0.760s\n",
      "training loss:\t\t0.014083\n",
      "Epoch 818 of 1000 took 0.735s\n",
      "training loss:\t\t0.014078\n",
      "Epoch 819 of 1000 took 0.714s\n",
      "training loss:\t\t0.014073\n",
      "Epoch 820 of 1000 took 0.705s\n",
      "training loss:\t\t0.014068\n",
      "Epoch 821 of 1000 took 0.730s\n",
      "training loss:\t\t0.014064\n",
      "Epoch 822 of 1000 took 0.705s\n",
      "training loss:\t\t0.014059\n",
      "Epoch 823 of 1000 took 0.710s\n",
      "training loss:\t\t0.014054\n",
      "Epoch 824 of 1000 took 0.767s\n",
      "training loss:\t\t0.014049\n",
      "Epoch 825 of 1000 took 0.742s\n",
      "training loss:\t\t0.014045\n",
      "Epoch 826 of 1000 took 0.699s\n",
      "training loss:\t\t0.014040\n",
      "Epoch 827 of 1000 took 0.700s\n",
      "training loss:\t\t0.014035\n",
      "Epoch 828 of 1000 took 0.700s\n",
      "training loss:\t\t0.014030\n",
      "Epoch 829 of 1000 took 0.705s\n",
      "training loss:\t\t0.014026\n",
      "Epoch 830 of 1000 took 0.702s\n",
      "training loss:\t\t0.014021\n",
      "Epoch 831 of 1000 took 0.706s\n",
      "training loss:\t\t0.014016\n",
      "Epoch 832 of 1000 took 0.740s\n",
      "training loss:\t\t0.014011\n",
      "Epoch 833 of 1000 took 0.706s\n",
      "training loss:\t\t0.014007\n",
      "Epoch 834 of 1000 took 0.706s\n",
      "training loss:\t\t0.014002\n",
      "Epoch 835 of 1000 took 0.703s\n",
      "training loss:\t\t0.013997\n",
      "Epoch 836 of 1000 took 0.703s\n",
      "training loss:\t\t0.013993\n",
      "Epoch 837 of 1000 took 0.701s\n",
      "training loss:\t\t0.013988\n",
      "Epoch 838 of 1000 took 0.705s\n",
      "training loss:\t\t0.013984\n",
      "Epoch 839 of 1000 took 0.705s\n",
      "training loss:\t\t0.013979\n",
      "Epoch 840 of 1000 took 0.707s\n",
      "training loss:\t\t0.013974\n",
      "Epoch 841 of 1000 took 0.712s\n",
      "training loss:\t\t0.013970\n",
      "Epoch 842 of 1000 took 0.703s\n",
      "training loss:\t\t0.013965\n",
      "Epoch 843 of 1000 took 0.703s\n",
      "training loss:\t\t0.013961\n",
      "Epoch 844 of 1000 took 0.704s\n",
      "training loss:\t\t0.013956\n",
      "Epoch 845 of 1000 took 0.704s\n",
      "training loss:\t\t0.013951\n",
      "Epoch 846 of 1000 took 0.704s\n",
      "training loss:\t\t0.013947\n",
      "Epoch 847 of 1000 took 0.708s\n",
      "training loss:\t\t0.013942\n",
      "Epoch 848 of 1000 took 0.707s\n",
      "training loss:\t\t0.013938\n",
      "Epoch 849 of 1000 took 0.700s\n",
      "training loss:\t\t0.013933\n",
      "Epoch 850 of 1000 took 0.702s\n",
      "training loss:\t\t0.013929\n",
      "Epoch 851 of 1000 took 0.701s\n",
      "training loss:\t\t0.013924\n",
      "Epoch 852 of 1000 took 0.708s\n",
      "training loss:\t\t0.013920\n",
      "Epoch 853 of 1000 took 0.738s\n",
      "training loss:\t\t0.013915\n",
      "Epoch 854 of 1000 took 0.742s\n",
      "training loss:\t\t0.013911\n",
      "Epoch 855 of 1000 took 0.711s\n",
      "training loss:\t\t0.013906\n",
      "Epoch 856 of 1000 took 0.705s\n",
      "training loss:\t\t0.013902\n",
      "Epoch 857 of 1000 took 0.787s\n",
      "training loss:\t\t0.013897\n",
      "Epoch 858 of 1000 took 0.717s\n",
      "training loss:\t\t0.013893\n",
      "Epoch 859 of 1000 took 0.704s\n",
      "training loss:\t\t0.013888\n",
      "Epoch 860 of 1000 took 0.703s\n",
      "training loss:\t\t0.013884\n",
      "Epoch 861 of 1000 took 0.704s\n",
      "training loss:\t\t0.013880\n",
      "Epoch 862 of 1000 took 0.703s\n",
      "training loss:\t\t0.013875\n",
      "Epoch 863 of 1000 took 0.701s\n",
      "training loss:\t\t0.013871\n",
      "Epoch 864 of 1000 took 0.701s\n",
      "training loss:\t\t0.013866\n",
      "Epoch 865 of 1000 took 0.700s\n",
      "training loss:\t\t0.013862\n",
      "Epoch 866 of 1000 took 0.702s\n",
      "training loss:\t\t0.013857\n",
      "Epoch 867 of 1000 took 0.704s\n",
      "training loss:\t\t0.013853\n",
      "Epoch 868 of 1000 took 0.700s\n",
      "training loss:\t\t0.013849\n",
      "Epoch 869 of 1000 took 0.700s\n",
      "training loss:\t\t0.013844\n",
      "Epoch 870 of 1000 took 0.699s\n",
      "training loss:\t\t0.013840\n",
      "Epoch 871 of 1000 took 0.700s\n",
      "training loss:\t\t0.013836\n",
      "Epoch 872 of 1000 took 0.743s\n",
      "training loss:\t\t0.013831\n",
      "Epoch 873 of 1000 took 0.703s\n",
      "training loss:\t\t0.013827\n",
      "Epoch 874 of 1000 took 0.703s\n",
      "training loss:\t\t0.013823\n",
      "Epoch 875 of 1000 took 0.760s\n",
      "training loss:\t\t0.013818\n",
      "Epoch 876 of 1000 took 0.702s\n",
      "training loss:\t\t0.013814\n",
      "Epoch 877 of 1000 took 0.703s\n",
      "training loss:\t\t0.013810\n",
      "Epoch 878 of 1000 took 0.703s\n",
      "training loss:\t\t0.013805\n",
      "Epoch 879 of 1000 took 0.704s\n",
      "training loss:\t\t0.013801\n",
      "Epoch 880 of 1000 took 0.745s\n",
      "training loss:\t\t0.013797\n",
      "Epoch 881 of 1000 took 0.723s\n",
      "training loss:\t\t0.013792\n",
      "Epoch 882 of 1000 took 0.702s\n",
      "training loss:\t\t0.013788\n",
      "Epoch 883 of 1000 took 0.703s\n",
      "training loss:\t\t0.013784\n",
      "Epoch 884 of 1000 took 0.704s\n",
      "training loss:\t\t0.013779\n",
      "Epoch 885 of 1000 took 0.699s\n",
      "training loss:\t\t0.013775\n",
      "Epoch 886 of 1000 took 0.701s\n",
      "training loss:\t\t0.013771\n",
      "Epoch 887 of 1000 took 0.701s\n",
      "training loss:\t\t0.013767\n",
      "Epoch 888 of 1000 took 0.702s\n",
      "training loss:\t\t0.013762\n",
      "Epoch 889 of 1000 took 0.705s\n",
      "training loss:\t\t0.013758\n",
      "Epoch 890 of 1000 took 0.734s\n",
      "training loss:\t\t0.013754\n",
      "Epoch 891 of 1000 took 0.702s\n",
      "training loss:\t\t0.013750\n",
      "Epoch 892 of 1000 took 0.701s\n",
      "training loss:\t\t0.013745\n",
      "Epoch 893 of 1000 took 0.701s\n",
      "training loss:\t\t0.013741\n",
      "Epoch 894 of 1000 took 0.704s\n",
      "training loss:\t\t0.013737\n",
      "Epoch 895 of 1000 took 0.705s\n",
      "training loss:\t\t0.013733\n",
      "Epoch 896 of 1000 took 0.756s\n",
      "training loss:\t\t0.013728\n",
      "Epoch 897 of 1000 took 0.726s\n",
      "training loss:\t\t0.013724\n",
      "Epoch 898 of 1000 took 0.711s\n",
      "training loss:\t\t0.013720\n",
      "Epoch 899 of 1000 took 0.706s\n",
      "training loss:\t\t0.013716\n",
      "Epoch 900 of 1000 took 0.709s\n",
      "training loss:\t\t0.013712\n",
      "Epoch 901 of 1000 took 0.703s\n",
      "training loss:\t\t0.013707\n",
      "Epoch 902 of 1000 took 0.701s\n",
      "training loss:\t\t0.013703\n",
      "Epoch 903 of 1000 took 0.703s\n",
      "training loss:\t\t0.013699\n",
      "Epoch 904 of 1000 took 0.720s\n",
      "training loss:\t\t0.013695\n",
      "Epoch 905 of 1000 took 0.707s\n",
      "training loss:\t\t0.013691\n",
      "Epoch 906 of 1000 took 0.716s\n",
      "training loss:\t\t0.013687\n",
      "Epoch 907 of 1000 took 0.758s\n",
      "training loss:\t\t0.013682\n",
      "Epoch 908 of 1000 took 0.703s\n",
      "training loss:\t\t0.013678\n",
      "Epoch 909 of 1000 took 0.704s\n",
      "training loss:\t\t0.013674\n",
      "Epoch 910 of 1000 took 0.745s\n",
      "training loss:\t\t0.013670\n",
      "Epoch 911 of 1000 took 0.733s\n",
      "training loss:\t\t0.013666\n",
      "Epoch 912 of 1000 took 0.746s\n",
      "training loss:\t\t0.013662\n",
      "Epoch 913 of 1000 took 0.738s\n",
      "training loss:\t\t0.013657\n",
      "Epoch 914 of 1000 took 0.733s\n",
      "training loss:\t\t0.013653\n",
      "Epoch 915 of 1000 took 0.726s\n",
      "training loss:\t\t0.013649\n",
      "Epoch 916 of 1000 took 0.746s\n",
      "training loss:\t\t0.013645\n",
      "Epoch 917 of 1000 took 0.719s\n",
      "training loss:\t\t0.013641\n",
      "Epoch 918 of 1000 took 0.706s\n",
      "training loss:\t\t0.013637\n",
      "Epoch 919 of 1000 took 0.711s\n",
      "training loss:\t\t0.013633\n",
      "Epoch 920 of 1000 took 0.709s\n",
      "training loss:\t\t0.013629\n",
      "Epoch 921 of 1000 took 0.707s\n",
      "training loss:\t\t0.013625\n",
      "Epoch 922 of 1000 took 0.709s\n",
      "training loss:\t\t0.013620\n",
      "Epoch 923 of 1000 took 0.709s\n",
      "training loss:\t\t0.013616\n",
      "Epoch 924 of 1000 took 0.706s\n",
      "training loss:\t\t0.013612\n",
      "Epoch 925 of 1000 took 0.703s\n",
      "training loss:\t\t0.013608\n",
      "Epoch 926 of 1000 took 0.703s\n",
      "training loss:\t\t0.013604\n",
      "Epoch 927 of 1000 took 0.701s\n",
      "training loss:\t\t0.013600\n",
      "Epoch 928 of 1000 took 0.702s\n",
      "training loss:\t\t0.013596\n",
      "Epoch 929 of 1000 took 0.703s\n",
      "training loss:\t\t0.013592\n",
      "Epoch 930 of 1000 took 0.703s\n",
      "training loss:\t\t0.013588\n",
      "Epoch 931 of 1000 took 0.711s\n",
      "training loss:\t\t0.013584\n",
      "Epoch 932 of 1000 took 0.762s\n",
      "training loss:\t\t0.013580\n",
      "Epoch 933 of 1000 took 0.758s\n",
      "training loss:\t\t0.013576\n",
      "Epoch 934 of 1000 took 0.737s\n",
      "training loss:\t\t0.013572\n",
      "Epoch 935 of 1000 took 0.738s\n",
      "training loss:\t\t0.013568\n",
      "Epoch 936 of 1000 took 0.737s\n",
      "training loss:\t\t0.013564\n",
      "Epoch 937 of 1000 took 0.743s\n",
      "training loss:\t\t0.013560\n",
      "Epoch 938 of 1000 took 0.736s\n",
      "training loss:\t\t0.013556\n",
      "Epoch 939 of 1000 took 0.700s\n",
      "training loss:\t\t0.013552\n",
      "Epoch 940 of 1000 took 0.707s\n",
      "training loss:\t\t0.013548\n",
      "Epoch 941 of 1000 took 0.742s\n",
      "training loss:\t\t0.013544\n",
      "Epoch 942 of 1000 took 0.711s\n",
      "training loss:\t\t0.013540\n",
      "Epoch 943 of 1000 took 0.703s\n",
      "training loss:\t\t0.013536\n",
      "Epoch 944 of 1000 took 0.710s\n",
      "training loss:\t\t0.013532\n",
      "Epoch 945 of 1000 took 0.733s\n",
      "training loss:\t\t0.013528\n",
      "Epoch 946 of 1000 took 0.713s\n",
      "training loss:\t\t0.013524\n",
      "Epoch 947 of 1000 took 0.703s\n",
      "training loss:\t\t0.013520\n",
      "Epoch 948 of 1000 took 0.740s\n",
      "training loss:\t\t0.013516\n",
      "Epoch 949 of 1000 took 0.737s\n",
      "training loss:\t\t0.013512\n",
      "Epoch 950 of 1000 took 0.716s\n",
      "training loss:\t\t0.013508\n",
      "Epoch 951 of 1000 took 0.704s\n",
      "training loss:\t\t0.013504\n",
      "Epoch 952 of 1000 took 0.710s\n",
      "training loss:\t\t0.013500\n",
      "Epoch 953 of 1000 took 0.724s\n",
      "training loss:\t\t0.013496\n",
      "Epoch 954 of 1000 took 0.706s\n",
      "training loss:\t\t0.013492\n",
      "Epoch 955 of 1000 took 0.704s\n",
      "training loss:\t\t0.013488\n",
      "Epoch 956 of 1000 took 0.700s\n",
      "training loss:\t\t0.013484\n",
      "Epoch 957 of 1000 took 0.702s\n",
      "training loss:\t\t0.013480\n",
      "Epoch 958 of 1000 took 0.700s\n",
      "training loss:\t\t0.013476\n",
      "Epoch 959 of 1000 took 0.705s\n",
      "training loss:\t\t0.013472\n",
      "Epoch 960 of 1000 took 0.736s\n",
      "training loss:\t\t0.013468\n",
      "Epoch 961 of 1000 took 0.777s\n",
      "training loss:\t\t0.013464\n",
      "Epoch 962 of 1000 took 0.735s\n",
      "training loss:\t\t0.013460\n",
      "Epoch 963 of 1000 took 0.704s\n",
      "training loss:\t\t0.013457\n",
      "Epoch 964 of 1000 took 0.704s\n",
      "training loss:\t\t0.013453\n",
      "Epoch 965 of 1000 took 0.702s\n",
      "training loss:\t\t0.013449\n",
      "Epoch 966 of 1000 took 0.703s\n",
      "training loss:\t\t0.013445\n",
      "Epoch 967 of 1000 took 0.701s\n",
      "training loss:\t\t0.013441\n",
      "Epoch 968 of 1000 took 0.699s\n",
      "training loss:\t\t0.013437\n",
      "Epoch 969 of 1000 took 0.700s\n",
      "training loss:\t\t0.013433\n",
      "Epoch 970 of 1000 took 0.703s\n",
      "training loss:\t\t0.013429\n",
      "Epoch 971 of 1000 took 0.704s\n",
      "training loss:\t\t0.013425\n",
      "Epoch 972 of 1000 took 0.701s\n",
      "training loss:\t\t0.013422\n",
      "Epoch 973 of 1000 took 0.724s\n",
      "training loss:\t\t0.013418\n",
      "Epoch 974 of 1000 took 0.703s\n",
      "training loss:\t\t0.013414\n",
      "Epoch 975 of 1000 took 0.701s\n",
      "training loss:\t\t0.013410\n",
      "Epoch 976 of 1000 took 0.699s\n",
      "training loss:\t\t0.013406\n",
      "Epoch 977 of 1000 took 0.698s\n",
      "training loss:\t\t0.013402\n",
      "Epoch 978 of 1000 took 0.700s\n",
      "training loss:\t\t0.013398\n",
      "Epoch 979 of 1000 took 0.700s\n",
      "training loss:\t\t0.013395\n",
      "Epoch 980 of 1000 took 0.705s\n",
      "training loss:\t\t0.013391\n",
      "Epoch 981 of 1000 took 0.704s\n",
      "training loss:\t\t0.013387\n",
      "Epoch 982 of 1000 took 0.706s\n",
      "training loss:\t\t0.013383\n",
      "Epoch 983 of 1000 took 0.706s\n",
      "training loss:\t\t0.013379\n",
      "Epoch 984 of 1000 took 0.707s\n",
      "training loss:\t\t0.013375\n",
      "Epoch 985 of 1000 took 0.704s\n",
      "training loss:\t\t0.013372\n",
      "Epoch 986 of 1000 took 0.740s\n",
      "training loss:\t\t0.013368\n",
      "Epoch 987 of 1000 took 0.703s\n",
      "training loss:\t\t0.013364\n",
      "Epoch 988 of 1000 took 0.776s\n",
      "training loss:\t\t0.013360\n",
      "Epoch 989 of 1000 took 0.763s\n",
      "training loss:\t\t0.013356\n",
      "Epoch 990 of 1000 took 0.723s\n",
      "training loss:\t\t0.013353\n",
      "Epoch 991 of 1000 took 0.773s\n",
      "training loss:\t\t0.013349\n",
      "Epoch 992 of 1000 took 0.708s\n",
      "training loss:\t\t0.013345\n",
      "Epoch 993 of 1000 took 0.706s\n",
      "training loss:\t\t0.013341\n",
      "Epoch 994 of 1000 took 0.739s\n",
      "training loss:\t\t0.013337\n",
      "Epoch 995 of 1000 took 0.740s\n",
      "training loss:\t\t0.013334\n",
      "Epoch 996 of 1000 took 0.714s\n",
      "training loss:\t\t0.013330\n",
      "Epoch 997 of 1000 took 0.708s\n",
      "training loss:\t\t0.013326\n",
      "Epoch 998 of 1000 took 0.713s\n",
      "training loss:\t\t0.013322\n",
      "Epoch 999 of 1000 took 0.705s\n",
      "training loss:\t\t0.013319\n",
      "Epoch 1000 of 1000 took 0.731s\n",
      "training loss:\t\t0.013315\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.matrix('targets')\n",
    "learnrate=0.01\n",
    "# Create neural network model (depending on first command line parameter)\n",
    "print(\"Building model and compiling functions...\")\n",
    "network = build_cnn(input_var)\n",
    "reconstructed = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.squared_error(reconstructed, target_var)\n",
    "loss = loss.mean()\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "   loss, params, learning_rate=learnrate, momentum=0.975)\n",
    "# updates = lasagne.updates.rmsprop(loss, params, learning_rate=learnrate)\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates,on_unused_input='warn')\n",
    "best_err = 0.03\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_err = train_fn(X, X_out)\n",
    "        # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"training loss:\\t\\t{:.6f}\".format(float(train_err)))\n",
    "#     if train_err < best_err:\n",
    "#         best_err = train_err\n",
    "#         np.savez('./output/autoencoder_mars.npz', *lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.matrix('targets')\n",
    "learnrate=0.005\n",
    "# Create neural network model (depending on first command line parameter)\n",
    "print(\"Building model and compiling functions...\")\n",
    "network = build_cnn(input_var)\n",
    "reconstructed = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.squared_error(reconstructed, target_var)\n",
    "loss = loss.mean()\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "\n",
    "with np.load('./output/autoencoder_mars.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "lasagne.layers.set_all_param_values(network, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1 of 500 took 0.361s\n",
      "0.0014875360066071153\n",
      "training loss:\t\t0.001488\n",
      "Epoch 2 of 500 took 0.334s\n",
      "0.001248935703188181\n",
      "training loss:\t\t0.001249\n",
      "Epoch 3 of 500 took 0.332s\n",
      "0.001231659553013742\n",
      "training loss:\t\t0.001232\n",
      "Epoch 4 of 500 took 0.332s\n",
      "0.0012827329337596893\n",
      "training loss:\t\t0.001283\n",
      "Epoch 5 of 500 took 0.332s\n",
      "0.0012903702445328236\n",
      "training loss:\t\t0.001290\n",
      "Epoch 6 of 500 took 0.332s\n",
      "0.0012558357557281852\n",
      "training loss:\t\t0.001256\n",
      "Epoch 7 of 500 took 0.332s\n",
      "0.0012255150359123945\n",
      "training loss:\t\t0.001226\n",
      "Epoch 8 of 500 took 0.332s\n",
      "0.001219461322762072\n",
      "training loss:\t\t0.001219\n",
      "Epoch 9 of 500 took 0.354s\n",
      "0.0012250897707417607\n",
      "training loss:\t\t0.001225\n",
      "Epoch 10 of 500 took 0.357s\n",
      "0.0012269688304513693\n",
      "training loss:\t\t0.001227\n",
      "Epoch 11 of 500 took 0.338s\n",
      "0.0012231176951900125\n",
      "training loss:\t\t0.001223\n",
      "Epoch 12 of 500 took 0.365s\n",
      "0.0012190439738333225\n",
      "training loss:\t\t0.001219\n",
      "Epoch 13 of 500 took 0.333s\n",
      "0.0012177638709545135\n",
      "training loss:\t\t0.001218\n",
      "Epoch 14 of 500 took 0.334s\n",
      "0.0012181627098470926\n",
      "training loss:\t\t0.001218\n",
      "Epoch 15 of 500 took 0.334s\n",
      "0.001218319172039628\n",
      "training loss:\t\t0.001218\n",
      "Epoch 16 of 500 took 0.344s\n",
      "0.001217770972289145\n",
      "training loss:\t\t0.001218\n",
      "Epoch 17 of 500 took 0.370s\n",
      "0.0012171283597126603\n",
      "training loss:\t\t0.001217\n",
      "Epoch 18 of 500 took 0.334s\n",
      "0.0012167944805696607\n",
      "training loss:\t\t0.001217\n",
      "Epoch 19 of 500 took 0.332s\n",
      "0.0012166870292276144\n",
      "training loss:\t\t0.001217\n",
      "Epoch 20 of 500 took 0.352s\n",
      "0.0012165787629783154\n",
      "training loss:\t\t0.001217\n",
      "Epoch 21 of 500 took 0.358s\n",
      "0.0012163979699835181\n",
      "training loss:\t\t0.001216\n",
      "Epoch 22 of 500 took 0.333s\n",
      "0.0012161987833678722\n",
      "training loss:\t\t0.001216\n",
      "Epoch 23 of 500 took 0.332s\n",
      "0.001216038828715682\n",
      "training loss:\t\t0.001216\n",
      "Epoch 24 of 500 took 0.332s\n",
      "0.001215910306200385\n",
      "training loss:\t\t0.001216\n",
      "Epoch 25 of 500 took 0.332s\n",
      "0.0012157929595559835\n",
      "training loss:\t\t0.001216\n",
      "Epoch 26 of 500 took 0.332s\n",
      "0.001215671538375318\n",
      "training loss:\t\t0.001216\n",
      "Epoch 27 of 500 took 0.332s\n",
      "0.001215551164932549\n",
      "training loss:\t\t0.001216\n",
      "Epoch 28 of 500 took 0.332s\n",
      "0.001215440104715526\n",
      "training loss:\t\t0.001215\n",
      "Epoch 29 of 500 took 0.333s\n",
      "0.001215339289046824\n",
      "training loss:\t\t0.001215\n",
      "Epoch 30 of 500 took 0.333s\n",
      "0.001215245109051466\n",
      "training loss:\t\t0.001215\n",
      "Epoch 31 of 500 took 0.332s\n",
      "0.0012151561677455902\n",
      "training loss:\t\t0.001215\n",
      "Epoch 32 of 500 took 0.333s\n",
      "0.0012150713009759784\n",
      "training loss:\t\t0.001215\n",
      "Epoch 33 of 500 took 0.333s\n",
      "0.0012149896938353777\n",
      "training loss:\t\t0.001215\n",
      "Epoch 34 of 500 took 0.332s\n",
      "0.0012149120448157191\n",
      "training loss:\t\t0.001215\n",
      "Epoch 35 of 500 took 0.332s\n",
      "0.0012148376554250717\n",
      "training loss:\t\t0.001215\n",
      "Epoch 36 of 500 took 0.332s\n",
      "0.0012147669913247228\n",
      "training loss:\t\t0.001215\n",
      "Epoch 37 of 500 took 0.332s\n",
      "0.0012146987719461322\n",
      "training loss:\t\t0.001215\n",
      "Epoch 38 of 500 took 0.332s\n",
      "0.0012146333465352654\n",
      "training loss:\t\t0.001215\n",
      "Epoch 39 of 500 took 0.332s\n",
      "0.0012145709479227662\n",
      "training loss:\t\t0.001215\n",
      "Epoch 40 of 500 took 0.332s\n",
      "0.0012145107612013817\n",
      "training loss:\t\t0.001215\n",
      "Epoch 41 of 500 took 0.332s\n",
      "0.0012144531356170774\n",
      "training loss:\t\t0.001214\n",
      "Epoch 42 of 500 took 0.334s\n",
      "0.001214396907016635\n",
      "training loss:\t\t0.001214\n",
      "Epoch 43 of 500 took 0.332s\n",
      "0.0012143420754000545\n",
      "training loss:\t\t0.001214\n",
      "Epoch 44 of 500 took 0.331s\n",
      "0.0012142891064286232\n",
      "training loss:\t\t0.001214\n",
      "Epoch 45 of 500 took 0.332s\n",
      "0.0012142382329329848\n",
      "training loss:\t\t0.001214\n",
      "Epoch 46 of 500 took 0.334s\n",
      "0.0012141913175582886\n",
      "training loss:\t\t0.001214\n",
      "Epoch 47 of 500 took 0.334s\n",
      "0.001214145915582776\n",
      "training loss:\t\t0.001214\n",
      "Epoch 48 of 500 took 0.334s\n",
      "0.0012141019105911255\n",
      "training loss:\t\t0.001214\n",
      "Epoch 49 of 500 took 0.332s\n",
      "0.0012140597682446241\n",
      "training loss:\t\t0.001214\n",
      "Epoch 50 of 500 took 0.334s\n",
      "0.0012140189064666629\n",
      "training loss:\t\t0.001214\n",
      "Epoch 51 of 500 took 0.334s\n",
      "0.00121398048941046\n",
      "training loss:\t\t0.001214\n",
      "Epoch 52 of 500 took 0.331s\n",
      "0.0012139435857534409\n",
      "training loss:\t\t0.001214\n",
      "Epoch 53 of 500 took 0.332s\n",
      "0.0012139070313423872\n",
      "training loss:\t\t0.001214\n",
      "Epoch 54 of 500 took 0.333s\n",
      "0.001213870127685368\n",
      "training loss:\t\t0.001214\n",
      "Epoch 55 of 500 took 0.334s\n",
      "0.0012138326419517398\n",
      "training loss:\t\t0.001214\n",
      "Epoch 56 of 500 took 0.334s\n",
      "0.0012137945741415024\n",
      "training loss:\t\t0.001214\n",
      "Epoch 57 of 500 took 0.334s\n",
      "0.0012137554585933685\n",
      "training loss:\t\t0.001214\n",
      "Epoch 58 of 500 took 0.333s\n",
      "0.0012137151788920164\n",
      "training loss:\t\t0.001214\n",
      "Epoch 59 of 500 took 0.334s\n",
      "0.0012136735022068024\n",
      "training loss:\t\t0.001214\n",
      "Epoch 60 of 500 took 0.333s\n",
      "0.001213631359860301\n",
      "training loss:\t\t0.001214\n",
      "Epoch 61 of 500 took 0.332s\n",
      "0.001213587587699294\n",
      "training loss:\t\t0.001214\n",
      "Epoch 62 of 500 took 0.332s\n",
      "0.0012135430006310344\n",
      "training loss:\t\t0.001214\n",
      "Epoch 63 of 500 took 0.335s\n",
      "0.0012134978314861655\n",
      "training loss:\t\t0.001213\n",
      "Epoch 64 of 500 took 0.334s\n",
      "0.0012134516146034002\n",
      "training loss:\t\t0.001213\n",
      "Epoch 65 of 500 took 0.334s\n",
      "0.0012134045828133821\n",
      "training loss:\t\t0.001213\n",
      "Epoch 66 of 500 took 0.334s\n",
      "0.0012133579002693295\n",
      "training loss:\t\t0.001213\n",
      "Epoch 67 of 500 took 0.332s\n",
      "0.0012133111013099551\n",
      "training loss:\t\t0.001213\n",
      "Epoch 68 of 500 took 0.333s\n",
      "0.0012132648844271898\n",
      "training loss:\t\t0.001213\n",
      "Epoch 69 of 500 took 0.333s\n",
      "0.0012132184347137809\n",
      "training loss:\t\t0.001213\n",
      "Epoch 70 of 500 took 0.331s\n",
      "0.0012131723342463374\n",
      "training loss:\t\t0.001213\n",
      "Epoch 71 of 500 took 0.333s\n",
      "0.0012131263501942158\n",
      "training loss:\t\t0.001213\n",
      "Epoch 72 of 500 took 0.333s\n",
      "0.0012130814138799906\n",
      "training loss:\t\t0.001213\n",
      "Epoch 73 of 500 took 0.334s\n",
      "0.001213036710396409\n",
      "training loss:\t\t0.001213\n",
      "Epoch 74 of 500 took 0.334s\n",
      "0.0012129932874813676\n",
      "training loss:\t\t0.001213\n",
      "Epoch 75 of 500 took 0.333s\n",
      "0.0012129503302276134\n",
      "training loss:\t\t0.001213\n",
      "Epoch 76 of 500 took 0.333s\n",
      "0.001212908304296434\n",
      "training loss:\t\t0.001213\n",
      "Epoch 77 of 500 took 0.334s\n",
      "0.0012128676753491163\n",
      "training loss:\t\t0.001213\n",
      "Epoch 78 of 500 took 0.334s\n",
      "0.001212828909046948\n",
      "training loss:\t\t0.001213\n",
      "Epoch 79 of 500 took 0.332s\n",
      "0.0012127908412367105\n",
      "training loss:\t\t0.001213\n",
      "Epoch 80 of 500 took 0.333s\n",
      "0.001212754868902266\n",
      "training loss:\t\t0.001213\n",
      "Epoch 81 of 500 took 0.334s\n",
      "0.0012127193622291088\n",
      "training loss:\t\t0.001213\n",
      "Epoch 82 of 500 took 0.334s\n",
      "0.0012126850197091699\n",
      "training loss:\t\t0.001213\n",
      "Epoch 83 of 500 took 0.334s\n",
      "0.0012126520741730928\n",
      "training loss:\t\t0.001213\n",
      "Epoch 84 of 500 took 0.334s\n",
      "0.0012126191286370158\n",
      "training loss:\t\t0.001213\n",
      "Epoch 85 of 500 took 0.331s\n",
      "0.001212586765177548\n",
      "training loss:\t\t0.001213\n",
      "Epoch 86 of 500 took 0.333s\n",
      "0.0012125547509640455\n",
      "training loss:\t\t0.001213\n",
      "Epoch 87 of 500 took 0.334s\n",
      "0.0012125230859965086\n",
      "training loss:\t\t0.001213\n",
      "Epoch 88 of 500 took 0.331s\n",
      "0.0012124915374442935\n",
      "training loss:\t\t0.001212\n",
      "Epoch 89 of 500 took 0.333s\n",
      "0.001212460920214653\n",
      "training loss:\t\t0.001212\n",
      "Epoch 90 of 500 took 0.334s\n",
      "0.0012124294880777597\n",
      "training loss:\t\t0.001212\n",
      "Epoch 91 of 500 took 0.334s\n",
      "0.0012123981723561883\n",
      "training loss:\t\t0.001212\n",
      "Epoch 92 of 500 took 0.334s\n",
      "0.001212366740219295\n",
      "training loss:\t\t0.001212\n",
      "Epoch 93 of 500 took 0.333s\n",
      "0.0012123362394049764\n",
      "training loss:\t\t0.001212\n",
      "Epoch 94 of 500 took 0.334s\n",
      "0.001212304923683405\n",
      "training loss:\t\t0.001212\n",
      "Epoch 95 of 500 took 0.334s\n",
      "0.0012122743064537644\n",
      "training loss:\t\t0.001212\n",
      "Epoch 96 of 500 took 0.334s\n",
      "0.0012122432235628366\n",
      "training loss:\t\t0.001212\n",
      "Epoch 97 of 500 took 0.331s\n",
      "0.0012122129555791616\n",
      "training loss:\t\t0.001212\n",
      "Epoch 98 of 500 took 0.332s\n",
      "0.0012121821055188775\n",
      "training loss:\t\t0.001212\n",
      "Epoch 99 of 500 took 0.334s\n",
      "0.0012121516047045588\n",
      "training loss:\t\t0.001212\n",
      "Epoch 100 of 500 took 0.334s\n",
      "0.0012121215695515275\n",
      "training loss:\t\t0.001212\n",
      "Epoch 101 of 500 took 0.334s\n",
      "0.0012120915343984962\n",
      "training loss:\t\t0.001212\n",
      "Epoch 102 of 500 took 0.334s\n",
      "0.0012120616156607866\n",
      "training loss:\t\t0.001212\n",
      "Epoch 103 of 500 took 0.334s\n",
      "0.001212032395415008\n",
      "training loss:\t\t0.001212\n",
      "Epoch 104 of 500 took 0.334s\n",
      "0.0012120023602619767\n",
      "training loss:\t\t0.001212\n",
      "Epoch 105 of 500 took 0.334s\n",
      "0.001211972557939589\n",
      "training loss:\t\t0.001212\n",
      "Epoch 106 of 500 took 0.331s\n",
      "0.0012119435705244541\n",
      "training loss:\t\t0.001212\n",
      "Epoch 107 of 500 took 0.332s\n",
      "0.0012119142338633537\n",
      "training loss:\t\t0.001212\n",
      "Epoch 108 of 500 took 0.334s\n",
      "0.0012118853628635406\n",
      "training loss:\t\t0.001212\n",
      "Epoch 109 of 500 took 0.334s\n",
      "0.0012118566082790494\n",
      "training loss:\t\t0.001212\n",
      "Epoch 110 of 500 took 0.334s\n",
      "0.00121182797010988\n",
      "training loss:\t\t0.001212\n",
      "Epoch 111 of 500 took 0.333s\n",
      "0.0012117999140173197\n",
      "training loss:\t\t0.001212\n",
      "Epoch 112 of 500 took 0.334s\n",
      "0.0012117709266021848\n",
      "training loss:\t\t0.001212\n",
      "Epoch 113 of 500 took 0.333s\n",
      "0.0012117427540943027\n",
      "training loss:\t\t0.001212\n",
      "Epoch 114 of 500 took 0.335s\n",
      "0.001211714930832386\n",
      "training loss:\t\t0.001212\n",
      "Epoch 115 of 500 took 0.331s\n",
      "0.001211686641909182\n",
      "training loss:\t\t0.001212\n",
      "Epoch 116 of 500 took 0.332s\n",
      "0.0012116587022319436\n",
      "training loss:\t\t0.001212\n",
      "Epoch 117 of 500 took 0.335s\n",
      "0.0012116309953853488\n",
      "training loss:\t\t0.001212\n",
      "Epoch 118 of 500 took 0.335s\n",
      "0.001211603288538754\n",
      "training loss:\t\t0.001212\n",
      "Epoch 119 of 500 took 0.331s\n",
      "0.001211575698107481\n",
      "training loss:\t\t0.001212\n",
      "Epoch 120 of 500 took 0.334s\n",
      "0.0012115477584302425\n",
      "training loss:\t\t0.001212\n",
      "Epoch 121 of 500 took 0.332s\n",
      "0.001211520517244935\n",
      "training loss:\t\t0.001212\n",
      "Epoch 122 of 500 took 0.334s\n",
      "0.0012114931596443057\n",
      "training loss:\t\t0.001211\n",
      "Epoch 123 of 500 took 0.334s\n",
      "0.001211465452797711\n",
      "training loss:\t\t0.001211\n",
      "Epoch 124 of 500 took 0.332s\n",
      "0.0012114386772736907\n",
      "training loss:\t\t0.001211\n",
      "Epoch 125 of 500 took 0.333s\n",
      "0.0012114123674109578\n",
      "training loss:\t\t0.001211\n",
      "Epoch 126 of 500 took 0.334s\n",
      "0.0012113852426409721\n",
      "training loss:\t\t0.001211\n",
      "Epoch 127 of 500 took 0.334s\n",
      "0.0012113585835322738\n",
      "training loss:\t\t0.001211\n",
      "Epoch 128 of 500 took 0.335s\n",
      "0.0012113323900848627\n",
      "training loss:\t\t0.001211\n",
      "Epoch 129 of 500 took 0.334s\n",
      "0.001211305963806808\n",
      "training loss:\t\t0.001211\n",
      "Epoch 130 of 500 took 0.331s\n",
      "0.0012112800031900406\n",
      "training loss:\t\t0.001211\n",
      "Epoch 131 of 500 took 0.335s\n",
      "0.001211254158988595\n",
      "training loss:\t\t0.001211\n",
      "Epoch 132 of 500 took 0.334s\n",
      "0.001211228547617793\n",
      "training loss:\t\t0.001211\n",
      "Epoch 133 of 500 took 0.333s\n",
      "0.0012112034019082785\n",
      "training loss:\t\t0.001211\n",
      "Epoch 134 of 500 took 0.332s\n",
      "0.0012111780233681202\n",
      "training loss:\t\t0.001211\n",
      "Epoch 135 of 500 took 0.334s\n",
      "0.0012111529940739274\n",
      "training loss:\t\t0.001211\n",
      "Epoch 136 of 500 took 0.334s\n",
      "0.0012111278483644128\n",
      "training loss:\t\t0.001211\n",
      "Epoch 137 of 500 took 0.341s\n",
      "0.0012111029354855418\n",
      "training loss:\t\t0.001211\n",
      "Epoch 138 of 500 took 0.369s\n",
      "0.0012110783718526363\n",
      "training loss:\t\t0.001211\n",
      "Epoch 139 of 500 took 0.344s\n",
      "0.0012110535753890872\n",
      "training loss:\t\t0.001211\n",
      "Epoch 140 of 500 took 0.332s\n",
      "0.001211028778925538\n",
      "training loss:\t\t0.001211\n",
      "Epoch 141 of 500 took 0.331s\n",
      "0.0012110037496313453\n",
      "training loss:\t\t0.001211\n",
      "Epoch 142 of 500 took 0.357s\n",
      "0.001210979069583118\n",
      "training loss:\t\t0.001211\n",
      "Epoch 143 of 500 took 0.332s\n",
      "0.0012109546223655343\n",
      "training loss:\t\t0.001211\n",
      "Epoch 144 of 500 took 0.332s\n",
      "0.001210929942317307\n",
      "training loss:\t\t0.001211\n",
      "Epoch 145 of 500 took 0.336s\n",
      "0.0012109053786844015\n",
      "training loss:\t\t0.001211\n",
      "Epoch 146 of 500 took 0.342s\n",
      "0.0012108812807127833\n",
      "training loss:\t\t0.001211\n",
      "Epoch 147 of 500 took 0.355s\n",
      "0.0012108570663258433\n",
      "training loss:\t\t0.001211\n",
      "Epoch 148 of 500 took 0.332s\n",
      "0.0012108328519389033\n",
      "training loss:\t\t0.001211\n",
      "Epoch 149 of 500 took 0.331s\n",
      "0.0012108089867979288\n",
      "training loss:\t\t0.001211\n",
      "Epoch 150 of 500 took 0.333s\n",
      "0.0012107848888263106\n",
      "training loss:\t\t0.001211\n",
      "Epoch 151 of 500 took 0.333s\n",
      "0.0012107606744393706\n",
      "training loss:\t\t0.001211\n",
      "Epoch 152 of 500 took 0.332s\n",
      "0.0012107364600524306\n",
      "training loss:\t\t0.001211\n",
      "Epoch 153 of 500 took 0.333s\n",
      "0.0012107130605727434\n",
      "training loss:\t\t0.001211\n",
      "Epoch 154 of 500 took 0.333s\n",
      "0.0012106893118470907\n",
      "training loss:\t\t0.001211\n",
      "Epoch 155 of 500 took 0.332s\n",
      "0.0012106660287827253\n",
      "training loss:\t\t0.001211\n",
      "Epoch 156 of 500 took 0.335s\n",
      "0.0012106426293030381\n",
      "training loss:\t\t0.001211\n",
      "Epoch 157 of 500 took 0.332s\n",
      "0.0012106188805773854\n",
      "training loss:\t\t0.001211\n",
      "Epoch 158 of 500 took 0.346s\n",
      "0.0012105959467589855\n",
      "training loss:\t\t0.001211\n",
      "Epoch 159 of 500 took 0.361s\n",
      "0.0012105725472792983\n",
      "training loss:\t\t0.001211\n",
      "Epoch 160 of 500 took 0.361s\n",
      "0.0012105493806302547\n",
      "training loss:\t\t0.001211\n",
      "Epoch 161 of 500 took 0.335s\n",
      "0.0012105266796424985\n",
      "training loss:\t\t0.001211\n",
      "Epoch 162 of 500 took 0.333s\n",
      "0.001210504095070064\n",
      "training loss:\t\t0.001211\n",
      "Epoch 163 of 500 took 0.373s\n",
      "0.0012104810448363423\n",
      "training loss:\t\t0.001210\n",
      "Epoch 164 of 500 took 0.357s\n",
      "0.0012104579946026206\n",
      "training loss:\t\t0.001210\n",
      "Epoch 165 of 500 took 0.333s\n",
      "0.001210435526445508\n",
      "training loss:\t\t0.001210\n",
      "Epoch 166 of 500 took 0.335s\n",
      "0.001210412592627108\n",
      "training loss:\t\t0.001210\n",
      "Epoch 167 of 500 took 0.334s\n",
      "0.0012103902408853173\n",
      "training loss:\t\t0.001210\n",
      "Epoch 168 of 500 took 0.333s\n",
      "0.0012103681219741702\n",
      "training loss:\t\t0.001210\n",
      "Epoch 169 of 500 took 0.333s\n",
      "0.0012103463523089886\n",
      "training loss:\t\t0.001210\n",
      "Epoch 170 of 500 took 0.333s\n",
      "0.0012103235349059105\n",
      "training loss:\t\t0.001210\n",
      "Epoch 171 of 500 took 0.333s\n",
      "0.001210301648825407\n",
      "training loss:\t\t0.001210\n",
      "Epoch 172 of 500 took 0.334s\n",
      "0.0012102796463295817\n",
      "training loss:\t\t0.001210\n",
      "Epoch 173 of 500 took 0.333s\n",
      "0.0012102576438337564\n",
      "training loss:\t\t0.001210\n",
      "Epoch 174 of 500 took 0.332s\n",
      "0.001210235757753253\n",
      "training loss:\t\t0.001210\n",
      "Epoch 175 of 500 took 0.332s\n",
      "0.001210214220918715\n",
      "training loss:\t\t0.001210\n",
      "Epoch 176 of 500 took 0.332s\n",
      "0.0012101921020075679\n",
      "training loss:\t\t0.001210\n",
      "Epoch 177 of 500 took 0.332s\n",
      "0.00121017056517303\n",
      "training loss:\t\t0.001210\n",
      "Epoch 178 of 500 took 0.333s\n",
      "0.00121014891192317\n",
      "training loss:\t\t0.001210\n",
      "Epoch 179 of 500 took 0.334s\n",
      "0.001210127375088632\n",
      "training loss:\t\t0.001210\n",
      "Epoch 180 of 500 took 0.349s\n",
      "0.0012101061875000596\n",
      "training loss:\t\t0.001210\n",
      "Epoch 181 of 500 took 0.332s\n",
      "0.001210084417834878\n",
      "training loss:\t\t0.001210\n",
      "Epoch 182 of 500 took 0.333s\n",
      "0.001210063579492271\n",
      "training loss:\t\t0.001210\n",
      "Epoch 183 of 500 took 0.335s\n",
      "0.001210042042657733\n",
      "training loss:\t\t0.001210\n",
      "Epoch 184 of 500 took 0.332s\n",
      "0.0012100210878998041\n",
      "training loss:\t\t0.001210\n",
      "Epoch 185 of 500 took 0.332s\n",
      "0.0012099999003112316\n",
      "training loss:\t\t0.001210\n",
      "Epoch 186 of 500 took 0.341s\n",
      "0.0012099783634766936\n",
      "training loss:\t\t0.001210\n",
      "Epoch 187 of 500 took 0.332s\n",
      "0.001209957874380052\n",
      "training loss:\t\t0.001210\n",
      "Epoch 188 of 500 took 0.332s\n",
      "0.001209936337545514\n",
      "training loss:\t\t0.001210\n",
      "Epoch 189 of 500 took 0.333s\n",
      "0.001209914917126298\n",
      "training loss:\t\t0.001210\n",
      "Epoch 190 of 500 took 0.332s\n",
      "0.0012098945444449782\n",
      "training loss:\t\t0.001210\n",
      "Epoch 191 of 500 took 0.333s\n",
      "0.0012098730076104403\n",
      "training loss:\t\t0.001210\n",
      "Epoch 192 of 500 took 0.332s\n",
      "0.001209852285683155\n",
      "training loss:\t\t0.001210\n",
      "Epoch 193 of 500 took 0.333s\n",
      "0.0012098316801711917\n",
      "training loss:\t\t0.001210\n",
      "Epoch 194 of 500 took 0.333s\n",
      "0.0012098100269213319\n",
      "training loss:\t\t0.001210\n",
      "Epoch 195 of 500 took 0.332s\n",
      "0.001209789770655334\n",
      "training loss:\t\t0.001210\n",
      "Epoch 196 of 500 took 0.333s\n",
      "0.0012097684666514397\n",
      "training loss:\t\t0.001210\n",
      "Epoch 197 of 500 took 0.334s\n",
      "0.0012097478611394763\n",
      "training loss:\t\t0.001210\n",
      "Epoch 198 of 500 took 0.332s\n",
      "0.0012097273720428348\n",
      "training loss:\t\t0.001210\n",
      "Epoch 199 of 500 took 0.334s\n",
      "0.0012097061844542623\n",
      "training loss:\t\t0.001210\n",
      "Epoch 200 of 500 took 0.332s\n",
      "0.001209686161018908\n",
      "training loss:\t\t0.001210\n",
      "Epoch 201 of 500 took 0.333s\n",
      "0.0012096652062609792\n",
      "training loss:\t\t0.001210\n",
      "Epoch 202 of 500 took 0.333s\n",
      "0.001209644484333694\n",
      "training loss:\t\t0.001210\n",
      "Epoch 203 of 500 took 0.332s\n",
      "0.0012096238788217306\n",
      "training loss:\t\t0.001210\n",
      "Epoch 204 of 500 took 0.333s\n",
      "0.0012096042046323419\n",
      "training loss:\t\t0.001210\n",
      "Epoch 205 of 500 took 0.333s\n",
      "0.001209583249874413\n",
      "training loss:\t\t0.001210\n",
      "Epoch 206 of 500 took 0.333s\n",
      "0.0012095626443624496\n",
      "training loss:\t\t0.001210\n",
      "Epoch 207 of 500 took 0.332s\n",
      "0.0012095425045117736\n",
      "training loss:\t\t0.001210\n",
      "Epoch 208 of 500 took 0.333s\n",
      "0.0012095221318304539\n",
      "training loss:\t\t0.001210\n",
      "Epoch 209 of 500 took 0.332s\n",
      "0.0012095016427338123\n",
      "training loss:\t\t0.001210\n",
      "Epoch 210 of 500 took 0.333s\n",
      "0.0012094815028831363\n",
      "training loss:\t\t0.001209\n",
      "Epoch 211 of 500 took 0.332s\n",
      "0.0012094612466171384\n",
      "training loss:\t\t0.001209\n",
      "Epoch 212 of 500 took 0.336s\n",
      "0.0012094415724277496\n",
      "training loss:\t\t0.001209\n",
      "Epoch 213 of 500 took 0.337s\n",
      "0.0012094214325770736\n",
      "training loss:\t\t0.001209\n",
      "Epoch 214 of 500 took 0.341s\n",
      "0.0012094014091417193\n",
      "training loss:\t\t0.001209\n",
      "Epoch 215 of 500 took 0.361s\n",
      "0.001209381502121687\n",
      "training loss:\t\t0.001209\n",
      "Epoch 216 of 500 took 0.357s\n",
      "0.0012093615951016545\n",
      "training loss:\t\t0.001209\n",
      "Epoch 217 of 500 took 0.346s\n",
      "0.0012093413388356566\n",
      "training loss:\t\t0.001209\n",
      "Epoch 218 of 500 took 0.342s\n",
      "0.001209321548230946\n",
      "training loss:\t\t0.001209\n",
      "Epoch 219 of 500 took 0.337s\n",
      "0.0012093022232875228\n",
      "training loss:\t\t0.001209\n",
      "Epoch 220 of 500 took 0.334s\n",
      "0.0012092820834368467\n",
      "training loss:\t\t0.001209\n",
      "Epoch 221 of 500 took 0.332s\n",
      "0.0012092628749087453\n",
      "training loss:\t\t0.001209\n",
      "Epoch 222 of 500 took 0.335s\n",
      "0.0012092426186427474\n",
      "training loss:\t\t0.001209\n",
      "Epoch 223 of 500 took 0.342s\n",
      "0.0012092231772840023\n",
      "training loss:\t\t0.001209\n",
      "Epoch 224 of 500 took 0.335s\n",
      "0.0012092040851712227\n",
      "training loss:\t\t0.001209\n",
      "Epoch 225 of 500 took 0.331s\n",
      "0.0012091841781511903\n",
      "training loss:\t\t0.001209\n",
      "Epoch 226 of 500 took 0.343s\n",
      "0.001209164853207767\n",
      "training loss:\t\t0.001209\n",
      "Epoch 227 of 500 took 0.333s\n",
      "0.001209145411849022\n",
      "training loss:\t\t0.001209\n",
      "Epoch 228 of 500 took 0.353s\n",
      "0.0012091256212443113\n",
      "training loss:\t\t0.001209\n",
      "Epoch 229 of 500 took 0.338s\n",
      "0.0012091067619621754\n",
      "training loss:\t\t0.001209\n",
      "Epoch 230 of 500 took 0.353s\n",
      "0.0012090866221114993\n",
      "training loss:\t\t0.001209\n",
      "Epoch 231 of 500 took 0.340s\n",
      "0.0012090675299987197\n",
      "training loss:\t\t0.001209\n",
      "Epoch 232 of 500 took 0.334s\n",
      "0.0012090489035472274\n",
      "training loss:\t\t0.001209\n",
      "Epoch 233 of 500 took 0.371s\n",
      "0.0012090291129425168\n",
      "training loss:\t\t0.001209\n",
      "Epoch 234 of 500 took 0.369s\n",
      "0.0012090097879990935\n",
      "training loss:\t\t0.001209\n",
      "Epoch 235 of 500 took 0.362s\n",
      "0.0012089910451322794\n",
      "training loss:\t\t0.001209\n",
      "Epoch 236 of 500 took 0.336s\n",
      "0.001208971836604178\n",
      "training loss:\t\t0.001209\n",
      "Epoch 237 of 500 took 0.330s\n",
      "0.0012089528609067202\n",
      "training loss:\t\t0.001209\n",
      "Epoch 238 of 500 took 0.333s\n",
      "0.0012089333031326532\n",
      "training loss:\t\t0.001209\n",
      "Epoch 239 of 500 took 0.332s\n",
      "0.0012089144438505173\n",
      "training loss:\t\t0.001209\n",
      "Epoch 240 of 500 took 0.365s\n",
      "0.0012088957009837031\n",
      "training loss:\t\t0.001209\n",
      "Epoch 241 of 500 took 0.342s\n",
      "0.0012088766088709235\n",
      "training loss:\t\t0.001209\n",
      "Epoch 242 of 500 took 0.330s\n",
      "0.0012088577495887876\n",
      "training loss:\t\t0.001209\n",
      "Epoch 243 of 500 took 0.330s\n",
      "0.0012088390067219734\n",
      "training loss:\t\t0.001209\n",
      "Epoch 244 of 500 took 0.330s\n",
      "0.0012088201474398375\n",
      "training loss:\t\t0.001209\n",
      "Epoch 245 of 500 took 0.360s\n",
      "0.0012088010553270578\n",
      "training loss:\t\t0.001209\n",
      "Epoch 246 of 500 took 0.333s\n",
      "0.0012087821960449219\n",
      "training loss:\t\t0.001209\n",
      "Epoch 247 of 500 took 0.330s\n",
      "0.0012087635695934296\n",
      "training loss:\t\t0.001209\n",
      "Epoch 248 of 500 took 0.337s\n",
      "0.001208745175972581\n",
      "training loss:\t\t0.001209\n",
      "Epoch 249 of 500 took 0.353s\n",
      "0.0012087264331057668\n",
      "training loss:\t\t0.001209\n",
      "Epoch 250 of 500 took 0.330s\n",
      "0.0012087082723155618\n",
      "training loss:\t\t0.001209\n",
      "Epoch 251 of 500 took 0.330s\n",
      "0.001208689296618104\n",
      "training loss:\t\t0.001209\n",
      "Epoch 252 of 500 took 0.332s\n",
      "0.0012086712522432208\n",
      "training loss:\t\t0.001209\n",
      "Epoch 253 of 500 took 0.359s\n",
      "0.0012086525093764067\n",
      "training loss:\t\t0.001209\n",
      "Epoch 254 of 500 took 0.333s\n",
      "0.0012086343485862017\n",
      "training loss:\t\t0.001209\n",
      "Epoch 255 of 500 took 0.342s\n",
      "0.0012086153728887439\n",
      "training loss:\t\t0.001209\n",
      "Epoch 256 of 500 took 0.357s\n",
      "0.001208597095683217\n",
      "training loss:\t\t0.001209\n",
      "Epoch 257 of 500 took 0.359s\n",
      "0.0012085785856470466\n",
      "training loss:\t\t0.001209\n",
      "Epoch 258 of 500 took 0.343s\n",
      "0.0012085611233487725\n",
      "training loss:\t\t0.001209\n",
      "Epoch 259 of 500 took 0.333s\n",
      "0.0012085424968972802\n",
      "training loss:\t\t0.001209\n",
      "Epoch 260 of 500 took 0.331s\n",
      "0.0012085241032764316\n",
      "training loss:\t\t0.001209\n",
      "Epoch 261 of 500 took 0.330s\n",
      "0.0012085058260709047\n",
      "training loss:\t\t0.001209\n",
      "Epoch 262 of 500 took 0.329s\n",
      "0.0012084880145266652\n",
      "training loss:\t\t0.001208\n",
      "Epoch 263 of 500 took 0.330s\n",
      "0.0012084707850590348\n",
      "training loss:\t\t0.001208\n",
      "Epoch 264 of 500 took 0.330s\n",
      "0.001208451809361577\n",
      "training loss:\t\t0.001208\n",
      "Epoch 265 of 500 took 0.330s\n",
      "0.0012084339978173375\n",
      "training loss:\t\t0.001208\n",
      "Epoch 266 of 500 took 0.330s\n",
      "0.0012084165355190635\n",
      "training loss:\t\t0.001208\n",
      "Epoch 267 of 500 took 0.368s\n",
      "0.0012083982583135366\n",
      "training loss:\t\t0.001208\n",
      "Epoch 268 of 500 took 0.353s\n",
      "0.0012083804467692971\n",
      "training loss:\t\t0.001208\n",
      "Epoch 269 of 500 took 0.350s\n",
      "0.0012083625188097358\n",
      "training loss:\t\t0.001208\n",
      "Epoch 270 of 500 took 0.330s\n",
      "0.0012083459878340364\n",
      "training loss:\t\t0.001208\n",
      "Epoch 271 of 500 took 0.330s\n",
      "0.0012083272449672222\n",
      "training loss:\t\t0.001208\n",
      "Epoch 272 of 500 took 0.331s\n",
      "0.0012083095498383045\n",
      "training loss:\t\t0.001208\n",
      "Epoch 273 of 500 took 0.329s\n",
      "0.0012082916218787432\n",
      "training loss:\t\t0.001208\n",
      "Epoch 274 of 500 took 0.333s\n",
      "0.00120827357750386\n",
      "training loss:\t\t0.001208\n",
      "Epoch 275 of 500 took 0.333s\n",
      "0.0012082556495442986\n",
      "training loss:\t\t0.001208\n",
      "Epoch 276 of 500 took 0.348s\n",
      "0.0012082381872460246\n",
      "training loss:\t\t0.001208\n",
      "Epoch 277 of 500 took 0.362s\n",
      "0.0012082209577783942\n",
      "training loss:\t\t0.001208\n",
      "Epoch 278 of 500 took 0.330s\n",
      "0.001208204310387373\n",
      "training loss:\t\t0.001208\n",
      "Epoch 279 of 500 took 0.330s\n",
      "0.001208186731673777\n",
      "training loss:\t\t0.001208\n",
      "Epoch 280 of 500 took 0.330s\n",
      "0.0012081689201295376\n",
      "training loss:\t\t0.001208\n",
      "Epoch 281 of 500 took 0.333s\n",
      "0.0012081509921699762\n",
      "training loss:\t\t0.001208\n",
      "Epoch 282 of 500 took 0.333s\n",
      "0.0012081341119483113\n",
      "training loss:\t\t0.001208\n",
      "Epoch 283 of 500 took 0.332s\n",
      "0.001208117580972612\n",
      "training loss:\t\t0.001208\n",
      "Epoch 284 of 500 took 0.334s\n",
      "0.0012080988381057978\n",
      "training loss:\t\t0.001208\n",
      "Epoch 285 of 500 took 0.333s\n",
      "0.0012080816086381674\n",
      "training loss:\t\t0.001208\n",
      "Epoch 286 of 500 took 0.333s\n",
      "0.0012080640299245715\n",
      "training loss:\t\t0.001208\n",
      "Epoch 287 of 500 took 0.334s\n",
      "0.0012080463347956538\n",
      "training loss:\t\t0.001208\n",
      "Epoch 288 of 500 took 0.334s\n",
      "0.0012080310843884945\n",
      "training loss:\t\t0.001208\n",
      "Epoch 289 of 500 took 0.333s\n",
      "0.001208013272844255\n",
      "training loss:\t\t0.001208\n",
      "Epoch 290 of 500 took 0.333s\n",
      "0.0012079955777153373\n",
      "training loss:\t\t0.001208\n",
      "Epoch 291 of 500 took 0.332s\n",
      "0.0012079784646630287\n",
      "training loss:\t\t0.001208\n",
      "Epoch 292 of 500 took 0.332s\n",
      "0.0012079615844413638\n",
      "training loss:\t\t0.001208\n",
      "Epoch 293 of 500 took 0.331s\n",
      "0.0012079450534656644\n",
      "training loss:\t\t0.001208\n",
      "Epoch 294 of 500 took 0.330s\n",
      "0.001207927125506103\n",
      "training loss:\t\t0.001208\n",
      "Epoch 295 of 500 took 0.330s\n",
      "0.0012079102452844381\n",
      "training loss:\t\t0.001208\n",
      "Epoch 296 of 500 took 0.329s\n",
      "0.001207892782986164\n",
      "training loss:\t\t0.001208\n",
      "Epoch 297 of 500 took 0.330s\n",
      "0.0012078770669177175\n",
      "training loss:\t\t0.001208\n",
      "Epoch 298 of 500 took 0.330s\n",
      "0.001207859953865409\n",
      "training loss:\t\t0.001208\n",
      "Epoch 299 of 500 took 0.330s\n",
      "0.001207842375151813\n",
      "training loss:\t\t0.001208\n",
      "Epoch 300 of 500 took 0.330s\n",
      "0.0012078253785148263\n",
      "training loss:\t\t0.001208\n",
      "Epoch 301 of 500 took 0.330s\n",
      "0.0012078088475391269\n",
      "training loss:\t\t0.001208\n",
      "Epoch 302 of 500 took 0.331s\n",
      "0.0012077924329787493\n",
      "training loss:\t\t0.001208\n",
      "Epoch 303 of 500 took 0.331s\n",
      "0.0012077752035111189\n",
      "training loss:\t\t0.001208\n",
      "Epoch 304 of 500 took 0.333s\n",
      "0.0012077577412128448\n",
      "training loss:\t\t0.001208\n",
      "Epoch 305 of 500 took 0.330s\n",
      "0.0012077412102371454\n",
      "training loss:\t\t0.001208\n",
      "Epoch 306 of 500 took 0.331s\n",
      "0.0012077258434146643\n",
      "training loss:\t\t0.001208\n",
      "Epoch 307 of 500 took 0.330s\n",
      "0.0012077083811163902\n",
      "training loss:\t\t0.001208\n",
      "Epoch 308 of 500 took 0.330s\n",
      "0.0012076916173100471\n",
      "training loss:\t\t0.001208\n",
      "Epoch 309 of 500 took 0.329s\n",
      "0.0012076746206730604\n",
      "training loss:\t\t0.001208\n",
      "Epoch 310 of 500 took 0.330s\n",
      "0.0012076585553586483\n",
      "training loss:\t\t0.001208\n",
      "Epoch 311 of 500 took 0.330s\n",
      "0.0012076429557055235\n",
      "training loss:\t\t0.001208\n",
      "Epoch 312 of 500 took 0.331s\n",
      "0.0012076256098225713\n",
      "training loss:\t\t0.001208\n",
      "Epoch 313 of 500 took 0.329s\n",
      "0.0012076087296009064\n",
      "training loss:\t\t0.001208\n",
      "Epoch 314 of 500 took 0.329s\n",
      "0.001207592198625207\n",
      "training loss:\t\t0.001208\n",
      "Epoch 315 of 500 took 0.332s\n",
      "0.0012075759004801512\n",
      "training loss:\t\t0.001208\n",
      "Epoch 316 of 500 took 0.332s\n",
      "0.0012075596023350954\n",
      "training loss:\t\t0.001208\n",
      "Epoch 317 of 500 took 0.333s\n",
      "0.0012075433041900396\n",
      "training loss:\t\t0.001208\n",
      "Epoch 318 of 500 took 0.350s\n",
      "0.0012075266567990184\n",
      "training loss:\t\t0.001208\n",
      "Epoch 319 of 500 took 0.356s\n",
      "0.0012075110571458936\n",
      "training loss:\t\t0.001208\n",
      "Epoch 320 of 500 took 0.349s\n",
      "0.0012074949918314815\n",
      "training loss:\t\t0.001207\n",
      "Epoch 321 of 500 took 0.351s\n",
      "0.001207478460855782\n",
      "training loss:\t\t0.001207\n",
      "Epoch 322 of 500 took 0.362s\n",
      "0.0012074619298800826\n",
      "training loss:\t\t0.001207\n",
      "Epoch 323 of 500 took 0.352s\n",
      "0.0012074453989043832\n",
      "training loss:\t\t0.001207\n",
      "Epoch 324 of 500 took 0.346s\n",
      "0.0012074299156665802\n",
      "training loss:\t\t0.001207\n",
      "Epoch 325 of 500 took 0.356s\n",
      "0.0012074141995981336\n",
      "training loss:\t\t0.001207\n",
      "Epoch 326 of 500 took 0.339s\n",
      "0.001207397086545825\n",
      "training loss:\t\t0.001207\n",
      "Epoch 327 of 500 took 0.335s\n",
      "0.00120738020632416\n",
      "training loss:\t\t0.001207\n",
      "Epoch 328 of 500 took 0.349s\n",
      "0.001207364839501679\n",
      "training loss:\t\t0.001207\n",
      "Epoch 329 of 500 took 0.358s\n",
      "0.0012073492398485541\n",
      "training loss:\t\t0.001207\n",
      "Epoch 330 of 500 took 0.334s\n",
      "0.0012073335237801075\n",
      "training loss:\t\t0.001207\n",
      "Epoch 331 of 500 took 0.336s\n",
      "0.0012073165271431208\n",
      "training loss:\t\t0.001207\n",
      "Epoch 332 of 500 took 0.333s\n",
      "0.001207300228998065\n",
      "training loss:\t\t0.001207\n",
      "Epoch 333 of 500 took 0.333s\n",
      "0.0012072848621755838\n",
      "training loss:\t\t0.001207\n",
      "Epoch 334 of 500 took 0.333s\n",
      "0.0012072694953531027\n",
      "training loss:\t\t0.001207\n",
      "Epoch 335 of 500 took 0.332s\n",
      "0.0012072529643774033\n",
      "training loss:\t\t0.001207\n",
      "Epoch 336 of 500 took 0.336s\n",
      "0.001207237015478313\n",
      "training loss:\t\t0.001207\n",
      "Epoch 337 of 500 took 0.332s\n",
      "0.0012072217650711536\n",
      "training loss:\t\t0.001207\n",
      "Epoch 338 of 500 took 0.336s\n",
      "0.0012072061654180288\n",
      "training loss:\t\t0.001207\n",
      "Epoch 339 of 500 took 0.333s\n",
      "0.0012071906821802258\n",
      "training loss:\t\t0.001207\n",
      "Epoch 340 of 500 took 0.332s\n",
      "0.0012071734527125955\n",
      "training loss:\t\t0.001207\n",
      "Epoch 341 of 500 took 0.332s\n",
      "0.0012071578530594707\n",
      "training loss:\t\t0.001207\n",
      "Epoch 342 of 500 took 0.333s\n",
      "0.001207142835482955\n",
      "training loss:\t\t0.001207\n",
      "Epoch 343 of 500 took 0.332s\n",
      "0.0012071274686604738\n",
      "training loss:\t\t0.001207\n",
      "Epoch 344 of 500 took 0.334s\n",
      "0.001207110588438809\n",
      "training loss:\t\t0.001207\n",
      "Epoch 345 of 500 took 0.333s\n",
      "0.001207095105201006\n",
      "training loss:\t\t0.001207\n",
      "Epoch 346 of 500 took 0.335s\n",
      "0.001207080204039812\n",
      "training loss:\t\t0.001207\n",
      "Epoch 347 of 500 took 0.332s\n",
      "0.0012070641387254\n",
      "training loss:\t\t0.001207\n",
      "Epoch 348 of 500 took 0.335s\n",
      "0.001207047956995666\n",
      "training loss:\t\t0.001207\n",
      "Epoch 349 of 500 took 0.334s\n",
      "0.001207032473757863\n",
      "training loss:\t\t0.001207\n",
      "Epoch 350 of 500 took 0.332s\n",
      "0.0012070171069353819\n",
      "training loss:\t\t0.001207\n",
      "Epoch 351 of 500 took 0.333s\n",
      "0.0012070018565282226\n",
      "training loss:\t\t0.001207\n",
      "Epoch 352 of 500 took 0.333s\n",
      "0.0012069857912138104\n",
      "training loss:\t\t0.001207\n",
      "Epoch 353 of 500 took 0.335s\n",
      "0.0012069707736372948\n",
      "training loss:\t\t0.001207\n",
      "Epoch 354 of 500 took 0.333s\n",
      "0.0012069559888914227\n",
      "training loss:\t\t0.001207\n",
      "Epoch 355 of 500 took 0.334s\n",
      "0.0012069398071616888\n",
      "training loss:\t\t0.001207\n",
      "Epoch 356 of 500 took 0.332s\n",
      "0.0012069244403392076\n",
      "training loss:\t\t0.001207\n",
      "Epoch 357 of 500 took 0.334s\n",
      "0.0012069088406860828\n",
      "training loss:\t\t0.001207\n",
      "Epoch 358 of 500 took 0.333s\n",
      "0.001206893939524889\n",
      "training loss:\t\t0.001207\n",
      "Epoch 359 of 500 took 0.332s\n",
      "0.0012068776413798332\n",
      "training loss:\t\t0.001207\n",
      "Epoch 360 of 500 took 0.332s\n",
      "0.0012068625073879957\n",
      "training loss:\t\t0.001207\n",
      "Epoch 361 of 500 took 0.334s\n",
      "0.0012068478390574455\n",
      "training loss:\t\t0.001207\n",
      "Epoch 362 of 500 took 0.334s\n",
      "0.0012068329378962517\n",
      "training loss:\t\t0.001207\n",
      "Epoch 363 of 500 took 0.334s\n",
      "0.001206817221827805\n",
      "training loss:\t\t0.001207\n",
      "Epoch 364 of 500 took 0.334s\n",
      "0.001206801738590002\n",
      "training loss:\t\t0.001207\n",
      "Epoch 365 of 500 took 0.332s\n",
      "0.0012067867210134864\n",
      "training loss:\t\t0.001207\n",
      "Epoch 366 of 500 took 0.334s\n",
      "0.001206770888529718\n",
      "training loss:\t\t0.001207\n",
      "Epoch 367 of 500 took 0.334s\n",
      "0.0012067557545378804\n",
      "training loss:\t\t0.001207\n",
      "Epoch 368 of 500 took 0.332s\n",
      "0.0012067414354532957\n",
      "training loss:\t\t0.001207\n",
      "Epoch 369 of 500 took 0.333s\n",
      "0.0012067261850461364\n",
      "training loss:\t\t0.001207\n",
      "Epoch 370 of 500 took 0.334s\n",
      "0.0012067107018083334\n",
      "training loss:\t\t0.001207\n",
      "Epoch 371 of 500 took 0.332s\n",
      "0.0012066964991390705\n",
      "training loss:\t\t0.001207\n",
      "Epoch 372 of 500 took 0.334s\n",
      "0.0012066804338246584\n",
      "training loss:\t\t0.001207\n",
      "Epoch 373 of 500 took 0.334s\n",
      "0.001206665299832821\n",
      "training loss:\t\t0.001207\n",
      "Epoch 374 of 500 took 0.332s\n",
      "0.0012066514464095235\n",
      "training loss:\t\t0.001207\n",
      "Epoch 375 of 500 took 0.335s\n",
      "0.0012066352646797895\n",
      "training loss:\t\t0.001207\n",
      "Epoch 376 of 500 took 0.332s\n",
      "0.0012066207127645612\n",
      "training loss:\t\t0.001207\n",
      "Epoch 377 of 500 took 0.332s\n",
      "0.0012066068593412638\n",
      "training loss:\t\t0.001207\n",
      "Epoch 378 of 500 took 0.333s\n",
      "0.001206591841764748\n",
      "training loss:\t\t0.001207\n",
      "Epoch 379 of 500 took 0.334s\n",
      "0.0012065761256963015\n",
      "training loss:\t\t0.001207\n",
      "Epoch 380 of 500 took 0.334s\n",
      "0.0012065604096278548\n",
      "training loss:\t\t0.001207\n",
      "Epoch 381 of 500 took 0.333s\n",
      "0.0012065473711118102\n",
      "training loss:\t\t0.001207\n",
      "Epoch 382 of 500 took 0.334s\n",
      "0.0012065328191965818\n",
      "training loss:\t\t0.001207\n",
      "Epoch 383 of 500 took 0.333s\n",
      "0.0012065164046362042\n",
      "training loss:\t\t0.001207\n",
      "Epoch 384 of 500 took 0.343s\n",
      "0.0012065012706443667\n",
      "training loss:\t\t0.001207\n",
      "Epoch 385 of 500 took 0.357s\n",
      "0.0012064873008057475\n",
      "training loss:\t\t0.001206\n",
      "Epoch 386 of 500 took 0.334s\n",
      "0.0012064725160598755\n",
      "training loss:\t\t0.001206\n",
      "Epoch 387 of 500 took 0.333s\n",
      "0.0012064570328220725\n",
      "training loss:\t\t0.001206\n",
      "Epoch 388 of 500 took 0.338s\n",
      "0.001206443877890706\n",
      "training loss:\t\t0.001206\n",
      "Epoch 389 of 500 took 0.352s\n",
      "0.0012064286274835467\n",
      "training loss:\t\t0.001206\n",
      "Epoch 390 of 500 took 0.333s\n",
      "0.0012064132606610656\n",
      "training loss:\t\t0.001206\n",
      "Epoch 391 of 500 took 0.343s\n",
      "0.0012063992908224463\n",
      "training loss:\t\t0.001206\n",
      "Epoch 392 of 500 took 0.355s\n",
      "0.0012063848553225398\n",
      "training loss:\t\t0.001206\n",
      "Epoch 393 of 500 took 0.353s\n",
      "0.0012063694885000587\n",
      "training loss:\t\t0.001206\n",
      "Epoch 394 of 500 took 0.356s\n",
      "0.0012063548201695085\n",
      "training loss:\t\t0.001206\n",
      "Epoch 395 of 500 took 0.361s\n",
      "0.0012063408503308892\n",
      "training loss:\t\t0.001206\n",
      "Epoch 396 of 500 took 0.358s\n",
      "0.0012063262984156609\n",
      "training loss:\t\t0.001206\n",
      "Epoch 397 of 500 took 0.357s\n",
      "0.001206311397254467\n",
      "training loss:\t\t0.001206\n",
      "Epoch 398 of 500 took 0.345s\n",
      "0.0012062984751537442\n",
      "training loss:\t\t0.001206\n",
      "Epoch 399 of 500 took 0.355s\n",
      "0.0012062834575772285\n",
      "training loss:\t\t0.001206\n",
      "Epoch 400 of 500 took 0.358s\n",
      "0.0012062680907547474\n",
      "training loss:\t\t0.001206\n",
      "Epoch 401 of 500 took 0.375s\n",
      "0.0012062533060088754\n",
      "training loss:\t\t0.001206\n",
      "Epoch 402 of 500 took 0.380s\n",
      "0.0012062402674928308\n",
      "training loss:\t\t0.001206\n",
      "Epoch 403 of 500 took 0.361s\n",
      "0.001206225948408246\n",
      "training loss:\t\t0.001206\n",
      "Epoch 404 of 500 took 0.330s\n",
      "0.0012062106980010867\n",
      "training loss:\t\t0.001206\n",
      "Epoch 405 of 500 took 0.333s\n",
      "0.001206196378916502\n",
      "training loss:\t\t0.001206\n",
      "Epoch 406 of 500 took 0.332s\n",
      "0.0012061821762472391\n",
      "training loss:\t\t0.001206\n",
      "Epoch 407 of 500 took 0.331s\n",
      "0.0012061677407473326\n",
      "training loss:\t\t0.001206\n",
      "Epoch 408 of 500 took 0.330s\n",
      "0.001206152606755495\n",
      "training loss:\t\t0.001206\n",
      "Epoch 409 of 500 took 0.330s\n",
      "0.0012061387533321977\n",
      "training loss:\t\t0.001206\n",
      "Epoch 410 of 500 took 0.330s\n",
      "0.0012061245506629348\n",
      "training loss:\t\t0.001206\n",
      "Epoch 411 of 500 took 0.330s\n",
      "0.0012061106972396374\n",
      "training loss:\t\t0.001206\n",
      "Epoch 412 of 500 took 0.333s\n",
      "0.0012060962617397308\n",
      "training loss:\t\t0.001206\n",
      "Epoch 413 of 500 took 0.332s\n",
      "0.001206082641147077\n",
      "training loss:\t\t0.001206\n",
      "Epoch 414 of 500 took 0.332s\n",
      "0.0012060677399858832\n",
      "training loss:\t\t0.001206\n",
      "Epoch 415 of 500 took 0.332s\n",
      "0.0012060534209012985\n",
      "training loss:\t\t0.001206\n",
      "Epoch 416 of 500 took 0.334s\n",
      "0.0012060394510626793\n",
      "training loss:\t\t0.001206\n",
      "Epoch 417 of 500 took 0.333s\n",
      "0.0012060264125466347\n",
      "training loss:\t\t0.001206\n",
      "Epoch 418 of 500 took 0.333s\n",
      "0.0012060112785547972\n",
      "training loss:\t\t0.001206\n",
      "Epoch 419 of 500 took 0.332s\n",
      "0.0012059967266395688\n",
      "training loss:\t\t0.001206\n",
      "Epoch 420 of 500 took 0.330s\n",
      "0.0012059835717082024\n",
      "training loss:\t\t0.001206\n",
      "Epoch 421 of 500 took 0.331s\n",
      "0.0012059687869623303\n",
      "training loss:\t\t0.001206\n",
      "Epoch 422 of 500 took 0.330s\n",
      "0.0012059547007083893\n",
      "training loss:\t\t0.001206\n",
      "Epoch 423 of 500 took 0.330s\n",
      "0.0012059423606842756\n",
      "training loss:\t\t0.001206\n",
      "Epoch 424 of 500 took 0.330s\n",
      "0.0012059275759384036\n",
      "training loss:\t\t0.001206\n",
      "Epoch 425 of 500 took 0.330s\n",
      "0.0012059130240231752\n",
      "training loss:\t\t0.001206\n",
      "Epoch 426 of 500 took 0.330s\n",
      "0.0012058989377692342\n",
      "training loss:\t\t0.001206\n",
      "Epoch 427 of 500 took 0.330s\n",
      "0.0012058860156685114\n",
      "training loss:\t\t0.001206\n",
      "Epoch 428 of 500 took 0.330s\n",
      "0.0012058720458298922\n",
      "training loss:\t\t0.001206\n",
      "Epoch 429 of 500 took 0.332s\n",
      "0.0012058574939146638\n",
      "training loss:\t\t0.001206\n",
      "Epoch 430 of 500 took 0.333s\n",
      "0.0012058429419994354\n",
      "training loss:\t\t0.001206\n",
      "Epoch 431 of 500 took 0.333s\n",
      "0.0012058302527293563\n",
      "training loss:\t\t0.001206\n",
      "Epoch 432 of 500 took 0.333s\n",
      "0.0012058165157213807\n",
      "training loss:\t\t0.001206\n",
      "Epoch 433 of 500 took 0.333s\n",
      "0.0012058018473908305\n",
      "training loss:\t\t0.001206\n",
      "Epoch 434 of 500 took 0.333s\n",
      "0.001205787411890924\n",
      "training loss:\t\t0.001206\n",
      "Epoch 435 of 500 took 0.332s\n",
      "0.0012057742569595575\n",
      "training loss:\t\t0.001206\n",
      "Epoch 436 of 500 took 0.332s\n",
      "0.0012057608691975474\n",
      "training loss:\t\t0.001206\n",
      "Epoch 437 of 500 took 0.330s\n",
      "0.0012057462008669972\n",
      "training loss:\t\t0.001206\n",
      "Epoch 438 of 500 took 0.331s\n",
      "0.0012057333951815963\n",
      "training loss:\t\t0.001206\n",
      "Epoch 439 of 500 took 0.330s\n",
      "0.0012057197745889425\n",
      "training loss:\t\t0.001206\n",
      "Epoch 440 of 500 took 0.330s\n",
      "0.0012057049898430705\n",
      "training loss:\t\t0.001206\n",
      "Epoch 441 of 500 took 0.330s\n",
      "0.0012056926498189569\n",
      "training loss:\t\t0.001206\n",
      "Epoch 442 of 500 took 0.330s\n",
      "0.0012056783307343721\n",
      "training loss:\t\t0.001206\n",
      "Epoch 443 of 500 took 0.330s\n",
      "0.0012056648265570402\n",
      "training loss:\t\t0.001206\n",
      "Epoch 444 of 500 took 0.330s\n",
      "0.0012056505074724555\n",
      "training loss:\t\t0.001206\n",
      "Epoch 445 of 500 took 0.330s\n",
      "0.0012056382838636637\n",
      "training loss:\t\t0.001206\n",
      "Epoch 446 of 500 took 0.330s\n",
      "0.00120562466327101\n",
      "training loss:\t\t0.001206\n",
      "Epoch 447 of 500 took 0.331s\n",
      "0.0012056106934323907\n",
      "training loss:\t\t0.001206\n",
      "Epoch 448 of 500 took 0.330s\n",
      "0.0012055960251018405\n",
      "training loss:\t\t0.001206\n",
      "Epoch 449 of 500 took 0.330s\n",
      "0.0012055826373398304\n",
      "training loss:\t\t0.001206\n",
      "Epoch 450 of 500 took 0.330s\n",
      "0.001205570180900395\n",
      "training loss:\t\t0.001206\n",
      "Epoch 451 of 500 took 0.330s\n",
      "0.0012055557454004884\n",
      "training loss:\t\t0.001206\n",
      "Epoch 452 of 500 took 0.330s\n",
      "0.0012055424740538\n",
      "training loss:\t\t0.001206\n",
      "Epoch 453 of 500 took 0.331s\n",
      "0.0012055295519530773\n",
      "training loss:\t\t0.001206\n",
      "Epoch 454 of 500 took 0.331s\n",
      "0.0012055159313604236\n",
      "training loss:\t\t0.001206\n",
      "Epoch 455 of 500 took 0.330s\n",
      "0.0012055023107677698\n",
      "training loss:\t\t0.001206\n",
      "Epoch 456 of 500 took 0.330s\n",
      "0.0012054890394210815\n",
      "training loss:\t\t0.001205\n",
      "Epoch 457 of 500 took 0.330s\n",
      "0.0012054749531671405\n",
      "training loss:\t\t0.001205\n",
      "Epoch 458 of 500 took 0.333s\n",
      "0.0012054619146510959\n",
      "training loss:\t\t0.001205\n",
      "Epoch 459 of 500 took 0.333s\n",
      "0.0012054493417963386\n",
      "training loss:\t\t0.001205\n",
      "Epoch 460 of 500 took 0.331s\n",
      "0.0012054353719577193\n",
      "training loss:\t\t0.001205\n",
      "Epoch 461 of 500 took 0.333s\n",
      "0.0012054219841957092\n",
      "training loss:\t\t0.001205\n",
      "Epoch 462 of 500 took 0.331s\n",
      "0.0012054095277562737\n",
      "training loss:\t\t0.001205\n",
      "Epoch 463 of 500 took 0.329s\n",
      "0.0012053949758410454\n",
      "training loss:\t\t0.001205\n",
      "Epoch 464 of 500 took 0.330s\n",
      "0.0012053821701556444\n",
      "training loss:\t\t0.001205\n",
      "Epoch 465 of 500 took 0.330s\n",
      "0.001205369597300887\n",
      "training loss:\t\t0.001205\n",
      "Epoch 466 of 500 took 0.331s\n",
      "0.0012053560931235552\n",
      "training loss:\t\t0.001205\n",
      "Epoch 467 of 500 took 0.334s\n",
      "0.0012053423561155796\n",
      "training loss:\t\t0.001205\n",
      "Epoch 468 of 500 took 0.330s\n",
      "0.001205329317599535\n",
      "training loss:\t\t0.001205\n",
      "Epoch 469 of 500 took 0.330s\n",
      "0.001205315813422203\n",
      "training loss:\t\t0.001205\n",
      "Epoch 470 of 500 took 0.332s\n",
      "0.0012053035898134112\n",
      "training loss:\t\t0.001205\n",
      "Epoch 471 of 500 took 0.330s\n",
      "0.0012052893871441483\n",
      "training loss:\t\t0.001205\n",
      "Epoch 472 of 500 took 0.330s\n",
      "0.0012052766978740692\n",
      "training loss:\t\t0.001205\n",
      "Epoch 473 of 500 took 0.331s\n",
      "0.0012052642414346337\n",
      "training loss:\t\t0.001205\n",
      "Epoch 474 of 500 took 0.331s\n",
      "0.00120525062084198\n",
      "training loss:\t\t0.001205\n",
      "Epoch 475 of 500 took 0.332s\n",
      "0.001205237116664648\n",
      "training loss:\t\t0.001205\n",
      "Epoch 476 of 500 took 0.332s\n",
      "0.0012052245438098907\n",
      "training loss:\t\t0.001205\n",
      "Epoch 477 of 500 took 0.332s\n",
      "0.001205210923217237\n",
      "training loss:\t\t0.001205\n",
      "Epoch 478 of 500 took 0.330s\n",
      "0.0012051980011165142\n",
      "training loss:\t\t0.001205\n",
      "Epoch 479 of 500 took 0.329s\n",
      "0.0012051864759996533\n",
      "training loss:\t\t0.001205\n",
      "Epoch 480 of 500 took 0.330s\n",
      "0.0012051729718223214\n",
      "training loss:\t\t0.001205\n",
      "Epoch 481 of 500 took 0.331s\n",
      "0.0012051592348143458\n",
      "training loss:\t\t0.001205\n",
      "Epoch 482 of 500 took 0.332s\n",
      "0.001205145730637014\n",
      "training loss:\t\t0.001205\n",
      "Epoch 483 of 500 took 0.331s\n",
      "0.0012051328085362911\n",
      "training loss:\t\t0.001205\n",
      "Epoch 484 of 500 took 0.330s\n",
      "0.001205120119266212\n",
      "training loss:\t\t0.001205\n",
      "Epoch 485 of 500 took 0.330s\n",
      "0.0012051075464114547\n",
      "training loss:\t\t0.001205\n",
      "Epoch 486 of 500 took 0.330s\n",
      "0.0012050942750647664\n",
      "training loss:\t\t0.001205\n",
      "Epoch 487 of 500 took 0.332s\n",
      "0.0012050825171172619\n",
      "training loss:\t\t0.001205\n",
      "Epoch 488 of 500 took 0.333s\n",
      "0.00120506901293993\n",
      "training loss:\t\t0.001205\n",
      "Epoch 489 of 500 took 0.335s\n",
      "0.0012050556251779199\n",
      "training loss:\t\t0.001205\n",
      "Epoch 490 of 500 took 0.335s\n",
      "0.0012050435179844499\n",
      "training loss:\t\t0.001205\n",
      "Epoch 491 of 500 took 0.358s\n",
      "0.0012050303630530834\n",
      "training loss:\t\t0.001205\n",
      "Epoch 492 of 500 took 0.363s\n",
      "0.0012050168588757515\n",
      "training loss:\t\t0.001205\n",
      "Epoch 493 of 500 took 0.361s\n",
      "0.0012050049845129251\n",
      "training loss:\t\t0.001205\n",
      "Epoch 494 of 500 took 0.339s\n",
      "0.0012049924116581678\n",
      "training loss:\t\t0.001205\n",
      "Epoch 495 of 500 took 0.334s\n",
      "0.0012049786746501923\n",
      "training loss:\t\t0.001205\n",
      "Epoch 496 of 500 took 0.370s\n",
      "0.0012049658689647913\n",
      "training loss:\t\t0.001205\n",
      "Epoch 497 of 500 took 0.382s\n",
      "0.0012049542274326086\n",
      "training loss:\t\t0.001205\n",
      "Epoch 498 of 500 took 0.366s\n",
      "0.0012049409560859203\n",
      "training loss:\t\t0.001205\n",
      "Epoch 499 of 500 took 0.331s\n",
      "0.0012049278011545539\n",
      "training loss:\t\t0.001205\n",
      "Epoch 500 of 500 took 0.361s\n",
      "0.0012049151118844748\n",
      "training loss:\t\t0.001205\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "    loss, params, learning_rate=0.0005, momentum=0.975)\n",
    "# updates = lasagne.updates.rmsprop(\n",
    "#     loss, params[10:12], learning_rate=0.01)\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates,on_unused_input='warn')\n",
    "print(\"Starting training...\")\n",
    "best_err = 0.02\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_err = train_fn(X, X_out)\n",
    "        # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(train_err)\n",
    "    print(\"training loss:\\t\\t{:.6f}\".format(float(train_err)))\n",
    "    if train_err < best_err:\n",
    "        best_err = train_err\n",
    "        np.savez('./output/autoencoder_mars.npz', *lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with np.load('./output/autoencoder_mars.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "lasagne.layers.set_all_param_values(network, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 50000\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "    loss, params, learning_rate=0.001, momentum=0.975)\n",
    "# updates = lasagne.updates.rmsprop(\n",
    "#     loss, params[10:12], learning_rate=0.01)\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates,on_unused_input='warn')\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_err = train_fn(X, X_out)\n",
    "        # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(train_err)\n",
    "    print(\"training loss:\\t\\t{:.6f}\".format(float(train_err)))\n",
    "    if train_err < best_err:\n",
    "        best_err = train_err\n",
    "        np.savez('./output/autoencoder_mars.npz', *lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
