{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convolutional autoencoder with MNIST\n",
    "###  inspired by the [Swarbrick's blog](https://swarbrickjones.wordpress.com/2015/04/29/convolutional-autoencoders-in-pythontheanolasagne/) and Professor [G.E.Hinton's paper](http://science.sciencemag.org/content/313/5786/504)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda_convnet (faster)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 750 Ti (CNMeM is disabled, CuDNN 3007)\n",
      "/home/rui/.pyenv/versions/3.5.0/envs/cae/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import os, sys, urllib, gzip\n",
    "sys.path.append('/home/rui/pylearn2')\n",
    "from __future__ import print_function\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "import numpy as np\n",
    "from lasagne.layers import get_output, InputLayer, DenseLayer, Upscale2DLayer, ReshapeLayer\n",
    "from lasagne.nonlinearities import rectify, leaky_rectify, tanh\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from lasagne.objectives import categorical_crossentropy\n",
    "import pylearn2\n",
    "from lasagne.layers.cuda_convnet import Conv2DCCLayer as Conv2DLayerFast\n",
    "from lasagne.regularization import regularize_layer_params, l2, l1\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import lasagne\n",
    "from lasagne.layers import Conv2DLayer as Conv2DLayerSlow\n",
    "from lasagne.layers import MaxPool2DLayer as MaxPool2DLayerSlow\n",
    "try:\n",
    "    from lasagne.layers.cuda_convnet import Conv2DCCLayer as Conv2DLayerFast\n",
    "    from lasagne.layers.cuda_convnet import MaxPool2DCCLayer as MaxPool2DLayerFast\n",
    "    print('Using cuda_convnet (faster)')\n",
    "except ImportError:\n",
    "    from lasagne.layers import Conv2DLayer as Conv2DLayerFast\n",
    "    from lasagne.layers import MaxPool2DLayer as MaxPool2DLayerFast\n",
    "    print('Using lasagne.layers (slower)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = gzip.open('/home/rui/Downloads/mnist.pkl.gz', 'rb')\n",
    "try:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "except:\n",
    "    train_set, valid_set, test_set = pickle.load(f)\n",
    "f.close()\n",
    "X, y = train_set\n",
    "X = np.reshape(X, (-1, 1, 28, 28))\n",
    "X_out = X.reshape((X.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_num_filters = 16\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "encode_size = 16\n",
    "dense_mid_size = 128\n",
    "pad_in = 'valid'    \n",
    "pad_out = 'full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn(input_var=None):\n",
    "    \n",
    "    network = InputLayer(shape=(None,  X.shape[1], X.shape[2], X.shape[3]),input_var=input_var)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "\n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "\n",
    "    network = MaxPool2DLayerFast(network, pool_size=pool_size)\n",
    "\n",
    "    network = Conv2DLayerFast(network, num_filters=2*conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "\n",
    "    network = MaxPool2DLayerFast(network, pool_size=pool_size)\n",
    "    \n",
    "    network = ReshapeLayer(network, shape =(([0], -1)))\n",
    "    \n",
    "    network = DenseLayer(network, num_units= dense_mid_size, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    encode_layer = DenseLayer(network, name= 'encode', num_units= encode_size, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    action_layer = DenseLayer(encode_layer, name= 'action', num_units= encode_size, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                            nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    network = DenseLayer(action_layer, num_units= 800, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    network = ReshapeLayer(network, shape =(([0], 2*conv_num_filters, 5, 5)))\n",
    "    \n",
    "    network = Upscale2DLayer(network, scale_factor = pool_size)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_out)\n",
    "\n",
    "    network = Upscale2DLayer(network, scale_factor = pool_size)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_out)\n",
    "\n",
    "    network = Conv2DLayerSlow(network, num_filters=1, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.sigmoid, filter_size=filter_size, pad=pad_out)\n",
    "    \n",
    "    network = ReshapeLayer(network, shape =(([0], -1)))\n",
    "\n",
    "    return network\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n"
     ]
    }
   ],
   "source": [
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.matrix('targets')\n",
    "learnrate=0.01\n",
    "# Create neural network model (depending on first command line parameter)\n",
    "network = build_cnn(input_var)\n",
    "\n",
    "with np.load('CAE_MNIST2.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "reconstructed = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.squared_error(reconstructed, target_var)\n",
    "loss = loss.mean()\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 30 took 15.382s\n",
      "  training loss:\t\t0.128702\n",
      "Epoch 2 of 30 took 15.357s\n",
      "  training loss:\t\t0.101073\n",
      "Epoch 3 of 30 took 15.367s\n",
      "  training loss:\t\t0.091356\n",
      "Epoch 4 of 30 took 15.369s\n",
      "  training loss:\t\t0.071927\n",
      "Epoch 5 of 30 took 15.387s\n",
      "  training loss:\t\t0.061472\n",
      "Epoch 6 of 30 took 15.395s\n",
      "  training loss:\t\t0.049544\n",
      "Epoch 7 of 30 took 15.378s\n",
      "  training loss:\t\t0.039866\n",
      "Epoch 8 of 30 took 15.408s\n",
      "  training loss:\t\t0.034431\n",
      "Epoch 9 of 30 took 15.377s\n",
      "  training loss:\t\t0.031566\n",
      "Epoch 10 of 30 took 15.383s\n",
      "  training loss:\t\t0.029721\n",
      "Epoch 11 of 30 took 15.384s\n",
      "  training loss:\t\t0.028369\n",
      "Epoch 12 of 30 took 15.538s\n",
      "  training loss:\t\t0.027291\n",
      "Epoch 13 of 30 took 15.483s\n",
      "  training loss:\t\t0.026373\n",
      "Epoch 14 of 30 took 15.508s\n",
      "  training loss:\t\t0.025548\n",
      "Epoch 15 of 30 took 15.545s\n",
      "  training loss:\t\t0.024828\n",
      "Epoch 16 of 30 took 15.418s\n",
      "  training loss:\t\t0.024241\n",
      "Epoch 17 of 30 took 15.876s\n",
      "  training loss:\t\t0.023776\n",
      "Epoch 18 of 30 took 15.692s\n",
      "  training loss:\t\t0.023396\n",
      "Epoch 19 of 30 took 15.386s\n",
      "  training loss:\t\t0.023070\n",
      "Epoch 20 of 30 took 15.385s\n",
      "  training loss:\t\t0.022783\n",
      "Epoch 21 of 30 took 15.404s\n",
      "  training loss:\t\t0.022524\n",
      "Epoch 22 of 30 took 15.669s\n",
      "  training loss:\t\t0.022288\n",
      "Epoch 23 of 30 took 15.744s\n",
      "  training loss:\t\t0.022071\n",
      "Epoch 24 of 30 took 15.521s\n",
      "  training loss:\t\t0.021870\n",
      "Epoch 25 of 30 took 15.429s\n",
      "  training loss:\t\t0.021682\n",
      "Epoch 26 of 30 took 15.475s\n",
      "  training loss:\t\t0.021505\n",
      "Epoch 27 of 30 took 15.724s\n",
      "  training loss:\t\t0.021339\n",
      "Epoch 28 of 30 took 15.955s\n",
      "  training loss:\t\t0.021181\n",
      "Epoch 29 of 30 took 15.756s\n",
      "  training loss:\t\t0.021031\n",
      "Epoch 30 of 30 took 15.655s\n",
      "  training loss:\t\t0.020887\n"
     ]
    }
   ],
   "source": [
    "# num_epochs = 30\n",
    "# input_var = T.tensor4('inputs')\n",
    "# target_var = T.matrix('targets')\n",
    "# learnrate=0.01\n",
    "# # Create neural network model (depending on first command line parameter)\n",
    "# print(\"Building model and compiling functions...\")\n",
    "# network, encode_layer = build_cnn(input_var)\n",
    "# l2_penalty = regularize_layer_params(network, l2)\n",
    "# l1_penalty = regularize_layer_params(network, l1)\n",
    "# reconstructed = lasagne.layers.get_output(network)\n",
    "# loss = lasagne.objectives.squared_error(reconstructed, target_var)\n",
    "# loss = loss.mean()\n",
    "# params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "# updates = lasagne.updates.nesterov_momentum(\n",
    "#     loss, params, learning_rate=learnrate, momentum=0.975)\n",
    "# train_fn = theano.function([input_var, target_var], loss, updates=updates,on_unused_input='warn')\n",
    "# print(\"Starting training...\")\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_err = 0\n",
    "#     train_batches = 0\n",
    "#     start_time = time.time()\n",
    "#     for batch in iterate_minibatches(X, X_out, 500, shuffle=False):\n",
    "#         inputs, targets = batch\n",
    "#         train_err += train_fn(inputs, targets)\n",
    "#         train_batches += 1\n",
    "\n",
    "#         # Then we print the results for this epoch:\n",
    "#     print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "#         epoch + 1, num_epochs, time.time() - start_time))\n",
    "#     print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "#     # Optionally, you could now dump the network weights to a file like this:\n",
    "# np.savez('CAE_MNIST2.npz', *lasagne.layers.get_all_param_values(network))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(num_epochs=10, learnrate=0.01):\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.matrix('targets')\n",
    "    learnrate=0.01\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    reconstructed = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.squared_error(reconstructed, target_var)\n",
    "    loss = loss.mean()\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=learnrate, momentum=0.975)\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates,on_unused_input='warn')\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X, X_out, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "            # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        # Optionally, you could now dump the network weights to a file like this:\n",
    "    np.savez('CAE_MNIST2.npz', *lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_layers = lasagne.layers.get_all_layers(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'action'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_layer = all_layers[9]\n",
    "action_layer.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_W = action_layer.W.get_value()\n",
    "original_b = action_layer.b.get_value()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action1_W = np.random.randn(original_W.shape[0],original_W.shape[1]).astype(np.float32)\n",
    "action1_b = np.random.randn(original_b.shape[0]).astype(np.float32)\n",
    "action2_W = np.random.randn(original_W.shape[0],original_W.shape[1]).astype(np.float32)\n",
    "action2_b = np.random.randn(original_b.shape[0]).astype(np.float32)\n",
    "action3_W = np.random.randn(original_W.shape[0],original_W.shape[1]).astype(np.float32)\n",
    "action3_b = np.random.randn(original_b.shape[0]).astype(np.float32)\n",
    "action4_W = np.random.randn(original_W.shape[0],original_W.shape[1]).astype(np.float32)\n",
    "action4_b = np.random.randn(original_b.shape[0]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action_layer.W.set_value(action1_W)\n",
    "action_layer.b.set_value(action1_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[action.W, action.b]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[10:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1 of 10 took 15.385s\n",
      "  training loss:\t\t0.027733\n",
      "Epoch 2 of 10 took 15.375s\n",
      "  training loss:\t\t0.022417\n",
      "Epoch 3 of 10 took 15.370s\n",
      "  training loss:\t\t0.021625\n",
      "Epoch 4 of 10 took 15.409s\n",
      "  training loss:\t\t0.021240\n",
      "Epoch 5 of 10 took 15.410s\n",
      "  training loss:\t\t0.020966\n",
      "Epoch 6 of 10 took 15.384s\n",
      "  training loss:\t\t0.020743\n",
      "Epoch 7 of 10 took 15.374s\n",
      "  training loss:\t\t0.020559\n",
      "Epoch 8 of 10 took 15.403s\n",
      "  training loss:\t\t0.020415\n",
      "Epoch 9 of 10 took 15.463s\n",
      "  training loss:\t\t0.020268\n",
      "Epoch 10 of 10 took 15.538s\n",
      "  training loss:\t\t0.020132\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "    loss, params[10:12], learning_rate=learnrate, momentum=0.975)\n",
    "# updates = lasagne.updates.rmsprop(\n",
    "#     loss, params[10:12], learning_rate=0.01)\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates,on_unused_input='warn')\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X, X_out, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action1_W  = action_layer.W.get_value()\n",
    "action1_b = action_layer.b.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_layer.W.set_value(original_W)\n",
    "action_layer.b.set_value(original_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1 of 1 took 9.679s\n",
      "  training loss:\t\t0.029062\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "    loss, params[10:12], learning_rate=learnrate, momentum=0.975)\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates,on_unused_input='warn')\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X, X_out, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "theano.sandbox.cuda.var.CudaNdarraySharedVariable"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_param = lasagne.layers.get_all_param_values(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_encode_net(input_var=None):\n",
    "    \n",
    "    network = InputLayer(shape=(None,  X.shape[1], X.shape[2], X.shape[3]),input_var=input_var)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "\n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "\n",
    "    network = MaxPool2DLayerFast(network, pool_size=pool_size)\n",
    "\n",
    "    network = Conv2DLayerFast(network, num_filters=2*conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "\n",
    "    network = MaxPool2DLayerFast(network, pool_size=pool_size)\n",
    "    \n",
    "    network = ReshapeLayer(network, shape =(([0], -1)))\n",
    "    \n",
    "    network = DenseLayer(network, num_units= dense_mid_size, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    encode_layer = DenseLayer(network, name= 'encode', num_units= encode_size, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    return encode_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encode_network = build_encode_net(input_var)\n",
    "lasagne.layers.set_all_param_values(encode_network, all_param[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_decode_net(input_var=None):\n",
    "    \n",
    "    network = InputLayer(shape=(None, encode_size),input_var=input_var)\n",
    "    \n",
    "    network = DenseLayer(network, num_units= 800, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    network = ReshapeLayer(network, shape =(([0], 2*conv_num_filters, 5, 5)))\n",
    "    \n",
    "    network = Upscale2DLayer(network, scale_factor = pool_size)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_out)\n",
    "\n",
    "    network = Upscale2DLayer(network, scale_factor = pool_size)\n",
    "    \n",
    "    network = Conv2DLayerSlow(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_out)\n",
    "\n",
    "    network = Conv2DLayerSlow(network, num_filters=1, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.sigmoid, filter_size=filter_size, pad=pad_out)\n",
    "    \n",
    "    network = ReshapeLayer(network, shape =(([0], -1)))\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decode_input_var = T.matrix('inputs')\n",
    "decode_network = build_decode_net(decode_input_var)\n",
    "lasagne.layers.set_all_param_values(decode_network, all_param[10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = CAE_encode(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_encode = X_encode.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CAE_encode = theano.function([input_var], [lasagne.layers.get_output(encode_network)])\n",
    "X_pred = np.zeros((50000,784))\n",
    "X_encode = np.zeros((50000,encode_size))\n",
    "i = 0\n",
    "for batch in iterate_minibatches(X, X_out, 1000, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    X_encode[1000*i:1000*(i+1)] = CAE_encode(inputs)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CAE_decode = theano.function([decode_input_var], [lasagne.layers.get_output(decode_network)])\n",
    "i = 0\n",
    "for batch in iterate_minibatches(X_encode, X_out, 1000, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    X_pred[1000*i:1000*(i+1)] = CAE_decode(inputs)[0]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savez('CAE_MNIST_learned_feature2.npz', X_pred,X_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313600112"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
