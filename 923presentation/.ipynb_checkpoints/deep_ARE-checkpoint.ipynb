{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A trivial example of 'deep ARE' with MNIST dataset\n",
    "###  inspired by the [Swarbrick's blog](https://swarbrickjones.wordpress.com/2015/04/29/convolutional-autoencoders-in-pythontheanolasagne/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea\n",
    "First train an autoencoder to learn a low dimension representation space for the original data. Then we can use a fully connected layer to approximate the action transform in the hidden space. Parameters  of the fully connected layer can be learned through gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Structure: \n",
    "![1](1.png )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 750 Ti (CNMeM is disabled, CuDNN 3007)\n",
      "/home/rui/.pyenv/versions/3.5.0/envs/cae/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda_convnet (faster)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, urllib, gzip\n",
    "sys.path.append('/home/rui/pylearn2')\n",
    "from __future__ import print_function\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "import numpy as np\n",
    "from lasagne.layers import get_output, InputLayer, DenseLayer, Upscale2DLayer, ReshapeLayer\n",
    "from lasagne.nonlinearities import rectify, leaky_rectify, tanh\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from lasagne.objectives import categorical_crossentropy\n",
    "import pylearn2\n",
    "from lasagne.layers.cuda_convnet import Conv2DCCLayer as Conv2DLayerFast\n",
    "from lasagne.regularization import regularize_layer_params, l2, l1\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import lasagne\n",
    "from lasagne.layers import Conv2DLayer as Conv2DLayerSlow\n",
    "from lasagne.layers import MaxPool2DLayer as MaxPool2DLayerSlow\n",
    "try:\n",
    "    from lasagne.layers.cuda_convnet import Conv2DCCLayer as Conv2DLayerFast\n",
    "    from lasagne.layers.cuda_convnet import MaxPool2DCCLayer as MaxPool2DLayerFast\n",
    "    print('Using cuda_convnet (faster)')\n",
    "except ImportError:\n",
    "    from lasagne.layers import Conv2DLayer as Conv2DLayerFast\n",
    "    from lasagne.layers import MaxPool2DLayer as MaxPool2DLayerFast\n",
    "    print('Using lasagne.layers (slower)')\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # Change matplotlib backend, in case we have no X server running..\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from IPython.display import Image as IPImage\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = gzip.open('/home/rui/Downloads/mnist.pkl.gz', 'rb')\n",
    "try:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "except:\n",
    "    train_set, valid_set, test_set = pickle.load(f)\n",
    "f.close()\n",
    "X, y = train_set\n",
    "X = np.reshape(X, (-1, 1, 28, 28))\n",
    "X_out = X.reshape((X.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_num_filters = 16\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "encode_size = 16\n",
    "dense_mid_size = 128\n",
    "pad_in = 'valid'    \n",
    "pad_out = 'full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_picture_array(X, rescale=2):\n",
    "    array = X.reshape(28,28)\n",
    "    array = np.clip(array, a_min = 0, a_max = 255)\n",
    "    return  array.repeat(rescale, axis = 0).repeat(rescale, axis = 1).astype(np.uint8())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADgAAAA4CAAAAACN7WTCAAABJUlEQVR4nO2TMUtCURTHf/chRUNE\nSUoQgcOzKTcHhUAUIvoKhZs0+DHaorWtsTWwycGpKR7aqKBCkBTSVgRJELyWW2I9L+I7Fxref7n3\nnPs/58c5cCFSJKsqFofDdHocO/M2in1fdolTMxiztCbi8MQCroGoSLGFEiWWuTXYNqhwSVeUaO5w\nAfQnMmGJOySNthWgIUo8YMlgSpACnkSJ20B7qumMJD3eRIkAzYDnZfY5Yg844VWcuKbPDA4lNlng\nEIcRHh/EuPtVODdRf7FzjnlhoImKT97p4NHihmceWWVRiqhnrPJAXqcGXNPB+7FUWOf+T2H4rZ5O\ntZSAKwtEs2pyxH9aqJTrihFn2qof0N/2cnI5MeJMM6qAnM0Z63XfFyRGsqEvx+YwC2NIz1sAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic_array = get_picture_array(X[2]*255)\n",
    "image = Image.fromarray(pic_array)\n",
    "image.save('temp.png', format=\"PNG\")  \n",
    "IPImage('temp.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define four action transforms (all four are translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def action1(X):\n",
    "    array = np.zeros(X.shape,dtype = np.float32)\n",
    "    array[:,:,:20,:] = X[:,:,8:28,:]\n",
    "    array[:,:,20:,:] = 0\n",
    "    return array\n",
    "def action2(X):\n",
    "    array = np.zeros(X.shape,dtype = np.float32)\n",
    "    array[:,:,8:,:] = X[:,:,0:20,:]\n",
    "    array[:,:,:8,:] = 0\n",
    "    return array\n",
    "def action3(X):\n",
    "    array = np.zeros(X.shape,dtype = np.float32)\n",
    "    array[:,:,:,:20] = X[:,:,:,8:28]\n",
    "    array[:,:,:,20:] = 0\n",
    "    return array\n",
    "def action4(X):\n",
    "    array = np.zeros(X.shape,dtype = np.float32)\n",
    "    array[:,:,:,8:28] = X[:,:,:,:20]\n",
    "    array[:,:,:,:8] = 0\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using 1000 digits image to generate the trainning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data0 = X[:1000,:,:,:]\n",
    "data1 = action1(data0)\n",
    "data2 = action2(data0)\n",
    "data3 = action3(data0)\n",
    "data4 = action4(data0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "action1 is moving the digit upward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADgAAAA4CAAAAACN7WTCAAABB0lEQVR4nO2QP0tCYRSHn/ciikNE\nggri0nBxsl1XQaKvYNQkDn4MN3F1c3QVanJoaoqLNeaQIKQ1tCWBIgS3oZfwz73XS54LEfdZDpz3\nd96HcxSaEXdc4EaGKV0uVzqGa3YHEX8/dIDRWmdfY560Z+wQuBE1nhH3CKU4Bl5FjTng0TXUIs0T\nH6JGgIHD8wGnnFMGGszEjQldTzAokSVKBYMFFksiPGwM/tqovkubGu9MtFHxyZwhFvfc8sYLR8Sk\njHrHOs8UdWvCNUOsn0iVJOOtwf2v2nSNlIBeAEZvruSMf3RQKdMUM/q6qu3wf9DHKRTEjL52VA69\nIHfs921b0BgSEhLyv/kCjYIq5QB8mF8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic_array = get_picture_array(data1[2]*255)\n",
    "image = Image.fromarray(pic_array)\n",
    "image.save('temp.png', format=\"PNG\")  \n",
    "IPImage('temp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.concatenate((data0,data1,data2,data3,data4),axis = 0)\n",
    "data_out = data.reshape((data.shape[0], -1))\n",
    "data1_out = data1.reshape((data1.shape[0], -1))\n",
    "data2_out = data2.reshape((data2.shape[0], -1))\n",
    "data3_out = data3.reshape((data3.shape[0], -1))\n",
    "data4_out = data4.reshape((data4.shape[0], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn(input_var=None):\n",
    "    \n",
    "    network = InputLayer(shape=(None,  X.shape[1], X.shape[2], X.shape[3]),input_var=input_var)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "\n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "\n",
    "    network = MaxPool2DLayerFast(network, pool_size=pool_size)\n",
    "\n",
    "    network = Conv2DLayerFast(network, num_filters=2*conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_in)\n",
    "\n",
    "    network = MaxPool2DLayerFast(network, pool_size=pool_size)\n",
    "    \n",
    "    network = ReshapeLayer(network, shape =(([0], -1)))\n",
    "    \n",
    "    network = DenseLayer(network, num_units= dense_mid_size, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    encode_layer = DenseLayer(network, name= 'encode', num_units= encode_size, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    action_layer = DenseLayer(encode_layer, name= 'action', num_units= encode_size, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                            nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    network = DenseLayer(action_layer, num_units= 800, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    network = ReshapeLayer(network, shape =(([0], 2*conv_num_filters, 5, 5)))\n",
    "    \n",
    "    network = Upscale2DLayer(network, scale_factor = pool_size)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_out)\n",
    "\n",
    "    network = Upscale2DLayer(network, scale_factor = pool_size)\n",
    "    \n",
    "    network = Conv2DLayerFast(network, num_filters=conv_num_filters, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh, filter_size=filter_size, pad=pad_out)\n",
    "\n",
    "    network = Conv2DLayerSlow(network, num_filters=1, W=lasagne.init.Orthogonal(1.0),\\\n",
    "                              nonlinearity=lasagne.nonlinearities.sigmoid, filter_size=filter_size, pad=pad_out)\n",
    "    \n",
    "    network = ReshapeLayer(network, shape =(([0], -1)))\n",
    "\n",
    "    return network\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.matrix('targets')\n",
    "learnrate=0.01\n",
    "# Create neural network model (depending on first command line parameter)\n",
    "network = build_cnn(input_var)\n",
    "\n",
    "with np.load('ARE_MNIST.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "reconstructed = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.squared_error(reconstructed, target_var)\n",
    "loss = loss.mean()\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first train the convolutional autoencoder using all 5000 trainning images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 300 took 1.601s\n",
      "  training loss:\t\t0.222821\n",
      "Epoch 2 of 300 took 1.538s\n",
      "  training loss:\t\t0.179169\n",
      "Epoch 3 of 300 took 1.536s\n",
      "  training loss:\t\t0.126112\n",
      "Epoch 4 of 300 took 1.537s\n",
      "  training loss:\t\t0.096390\n",
      "Epoch 5 of 300 took 1.536s\n",
      "  training loss:\t\t0.091895\n",
      "Epoch 6 of 300 took 1.537s\n",
      "  training loss:\t\t0.093381\n",
      "Epoch 7 of 300 took 1.538s\n",
      "  training loss:\t\t0.094566\n",
      "Epoch 8 of 300 took 1.538s\n",
      "  training loss:\t\t0.095181\n",
      "Epoch 9 of 300 took 1.537s\n",
      "  training loss:\t\t0.095478\n",
      "Epoch 10 of 300 took 1.537s\n",
      "  training loss:\t\t0.095600\n",
      "Epoch 11 of 300 took 1.537s\n",
      "  training loss:\t\t0.095611\n",
      "Epoch 12 of 300 took 1.539s\n",
      "  training loss:\t\t0.095541\n",
      "Epoch 13 of 300 took 1.537s\n",
      "  training loss:\t\t0.095401\n",
      "Epoch 14 of 300 took 1.536s\n",
      "  training loss:\t\t0.095194\n",
      "Epoch 15 of 300 took 1.536s\n",
      "  training loss:\t\t0.094914\n",
      "Epoch 16 of 300 took 1.537s\n",
      "  training loss:\t\t0.094550\n",
      "Epoch 17 of 300 took 1.536s\n",
      "  training loss:\t\t0.094087\n",
      "Epoch 18 of 300 took 1.536s\n",
      "  training loss:\t\t0.093504\n",
      "Epoch 19 of 300 took 1.537s\n",
      "  training loss:\t\t0.092778\n",
      "Epoch 20 of 300 took 1.537s\n",
      "  training loss:\t\t0.091887\n",
      "Epoch 21 of 300 took 1.536s\n",
      "  training loss:\t\t0.090823\n",
      "Epoch 22 of 300 took 1.536s\n",
      "  training loss:\t\t0.089601\n",
      "Epoch 23 of 300 took 1.536s\n",
      "  training loss:\t\t0.088289\n",
      "Epoch 24 of 300 took 1.537s\n",
      "  training loss:\t\t0.087020\n",
      "Epoch 25 of 300 took 1.536s\n",
      "  training loss:\t\t0.085980\n",
      "Epoch 26 of 300 took 1.536s\n",
      "  training loss:\t\t0.085340\n",
      "Epoch 27 of 300 took 1.536s\n",
      "  training loss:\t\t0.085141\n",
      "Epoch 28 of 300 took 1.536s\n",
      "  training loss:\t\t0.085231\n",
      "Epoch 29 of 300 took 1.537s\n",
      "  training loss:\t\t0.085346\n",
      "Epoch 30 of 300 took 1.537s\n",
      "  training loss:\t\t0.085301\n",
      "Epoch 31 of 300 took 1.536s\n",
      "  training loss:\t\t0.085098\n",
      "Epoch 32 of 300 took 1.536s\n",
      "  training loss:\t\t0.084848\n",
      "Epoch 33 of 300 took 1.536s\n",
      "  training loss:\t\t0.084643\n",
      "Epoch 34 of 300 took 1.537s\n",
      "  training loss:\t\t0.084508\n",
      "Epoch 35 of 300 took 1.536s\n",
      "  training loss:\t\t0.084420\n",
      "Epoch 36 of 300 took 1.537s\n",
      "  training loss:\t\t0.084343\n",
      "Epoch 37 of 300 took 1.537s\n",
      "  training loss:\t\t0.084254\n",
      "Epoch 38 of 300 took 1.536s\n",
      "  training loss:\t\t0.084142\n",
      "Epoch 39 of 300 took 1.537s\n",
      "  training loss:\t\t0.084009\n",
      "Epoch 40 of 300 took 1.537s\n",
      "  training loss:\t\t0.083864\n",
      "Epoch 41 of 300 took 1.537s\n",
      "  training loss:\t\t0.083718\n",
      "Epoch 42 of 300 took 1.535s\n",
      "  training loss:\t\t0.083574\n",
      "Epoch 43 of 300 took 1.537s\n",
      "  training loss:\t\t0.083429\n",
      "Epoch 44 of 300 took 1.537s\n",
      "  training loss:\t\t0.083275\n",
      "Epoch 45 of 300 took 1.537s\n",
      "  training loss:\t\t0.083105\n",
      "Epoch 46 of 300 took 1.538s\n",
      "  training loss:\t\t0.082916\n",
      "Epoch 47 of 300 took 1.537s\n",
      "  training loss:\t\t0.082709\n",
      "Epoch 48 of 300 took 1.537s\n",
      "  training loss:\t\t0.082487\n",
      "Epoch 49 of 300 took 1.536s\n",
      "  training loss:\t\t0.082246\n",
      "Epoch 50 of 300 took 1.537s\n",
      "  training loss:\t\t0.081980\n",
      "Epoch 51 of 300 took 1.540s\n",
      "  training loss:\t\t0.081682\n",
      "Epoch 52 of 300 took 1.537s\n",
      "  training loss:\t\t0.081340\n",
      "Epoch 53 of 300 took 1.538s\n",
      "  training loss:\t\t0.080943\n",
      "Epoch 54 of 300 took 1.537s\n",
      "  training loss:\t\t0.080475\n",
      "Epoch 55 of 300 took 1.541s\n",
      "  training loss:\t\t0.079909\n",
      "Epoch 56 of 300 took 1.537s\n",
      "  training loss:\t\t0.079211\n",
      "Epoch 57 of 300 took 1.537s\n",
      "  training loss:\t\t0.078332\n",
      "Epoch 58 of 300 took 1.538s\n",
      "  training loss:\t\t0.077213\n",
      "Epoch 59 of 300 took 1.542s\n",
      "  training loss:\t\t0.075790\n",
      "Epoch 60 of 300 took 1.538s\n",
      "  training loss:\t\t0.074033\n",
      "Epoch 61 of 300 took 1.537s\n",
      "  training loss:\t\t0.072009\n",
      "Epoch 62 of 300 took 1.536s\n",
      "  training loss:\t\t0.069912\n",
      "Epoch 63 of 300 took 1.540s\n",
      "  training loss:\t\t0.067964\n",
      "Epoch 64 of 300 took 1.538s\n",
      "  training loss:\t\t0.066250\n",
      "Epoch 65 of 300 took 1.538s\n",
      "  training loss:\t\t0.064702\n",
      "Epoch 66 of 300 took 1.537s\n",
      "  training loss:\t\t0.063248\n",
      "Epoch 67 of 300 took 1.537s\n",
      "  training loss:\t\t0.061940\n",
      "Epoch 68 of 300 took 1.538s\n",
      "  training loss:\t\t0.060896\n",
      "Epoch 69 of 300 took 1.537s\n",
      "  training loss:\t\t0.060149\n",
      "Epoch 70 of 300 took 1.538s\n",
      "  training loss:\t\t0.059577\n",
      "Epoch 71 of 300 took 1.537s\n",
      "  training loss:\t\t0.058982\n",
      "Epoch 72 of 300 took 1.538s\n",
      "  training loss:\t\t0.058279\n",
      "Epoch 73 of 300 took 1.539s\n",
      "  training loss:\t\t0.057559\n",
      "Epoch 74 of 300 took 1.538s\n",
      "  training loss:\t\t0.056920\n",
      "Epoch 75 of 300 took 1.539s\n",
      "  training loss:\t\t0.056360\n",
      "Epoch 76 of 300 took 1.537s\n",
      "  training loss:\t\t0.055819\n",
      "Epoch 77 of 300 took 1.537s\n",
      "  training loss:\t\t0.055248\n",
      "Epoch 78 of 300 took 1.539s\n",
      "  training loss:\t\t0.054676\n",
      "Epoch 79 of 300 took 1.539s\n",
      "  training loss:\t\t0.054146\n",
      "Epoch 80 of 300 took 1.539s\n",
      "  training loss:\t\t0.053642\n",
      "Epoch 81 of 300 took 1.540s\n",
      "  training loss:\t\t0.053136\n",
      "Epoch 82 of 300 took 1.538s\n",
      "  training loss:\t\t0.052627\n",
      "Epoch 83 of 300 took 1.538s\n",
      "  training loss:\t\t0.052125\n",
      "Epoch 84 of 300 took 1.539s\n",
      "  training loss:\t\t0.051634\n",
      "Epoch 85 of 300 took 1.541s\n",
      "  training loss:\t\t0.051149\n",
      "Epoch 86 of 300 took 1.538s\n",
      "  training loss:\t\t0.050670\n",
      "Epoch 87 of 300 took 1.539s\n",
      "  training loss:\t\t0.050199\n",
      "Epoch 88 of 300 took 1.538s\n",
      "  training loss:\t\t0.049737\n",
      "Epoch 89 of 300 took 1.539s\n",
      "  training loss:\t\t0.049284\n",
      "Epoch 90 of 300 took 1.542s\n",
      "  training loss:\t\t0.048843\n",
      "Epoch 91 of 300 took 1.540s\n",
      "  training loss:\t\t0.048416\n",
      "Epoch 92 of 300 took 1.541s\n",
      "  training loss:\t\t0.048006\n",
      "Epoch 93 of 300 took 1.539s\n",
      "  training loss:\t\t0.047615\n",
      "Epoch 94 of 300 took 1.538s\n",
      "  training loss:\t\t0.047241\n",
      "Epoch 95 of 300 took 1.539s\n",
      "  training loss:\t\t0.046887\n",
      "Epoch 96 of 300 took 1.539s\n",
      "  training loss:\t\t0.046552\n",
      "Epoch 97 of 300 took 1.539s\n",
      "  training loss:\t\t0.046233\n",
      "Epoch 98 of 300 took 1.538s\n",
      "  training loss:\t\t0.045928\n",
      "Epoch 99 of 300 took 1.538s\n",
      "  training loss:\t\t0.045634\n",
      "Epoch 100 of 300 took 1.537s\n",
      "  training loss:\t\t0.045350\n",
      "Epoch 101 of 300 took 1.538s\n",
      "  training loss:\t\t0.045074\n",
      "Epoch 102 of 300 took 1.538s\n",
      "  training loss:\t\t0.044804\n",
      "Epoch 103 of 300 took 1.539s\n",
      "  training loss:\t\t0.044540\n",
      "Epoch 104 of 300 took 1.537s\n",
      "  training loss:\t\t0.044281\n",
      "Epoch 105 of 300 took 1.538s\n",
      "  training loss:\t\t0.044028\n",
      "Epoch 106 of 300 took 1.539s\n",
      "  training loss:\t\t0.043779\n",
      "Epoch 107 of 300 took 1.539s\n",
      "  training loss:\t\t0.043534\n",
      "Epoch 108 of 300 took 1.539s\n",
      "  training loss:\t\t0.043294\n",
      "Epoch 109 of 300 took 1.538s\n",
      "  training loss:\t\t0.043057\n",
      "Epoch 110 of 300 took 1.539s\n",
      "  training loss:\t\t0.042826\n",
      "Epoch 111 of 300 took 1.537s\n",
      "  training loss:\t\t0.042598\n",
      "Epoch 112 of 300 took 1.538s\n",
      "  training loss:\t\t0.042376\n",
      "Epoch 113 of 300 took 1.539s\n",
      "  training loss:\t\t0.042159\n",
      "Epoch 114 of 300 took 1.538s\n",
      "  training loss:\t\t0.041948\n",
      "Epoch 115 of 300 took 1.537s\n",
      "  training loss:\t\t0.041742\n",
      "Epoch 116 of 300 took 1.538s\n",
      "  training loss:\t\t0.041542\n",
      "Epoch 117 of 300 took 1.539s\n",
      "  training loss:\t\t0.041348\n",
      "Epoch 118 of 300 took 1.538s\n",
      "  training loss:\t\t0.041160\n",
      "Epoch 119 of 300 took 1.538s\n",
      "  training loss:\t\t0.040978\n",
      "Epoch 120 of 300 took 1.540s\n",
      "  training loss:\t\t0.040800\n",
      "Epoch 121 of 300 took 1.538s\n",
      "  training loss:\t\t0.040627\n",
      "Epoch 122 of 300 took 1.538s\n",
      "  training loss:\t\t0.040459\n",
      "Epoch 123 of 300 took 1.538s\n",
      "  training loss:\t\t0.040294\n",
      "Epoch 124 of 300 took 1.540s\n",
      "  training loss:\t\t0.040132\n",
      "Epoch 125 of 300 took 1.539s\n",
      "  training loss:\t\t0.039973\n",
      "Epoch 126 of 300 took 1.540s\n",
      "  training loss:\t\t0.039817\n",
      "Epoch 127 of 300 took 1.538s\n",
      "  training loss:\t\t0.039663\n",
      "Epoch 128 of 300 took 1.539s\n",
      "  training loss:\t\t0.039511\n",
      "Epoch 129 of 300 took 1.540s\n",
      "  training loss:\t\t0.039361\n",
      "Epoch 130 of 300 took 1.539s\n",
      "  training loss:\t\t0.039214\n",
      "Epoch 131 of 300 took 1.540s\n",
      "  training loss:\t\t0.039068\n",
      "Epoch 132 of 300 took 1.539s\n",
      "  training loss:\t\t0.038925\n",
      "Epoch 133 of 300 took 1.539s\n",
      "  training loss:\t\t0.038784\n",
      "Epoch 134 of 300 took 1.541s\n",
      "  training loss:\t\t0.038644\n",
      "Epoch 135 of 300 took 1.538s\n",
      "  training loss:\t\t0.038507\n",
      "Epoch 136 of 300 took 1.539s\n",
      "  training loss:\t\t0.038372\n",
      "Epoch 137 of 300 took 1.540s\n",
      "  training loss:\t\t0.038238\n",
      "Epoch 138 of 300 took 1.539s\n",
      "  training loss:\t\t0.038107\n",
      "Epoch 139 of 300 took 1.539s\n",
      "  training loss:\t\t0.037977\n",
      "Epoch 140 of 300 took 1.539s\n",
      "  training loss:\t\t0.037849\n",
      "Epoch 141 of 300 took 1.539s\n",
      "  training loss:\t\t0.037723\n",
      "Epoch 142 of 300 took 1.540s\n",
      "  training loss:\t\t0.037598\n",
      "Epoch 143 of 300 took 1.540s\n",
      "  training loss:\t\t0.037475\n",
      "Epoch 144 of 300 took 1.537s\n",
      "  training loss:\t\t0.037355\n",
      "Epoch 145 of 300 took 1.539s\n",
      "  training loss:\t\t0.037235\n",
      "Epoch 146 of 300 took 1.538s\n",
      "  training loss:\t\t0.037118\n",
      "Epoch 147 of 300 took 1.539s\n",
      "  training loss:\t\t0.037002\n",
      "Epoch 148 of 300 took 1.539s\n",
      "  training loss:\t\t0.036888\n",
      "Epoch 149 of 300 took 1.539s\n",
      "  training loss:\t\t0.036775\n",
      "Epoch 150 of 300 took 1.539s\n",
      "  training loss:\t\t0.036664\n",
      "Epoch 151 of 300 took 1.539s\n",
      "  training loss:\t\t0.036555\n",
      "Epoch 152 of 300 took 1.540s\n",
      "  training loss:\t\t0.036447\n",
      "Epoch 153 of 300 took 1.539s\n",
      "  training loss:\t\t0.036341\n",
      "Epoch 154 of 300 took 1.539s\n",
      "  training loss:\t\t0.036236\n",
      "Epoch 155 of 300 took 1.539s\n",
      "  training loss:\t\t0.036133\n",
      "Epoch 156 of 300 took 1.540s\n",
      "  training loss:\t\t0.036031\n",
      "Epoch 157 of 300 took 1.541s\n",
      "  training loss:\t\t0.035930\n",
      "Epoch 158 of 300 took 1.540s\n",
      "  training loss:\t\t0.035831\n",
      "Epoch 159 of 300 took 1.539s\n",
      "  training loss:\t\t0.035732\n",
      "Epoch 160 of 300 took 1.539s\n",
      "  training loss:\t\t0.035635\n",
      "Epoch 161 of 300 took 1.539s\n",
      "  training loss:\t\t0.035539\n",
      "Epoch 162 of 300 took 1.539s\n",
      "  training loss:\t\t0.035444\n",
      "Epoch 163 of 300 took 1.540s\n",
      "  training loss:\t\t0.035350\n",
      "Epoch 164 of 300 took 1.539s\n",
      "  training loss:\t\t0.035257\n",
      "Epoch 165 of 300 took 1.538s\n",
      "  training loss:\t\t0.035165\n",
      "Epoch 166 of 300 took 1.540s\n",
      "  training loss:\t\t0.035073\n",
      "Epoch 167 of 300 took 1.538s\n",
      "  training loss:\t\t0.034982\n",
      "Epoch 168 of 300 took 1.540s\n",
      "  training loss:\t\t0.034892\n",
      "Epoch 169 of 300 took 1.539s\n",
      "  training loss:\t\t0.034803\n",
      "Epoch 170 of 300 took 1.539s\n",
      "  training loss:\t\t0.034714\n",
      "Epoch 171 of 300 took 1.543s\n",
      "  training loss:\t\t0.034626\n",
      "Epoch 172 of 300 took 1.540s\n",
      "  training loss:\t\t0.034539\n",
      "Epoch 173 of 300 took 1.540s\n",
      "  training loss:\t\t0.034452\n",
      "Epoch 174 of 300 took 1.538s\n",
      "  training loss:\t\t0.034365\n",
      "Epoch 175 of 300 took 1.538s\n",
      "  training loss:\t\t0.034280\n",
      "Epoch 176 of 300 took 1.540s\n",
      "  training loss:\t\t0.034194\n",
      "Epoch 177 of 300 took 1.540s\n",
      "  training loss:\t\t0.034109\n",
      "Epoch 178 of 300 took 1.539s\n",
      "  training loss:\t\t0.034025\n",
      "Epoch 179 of 300 took 1.539s\n",
      "  training loss:\t\t0.033941\n",
      "Epoch 180 of 300 took 1.540s\n",
      "  training loss:\t\t0.033857\n",
      "Epoch 181 of 300 took 1.539s\n",
      "  training loss:\t\t0.033773\n",
      "Epoch 182 of 300 took 1.539s\n",
      "  training loss:\t\t0.033690\n",
      "Epoch 183 of 300 took 1.540s\n",
      "  training loss:\t\t0.033608\n",
      "Epoch 184 of 300 took 1.540s\n",
      "  training loss:\t\t0.033525\n",
      "Epoch 185 of 300 took 1.540s\n",
      "  training loss:\t\t0.033443\n",
      "Epoch 186 of 300 took 1.539s\n",
      "  training loss:\t\t0.033362\n",
      "Epoch 187 of 300 took 1.540s\n",
      "  training loss:\t\t0.033280\n",
      "Epoch 188 of 300 took 1.540s\n",
      "  training loss:\t\t0.033199\n",
      "Epoch 189 of 300 took 1.539s\n",
      "  training loss:\t\t0.033118\n",
      "Epoch 190 of 300 took 1.541s\n",
      "  training loss:\t\t0.033038\n",
      "Epoch 191 of 300 took 1.539s\n",
      "  training loss:\t\t0.032958\n",
      "Epoch 192 of 300 took 1.539s\n",
      "  training loss:\t\t0.032878\n",
      "Epoch 193 of 300 took 1.539s\n",
      "  training loss:\t\t0.032798\n",
      "Epoch 194 of 300 took 1.539s\n",
      "  training loss:\t\t0.032719\n",
      "Epoch 195 of 300 took 1.540s\n",
      "  training loss:\t\t0.032641\n",
      "Epoch 196 of 300 took 1.539s\n",
      "  training loss:\t\t0.032562\n",
      "Epoch 197 of 300 took 1.542s\n",
      "  training loss:\t\t0.032484\n",
      "Epoch 198 of 300 took 1.540s\n",
      "  training loss:\t\t0.032407\n",
      "Epoch 199 of 300 took 1.539s\n",
      "  training loss:\t\t0.032330\n",
      "Epoch 200 of 300 took 1.539s\n",
      "  training loss:\t\t0.032253\n",
      "Epoch 201 of 300 took 1.541s\n",
      "  training loss:\t\t0.032177\n",
      "Epoch 202 of 300 took 1.541s\n",
      "  training loss:\t\t0.032102\n",
      "Epoch 203 of 300 took 1.539s\n",
      "  training loss:\t\t0.032027\n",
      "Epoch 204 of 300 took 1.540s\n",
      "  training loss:\t\t0.031952\n",
      "Epoch 205 of 300 took 1.539s\n",
      "  training loss:\t\t0.031878\n",
      "Epoch 206 of 300 took 1.539s\n",
      "  training loss:\t\t0.031805\n",
      "Epoch 207 of 300 took 1.539s\n",
      "  training loss:\t\t0.031733\n",
      "Epoch 208 of 300 took 1.541s\n",
      "  training loss:\t\t0.031661\n",
      "Epoch 209 of 300 took 1.540s\n",
      "  training loss:\t\t0.031591\n",
      "Epoch 210 of 300 took 1.539s\n",
      "  training loss:\t\t0.031520\n",
      "Epoch 211 of 300 took 1.540s\n",
      "  training loss:\t\t0.031451\n",
      "Epoch 212 of 300 took 1.540s\n",
      "  training loss:\t\t0.031383\n",
      "Epoch 213 of 300 took 1.539s\n",
      "  training loss:\t\t0.031315\n",
      "Epoch 214 of 300 took 1.540s\n",
      "  training loss:\t\t0.031249\n",
      "Epoch 215 of 300 took 1.540s\n",
      "  training loss:\t\t0.031183\n",
      "Epoch 216 of 300 took 1.540s\n",
      "  training loss:\t\t0.031118\n",
      "Epoch 217 of 300 took 1.540s\n",
      "  training loss:\t\t0.031054\n",
      "Epoch 218 of 300 took 1.539s\n",
      "  training loss:\t\t0.030990\n",
      "Epoch 219 of 300 took 1.540s\n",
      "  training loss:\t\t0.030928\n",
      "Epoch 220 of 300 took 1.541s\n",
      "  training loss:\t\t0.030866\n",
      "Epoch 221 of 300 took 1.541s\n",
      "  training loss:\t\t0.030807\n",
      "Epoch 222 of 300 took 1.539s\n",
      "  training loss:\t\t0.030746\n",
      "Epoch 223 of 300 took 1.540s\n",
      "  training loss:\t\t0.030690\n",
      "Epoch 224 of 300 took 1.540s\n",
      "  training loss:\t\t0.030632\n",
      "Epoch 225 of 300 took 1.539s\n",
      "  training loss:\t\t0.030583\n",
      "Epoch 226 of 300 took 1.540s\n",
      "  training loss:\t\t0.030533\n",
      "Epoch 227 of 300 took 1.539s\n",
      "  training loss:\t\t0.030499\n",
      "Epoch 228 of 300 took 1.540s\n",
      "  training loss:\t\t0.030475\n",
      "Epoch 229 of 300 took 1.538s\n",
      "  training loss:\t\t0.030509\n",
      "Epoch 230 of 300 took 1.540s\n",
      "  training loss:\t\t0.030571\n",
      "Epoch 231 of 300 took 1.542s\n",
      "  training loss:\t\t0.030763\n",
      "Epoch 232 of 300 took 1.539s\n",
      "  training loss:\t\t0.031066\n",
      "Epoch 233 of 300 took 1.539s\n",
      "  training loss:\t\t0.031609\n",
      "Epoch 234 of 300 took 1.539s\n",
      "  training loss:\t\t0.031898\n",
      "Epoch 235 of 300 took 1.539s\n",
      "  training loss:\t\t0.031826\n",
      "Epoch 236 of 300 took 1.539s\n",
      "  training loss:\t\t0.031766\n",
      "Epoch 237 of 300 took 1.540s\n",
      "  training loss:\t\t0.031795\n",
      "Epoch 238 of 300 took 1.540s\n",
      "  training loss:\t\t0.031695\n",
      "Epoch 239 of 300 took 1.540s\n",
      "  training loss:\t\t0.031887\n",
      "Epoch 240 of 300 took 1.540s\n",
      "  training loss:\t\t0.032540\n",
      "Epoch 241 of 300 took 1.541s\n",
      "  training loss:\t\t0.031923\n",
      "Epoch 242 of 300 took 1.543s\n",
      "  training loss:\t\t0.031690\n",
      "Epoch 243 of 300 took 1.540s\n",
      "  training loss:\t\t0.032014\n",
      "Epoch 244 of 300 took 1.539s\n",
      "  training loss:\t\t0.032798\n",
      "Epoch 245 of 300 took 1.540s\n",
      "  training loss:\t\t0.031868\n",
      "Epoch 246 of 300 took 1.540s\n",
      "  training loss:\t\t0.031547\n",
      "Epoch 247 of 300 took 1.539s\n",
      "  training loss:\t\t0.031000\n",
      "Epoch 248 of 300 took 1.539s\n",
      "  training loss:\t\t0.030771\n",
      "Epoch 249 of 300 took 1.539s\n",
      "  training loss:\t\t0.030400\n",
      "Epoch 250 of 300 took 1.539s\n",
      "  training loss:\t\t0.030062\n",
      "Epoch 251 of 300 took 1.541s\n",
      "  training loss:\t\t0.030079\n",
      "Epoch 252 of 300 took 1.541s\n",
      "  training loss:\t\t0.030046\n",
      "Epoch 253 of 300 took 1.626s\n",
      "  training loss:\t\t0.029737\n",
      "Epoch 254 of 300 took 1.573s\n",
      "  training loss:\t\t0.029697\n",
      "Epoch 255 of 300 took 1.569s\n",
      "  training loss:\t\t0.029722\n",
      "Epoch 256 of 300 took 1.595s\n",
      "  training loss:\t\t0.029590\n",
      "Epoch 257 of 300 took 1.568s\n",
      "  training loss:\t\t0.029490\n",
      "Epoch 258 of 300 took 1.606s\n",
      "  training loss:\t\t0.029443\n",
      "Epoch 259 of 300 took 1.565s\n",
      "  training loss:\t\t0.029434\n",
      "Epoch 260 of 300 took 1.596s\n",
      "  training loss:\t\t0.029335\n",
      "Epoch 261 of 300 took 1.560s\n",
      "  training loss:\t\t0.029176\n",
      "Epoch 262 of 300 took 1.542s\n",
      "  training loss:\t\t0.029139\n",
      "Epoch 263 of 300 took 1.545s\n",
      "  training loss:\t\t0.029099\n",
      "Epoch 264 of 300 took 1.546s\n",
      "  training loss:\t\t0.028998\n",
      "Epoch 265 of 300 took 1.546s\n",
      "  training loss:\t\t0.028930\n",
      "Epoch 266 of 300 took 1.546s\n",
      "  training loss:\t\t0.028879\n",
      "Epoch 267 of 300 took 1.544s\n",
      "  training loss:\t\t0.028846\n",
      "Epoch 268 of 300 took 1.542s\n",
      "  training loss:\t\t0.028795\n",
      "Epoch 269 of 300 took 1.547s\n",
      "  training loss:\t\t0.028719\n",
      "Epoch 270 of 300 took 1.544s\n",
      "  training loss:\t\t0.028668\n",
      "Epoch 271 of 300 took 1.544s\n",
      "  training loss:\t\t0.028626\n",
      "Epoch 272 of 300 took 1.545s\n",
      "  training loss:\t\t0.028593\n",
      "Epoch 273 of 300 took 1.545s\n",
      "  training loss:\t\t0.028567\n",
      "Epoch 274 of 300 took 1.543s\n",
      "  training loss:\t\t0.028526\n",
      "Epoch 275 of 300 took 1.544s\n",
      "  training loss:\t\t0.028475\n",
      "Epoch 276 of 300 took 1.544s\n",
      "  training loss:\t\t0.028441\n",
      "Epoch 277 of 300 took 1.544s\n",
      "  training loss:\t\t0.028419\n",
      "Epoch 278 of 300 took 1.546s\n",
      "  training loss:\t\t0.028389\n",
      "Epoch 279 of 300 took 1.547s\n",
      "  training loss:\t\t0.028373\n",
      "Epoch 280 of 300 took 1.546s\n",
      "  training loss:\t\t0.028364\n",
      "Epoch 281 of 300 took 1.544s\n",
      "  training loss:\t\t0.028343\n",
      "Epoch 282 of 300 took 1.545s\n",
      "  training loss:\t\t0.028357\n",
      "Epoch 283 of 300 took 1.544s\n",
      "  training loss:\t\t0.028464\n",
      "Epoch 284 of 300 took 1.546s\n",
      "  training loss:\t\t0.028689\n",
      "Epoch 285 of 300 took 1.545s\n",
      "  training loss:\t\t0.028900\n",
      "Epoch 286 of 300 took 1.545s\n",
      "  training loss:\t\t0.029278\n",
      "Epoch 287 of 300 took 1.545s\n",
      "  training loss:\t\t0.029503\n",
      "Epoch 288 of 300 took 1.544s\n",
      "  training loss:\t\t0.031112\n",
      "Epoch 289 of 300 took 1.543s\n",
      "  training loss:\t\t0.030822\n",
      "Epoch 290 of 300 took 1.544s\n",
      "  training loss:\t\t0.032076\n",
      "Epoch 291 of 300 took 1.544s\n",
      "  training loss:\t\t0.029673\n",
      "Epoch 292 of 300 took 1.543s\n",
      "  training loss:\t\t0.029445\n",
      "Epoch 293 of 300 took 1.543s\n",
      "  training loss:\t\t0.029269\n",
      "Epoch 294 of 300 took 1.544s\n",
      "  training loss:\t\t0.029482\n",
      "Epoch 295 of 300 took 1.543s\n",
      "  training loss:\t\t0.029449\n",
      "Epoch 296 of 300 took 1.545s\n",
      "  training loss:\t\t0.029177\n",
      "Epoch 297 of 300 took 1.545s\n",
      "  training loss:\t\t0.029125\n",
      "Epoch 298 of 300 took 1.543s\n",
      "  training loss:\t\t0.028892\n",
      "Epoch 299 of 300 took 1.545s\n",
      "  training loss:\t\t0.029085\n",
      "Epoch 300 of 300 took 1.545s\n",
      "  training loss:\t\t0.028976\n"
     ]
    }
   ],
   "source": [
    "# num_epochs = 1000\n",
    "# input_var = T.tensor4('inputs')\n",
    "# target_var = T.matrix('targets')\n",
    "# learnrate=0.01\n",
    "# # Create neural network model (depending on first command line parameter)\n",
    "# print(\"Building model and compiling functions...\")\n",
    "# network = build_cnn(input_var)\n",
    "# reconstructed = lasagne.layers.get_output(network)\n",
    "# loss = lasagne.objectives.squared_error(reconstructed, target_var)\n",
    "# loss = loss.mean()\n",
    "# params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "# updates = lasagne.updates.nesterov_momentum(\n",
    "#     loss, params, learning_rate=learnrate, momentum=0.975)\n",
    "# #lasagne.updates.rmsprop(loss, params, learning_rate=learnrate)\n",
    "# train_fn = theano.function([input_var, target_var], loss, updates=updates,on_unused_input='warn')\n",
    "# print(\"Starting training...\")\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_err = 0\n",
    "#     train_batches = 0\n",
    "#     start_time = time.time()\n",
    "#     for batch in iterate_minibatches(data, data_out, 500, shuffle=False):\n",
    "#         inputs, targets = batch\n",
    "#         train_err += train_fn(inputs, targets)\n",
    "#         train_batches += 1\n",
    "\n",
    "#         # Then we print the results for this epoch:\n",
    "#     print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "#         epoch + 1, num_epochs, time.time() - start_time))\n",
    "#     print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "#     # Optionally, you could now dump the network weights to a file like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1 of 1000 took 1.572s\n",
      "  training loss:\t\t0.024040\n",
      "Epoch 2 of 1000 took 1.542s\n",
      "  training loss:\t\t0.024014\n",
      "Epoch 3 of 1000 took 1.541s\n",
      "  training loss:\t\t0.023954\n",
      "Epoch 4 of 1000 took 1.543s\n",
      "  training loss:\t\t0.023934\n",
      "Epoch 5 of 1000 took 1.543s\n",
      "  training loss:\t\t0.023907\n",
      "Epoch 6 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023892\n",
      "Epoch 7 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023883\n",
      "Epoch 8 of 1000 took 1.543s\n",
      "  training loss:\t\t0.023873\n",
      "Epoch 9 of 1000 took 1.612s\n",
      "  training loss:\t\t0.023866\n",
      "Epoch 10 of 1000 took 1.586s\n",
      "  training loss:\t\t0.023859\n",
      "Epoch 11 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023851\n",
      "Epoch 12 of 1000 took 1.543s\n",
      "  training loss:\t\t0.023844\n",
      "Epoch 13 of 1000 took 1.545s\n",
      "  training loss:\t\t0.023837\n",
      "Epoch 14 of 1000 took 1.594s\n",
      "  training loss:\t\t0.023830\n",
      "Epoch 15 of 1000 took 1.543s\n",
      "  training loss:\t\t0.023823\n",
      "Epoch 16 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023816\n",
      "Epoch 17 of 1000 took 1.546s\n",
      "  training loss:\t\t0.023809\n",
      "Epoch 18 of 1000 took 1.543s\n",
      "  training loss:\t\t0.023802\n",
      "Epoch 19 of 1000 took 1.545s\n",
      "  training loss:\t\t0.023795\n",
      "Epoch 20 of 1000 took 1.546s\n",
      "  training loss:\t\t0.023788\n",
      "Epoch 21 of 1000 took 1.581s\n",
      "  training loss:\t\t0.023781\n",
      "Epoch 22 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023774\n",
      "Epoch 23 of 1000 took 1.545s\n",
      "  training loss:\t\t0.023767\n",
      "Epoch 24 of 1000 took 1.605s\n",
      "  training loss:\t\t0.023760\n",
      "Epoch 25 of 1000 took 1.606s\n",
      "  training loss:\t\t0.023753\n",
      "Epoch 26 of 1000 took 1.558s\n",
      "  training loss:\t\t0.023746\n",
      "Epoch 27 of 1000 took 1.565s\n",
      "  training loss:\t\t0.023740\n",
      "Epoch 28 of 1000 took 1.557s\n",
      "  training loss:\t\t0.023733\n",
      "Epoch 29 of 1000 took 1.547s\n",
      "  training loss:\t\t0.023726\n",
      "Epoch 30 of 1000 took 1.545s\n",
      "  training loss:\t\t0.023719\n",
      "Epoch 31 of 1000 took 1.546s\n",
      "  training loss:\t\t0.023712\n",
      "Epoch 32 of 1000 took 1.547s\n",
      "  training loss:\t\t0.023705\n",
      "Epoch 33 of 1000 took 1.585s\n",
      "  training loss:\t\t0.023698\n",
      "Epoch 34 of 1000 took 1.545s\n",
      "  training loss:\t\t0.023692\n",
      "Epoch 35 of 1000 took 1.545s\n",
      "  training loss:\t\t0.023685\n",
      "Epoch 36 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023678\n",
      "Epoch 37 of 1000 took 1.546s\n",
      "  training loss:\t\t0.023671\n",
      "Epoch 38 of 1000 took 1.552s\n",
      "  training loss:\t\t0.023664\n",
      "Epoch 39 of 1000 took 1.545s\n",
      "  training loss:\t\t0.023658\n",
      "Epoch 40 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023651\n",
      "Epoch 41 of 1000 took 1.546s\n",
      "  training loss:\t\t0.023644\n",
      "Epoch 42 of 1000 took 1.617s\n",
      "  training loss:\t\t0.023637\n",
      "Epoch 43 of 1000 took 1.585s\n",
      "  training loss:\t\t0.023631\n",
      "Epoch 44 of 1000 took 1.539s\n",
      "  training loss:\t\t0.023624\n",
      "Epoch 45 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023617\n",
      "Epoch 46 of 1000 took 1.666s\n",
      "  training loss:\t\t0.023611\n",
      "Epoch 47 of 1000 took 1.741s\n",
      "  training loss:\t\t0.023604\n",
      "Epoch 48 of 1000 took 1.697s\n",
      "  training loss:\t\t0.023597\n",
      "Epoch 49 of 1000 took 1.693s\n",
      "  training loss:\t\t0.023591\n",
      "Epoch 50 of 1000 took 1.697s\n",
      "  training loss:\t\t0.023584\n",
      "Epoch 51 of 1000 took 1.550s\n",
      "  training loss:\t\t0.023577\n",
      "Epoch 52 of 1000 took 1.727s\n",
      "  training loss:\t\t0.023571\n",
      "Epoch 53 of 1000 took 1.707s\n",
      "  training loss:\t\t0.023564\n",
      "Epoch 54 of 1000 took 1.547s\n",
      "  training loss:\t\t0.023557\n",
      "Epoch 55 of 1000 took 1.601s\n",
      "  training loss:\t\t0.023551\n",
      "Epoch 56 of 1000 took 1.570s\n",
      "  training loss:\t\t0.023544\n",
      "Epoch 57 of 1000 took 1.604s\n",
      "  training loss:\t\t0.023538\n",
      "Epoch 58 of 1000 took 1.563s\n",
      "  training loss:\t\t0.023531\n",
      "Epoch 59 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023524\n",
      "Epoch 60 of 1000 took 1.542s\n",
      "  training loss:\t\t0.023518\n",
      "Epoch 61 of 1000 took 1.603s\n",
      "  training loss:\t\t0.023511\n",
      "Epoch 62 of 1000 took 1.591s\n",
      "  training loss:\t\t0.023505\n",
      "Epoch 63 of 1000 took 1.627s\n",
      "  training loss:\t\t0.023498\n",
      "Epoch 64 of 1000 took 1.625s\n",
      "  training loss:\t\t0.023492\n",
      "Epoch 65 of 1000 took 1.579s\n",
      "  training loss:\t\t0.023485\n",
      "Epoch 66 of 1000 took 1.620s\n",
      "  training loss:\t\t0.023479\n",
      "Epoch 67 of 1000 took 1.638s\n",
      "  training loss:\t\t0.023472\n",
      "Epoch 68 of 1000 took 1.604s\n",
      "  training loss:\t\t0.023466\n",
      "Epoch 69 of 1000 took 1.581s\n",
      "  training loss:\t\t0.023459\n",
      "Epoch 70 of 1000 took 1.542s\n",
      "  training loss:\t\t0.023453\n",
      "Epoch 71 of 1000 took 1.539s\n",
      "  training loss:\t\t0.023446\n",
      "Epoch 72 of 1000 took 1.538s\n",
      "  training loss:\t\t0.023440\n",
      "Epoch 73 of 1000 took 1.538s\n",
      "  training loss:\t\t0.023433\n",
      "Epoch 74 of 1000 took 1.538s\n",
      "  training loss:\t\t0.023427\n",
      "Epoch 75 of 1000 took 1.538s\n",
      "  training loss:\t\t0.023420\n",
      "Epoch 76 of 1000 took 1.538s\n",
      "  training loss:\t\t0.023414\n",
      "Epoch 77 of 1000 took 1.539s\n",
      "  training loss:\t\t0.023408\n",
      "Epoch 78 of 1000 took 1.538s\n",
      "  training loss:\t\t0.023401\n",
      "Epoch 79 of 1000 took 1.540s\n",
      "  training loss:\t\t0.023395\n",
      "Epoch 80 of 1000 took 1.538s\n",
      "  training loss:\t\t0.023388\n",
      "Epoch 81 of 1000 took 1.538s\n",
      "  training loss:\t\t0.023382\n",
      "Epoch 82 of 1000 took 1.539s\n",
      "  training loss:\t\t0.023376\n",
      "Epoch 83 of 1000 took 1.542s\n",
      "  training loss:\t\t0.023369\n",
      "Epoch 84 of 1000 took 1.538s\n",
      "  training loss:\t\t0.023363\n",
      "Epoch 85 of 1000 took 1.538s\n",
      "  training loss:\t\t0.023357\n",
      "Epoch 86 of 1000 took 1.540s\n",
      "  training loss:\t\t0.023350\n",
      "Epoch 87 of 1000 took 1.538s\n",
      "  training loss:\t\t0.023344\n",
      "Epoch 88 of 1000 took 1.538s\n",
      "  training loss:\t\t0.023338\n",
      "Epoch 89 of 1000 took 1.538s\n",
      "  training loss:\t\t0.023331\n",
      "Epoch 90 of 1000 took 1.539s\n",
      "  training loss:\t\t0.023325\n",
      "Epoch 91 of 1000 took 1.540s\n",
      "  training loss:\t\t0.023319\n",
      "Epoch 92 of 1000 took 1.539s\n",
      "  training loss:\t\t0.023312\n",
      "Epoch 93 of 1000 took 1.541s\n",
      "  training loss:\t\t0.023306\n",
      "Epoch 94 of 1000 took 1.538s\n",
      "  training loss:\t\t0.023300\n",
      "Epoch 95 of 1000 took 1.539s\n",
      "  training loss:\t\t0.023294\n",
      "Epoch 96 of 1000 took 1.539s\n",
      "  training loss:\t\t0.023287\n",
      "Epoch 97 of 1000 took 1.538s\n",
      "  training loss:\t\t0.023281\n",
      "Epoch 98 of 1000 took 1.538s\n",
      "  training loss:\t\t0.023275\n",
      "Epoch 99 of 1000 took 1.546s\n",
      "  training loss:\t\t0.023268\n",
      "Epoch 100 of 1000 took 1.540s\n",
      "  training loss:\t\t0.023262\n",
      "Epoch 101 of 1000 took 1.539s\n",
      "  training loss:\t\t0.023256\n",
      "Epoch 102 of 1000 took 1.573s\n",
      "  training loss:\t\t0.023250\n",
      "Epoch 103 of 1000 took 1.670s\n",
      "  training loss:\t\t0.023244\n",
      "Epoch 104 of 1000 took 1.723s\n",
      "  training loss:\t\t0.023237\n",
      "Epoch 105 of 1000 took 1.714s\n",
      "  training loss:\t\t0.023231\n",
      "Epoch 106 of 1000 took 1.667s\n",
      "  training loss:\t\t0.023225\n",
      "Epoch 107 of 1000 took 1.551s\n",
      "  training loss:\t\t0.023219\n",
      "Epoch 108 of 1000 took 1.663s\n",
      "  training loss:\t\t0.023213\n",
      "Epoch 109 of 1000 took 1.659s\n",
      "  training loss:\t\t0.023206\n",
      "Epoch 110 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023200\n",
      "Epoch 111 of 1000 took 1.575s\n",
      "  training loss:\t\t0.023194\n",
      "Epoch 112 of 1000 took 1.558s\n",
      "  training loss:\t\t0.023188\n",
      "Epoch 113 of 1000 took 1.604s\n",
      "  training loss:\t\t0.023182\n",
      "Epoch 114 of 1000 took 1.545s\n",
      "  training loss:\t\t0.023176\n",
      "Epoch 115 of 1000 took 1.543s\n",
      "  training loss:\t\t0.023170\n",
      "Epoch 116 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023164\n",
      "Epoch 117 of 1000 took 1.545s\n",
      "  training loss:\t\t0.023158\n",
      "Epoch 118 of 1000 took 1.543s\n",
      "  training loss:\t\t0.023151\n",
      "Epoch 119 of 1000 took 1.546s\n",
      "  training loss:\t\t0.023145\n",
      "Epoch 120 of 1000 took 1.543s\n",
      "  training loss:\t\t0.023139\n",
      "Epoch 121 of 1000 took 1.543s\n",
      "  training loss:\t\t0.023133\n",
      "Epoch 122 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023127\n",
      "Epoch 123 of 1000 took 1.543s\n",
      "  training loss:\t\t0.023121\n",
      "Epoch 124 of 1000 took 1.542s\n",
      "  training loss:\t\t0.023115\n",
      "Epoch 125 of 1000 took 1.548s\n",
      "  training loss:\t\t0.023109\n",
      "Epoch 126 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023103\n",
      "Epoch 127 of 1000 took 1.546s\n",
      "  training loss:\t\t0.023097\n",
      "Epoch 128 of 1000 took 1.542s\n",
      "  training loss:\t\t0.023091\n",
      "Epoch 129 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023085\n",
      "Epoch 130 of 1000 took 1.543s\n",
      "  training loss:\t\t0.023079\n",
      "Epoch 131 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023073\n",
      "Epoch 132 of 1000 took 1.546s\n",
      "  training loss:\t\t0.023067\n",
      "Epoch 133 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023061\n",
      "Epoch 134 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023055\n",
      "Epoch 135 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023049\n",
      "Epoch 136 of 1000 took 1.584s\n",
      "  training loss:\t\t0.023043\n",
      "Epoch 137 of 1000 took 1.695s\n",
      "  training loss:\t\t0.023038\n",
      "Epoch 138 of 1000 took 1.590s\n",
      "  training loss:\t\t0.023032\n",
      "Epoch 139 of 1000 took 1.545s\n",
      "  training loss:\t\t0.023026\n",
      "Epoch 140 of 1000 took 1.543s\n",
      "  training loss:\t\t0.023020\n",
      "Epoch 141 of 1000 took 1.546s\n",
      "  training loss:\t\t0.023014\n",
      "Epoch 142 of 1000 took 1.544s\n",
      "  training loss:\t\t0.023008\n",
      "Epoch 143 of 1000 took 1.543s\n",
      "  training loss:\t\t0.023002\n",
      "Epoch 144 of 1000 took 1.543s\n",
      "  training loss:\t\t0.022996\n",
      "Epoch 145 of 1000 took 1.544s\n",
      "  training loss:\t\t0.022990\n",
      "Epoch 146 of 1000 took 1.544s\n",
      "  training loss:\t\t0.022985\n",
      "Epoch 147 of 1000 took 1.544s\n",
      "  training loss:\t\t0.022979\n",
      "Epoch 148 of 1000 took 1.544s\n",
      "  training loss:\t\t0.022973\n",
      "Epoch 149 of 1000 took 1.544s\n",
      "  training loss:\t\t0.022967\n",
      "Epoch 150 of 1000 took 1.542s\n",
      "  training loss:\t\t0.022961\n",
      "Epoch 151 of 1000 took 1.543s\n",
      "  training loss:\t\t0.022955\n",
      "Epoch 152 of 1000 took 1.544s\n",
      "  training loss:\t\t0.022950\n",
      "Epoch 153 of 1000 took 1.544s\n",
      "  training loss:\t\t0.022944\n",
      "Epoch 154 of 1000 took 1.543s\n",
      "  training loss:\t\t0.022938\n",
      "Epoch 155 of 1000 took 1.545s\n",
      "  training loss:\t\t0.022932\n",
      "Epoch 156 of 1000 took 1.557s\n",
      "  training loss:\t\t0.022926\n",
      "Epoch 157 of 1000 took 1.542s\n",
      "  training loss:\t\t0.022921\n",
      "Epoch 158 of 1000 took 1.549s\n",
      "  training loss:\t\t0.022915\n",
      "Epoch 159 of 1000 took 1.556s\n",
      "  training loss:\t\t0.022909\n",
      "Epoch 160 of 1000 took 1.549s\n",
      "  training loss:\t\t0.022903\n",
      "Epoch 161 of 1000 took 1.618s\n",
      "  training loss:\t\t0.022897\n",
      "Epoch 162 of 1000 took 1.613s\n",
      "  training loss:\t\t0.022892\n",
      "Epoch 163 of 1000 took 1.677s\n",
      "  training loss:\t\t0.022886\n",
      "Epoch 164 of 1000 took 1.573s\n",
      "  training loss:\t\t0.022880\n",
      "Epoch 165 of 1000 took 1.556s\n",
      "  training loss:\t\t0.022875\n",
      "Epoch 166 of 1000 took 1.574s\n",
      "  training loss:\t\t0.022869\n",
      "Epoch 167 of 1000 took 1.566s\n",
      "  training loss:\t\t0.022863\n",
      "Epoch 168 of 1000 took 1.628s\n",
      "  training loss:\t\t0.022857\n",
      "Epoch 169 of 1000 took 1.560s\n",
      "  training loss:\t\t0.022852\n",
      "Epoch 170 of 1000 took 1.563s\n",
      "  training loss:\t\t0.022846\n",
      "Epoch 171 of 1000 took 1.587s\n",
      "  training loss:\t\t0.022840\n",
      "Epoch 172 of 1000 took 1.617s\n",
      "  training loss:\t\t0.022835\n",
      "Epoch 173 of 1000 took 1.632s\n",
      "  training loss:\t\t0.022829\n",
      "Epoch 174 of 1000 took 1.624s\n",
      "  training loss:\t\t0.022823\n",
      "Epoch 175 of 1000 took 1.642s\n",
      "  training loss:\t\t0.022818\n",
      "Epoch 176 of 1000 took 1.561s\n",
      "  training loss:\t\t0.022812\n",
      "Epoch 177 of 1000 took 1.586s\n",
      "  training loss:\t\t0.022806\n",
      "Epoch 178 of 1000 took 1.558s\n",
      "  training loss:\t\t0.022801\n",
      "Epoch 179 of 1000 took 1.555s\n",
      "  training loss:\t\t0.022795\n",
      "Epoch 180 of 1000 took 1.561s\n",
      "  training loss:\t\t0.022790\n",
      "Epoch 181 of 1000 took 1.616s\n",
      "  training loss:\t\t0.022784\n",
      "Epoch 182 of 1000 took 1.614s\n",
      "  training loss:\t\t0.022778\n",
      "Epoch 183 of 1000 took 1.574s\n",
      "  training loss:\t\t0.022773\n",
      "Epoch 184 of 1000 took 1.571s\n",
      "  training loss:\t\t0.022767\n",
      "Epoch 185 of 1000 took 1.575s\n",
      "  training loss:\t\t0.022762\n",
      "Epoch 186 of 1000 took 1.548s\n",
      "  training loss:\t\t0.022756\n",
      "Epoch 187 of 1000 took 1.542s\n",
      "  training loss:\t\t0.022751\n",
      "Epoch 188 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022745\n",
      "Epoch 189 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022739\n",
      "Epoch 190 of 1000 took 1.563s\n",
      "  training loss:\t\t0.022734\n",
      "Epoch 191 of 1000 took 1.542s\n",
      "  training loss:\t\t0.022728\n",
      "Epoch 192 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022723\n",
      "Epoch 193 of 1000 took 1.542s\n",
      "  training loss:\t\t0.022717\n",
      "Epoch 194 of 1000 took 1.543s\n",
      "  training loss:\t\t0.022712\n",
      "Epoch 195 of 1000 took 1.543s\n",
      "  training loss:\t\t0.022706\n",
      "Epoch 196 of 1000 took 1.542s\n",
      "  training loss:\t\t0.022701\n",
      "Epoch 197 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022695\n",
      "Epoch 198 of 1000 took 1.542s\n",
      "  training loss:\t\t0.022690\n",
      "Epoch 199 of 1000 took 1.542s\n",
      "  training loss:\t\t0.022684\n",
      "Epoch 200 of 1000 took 1.542s\n",
      "  training loss:\t\t0.022679\n",
      "Epoch 201 of 1000 took 1.540s\n",
      "  training loss:\t\t0.022673\n",
      "Epoch 202 of 1000 took 1.542s\n",
      "  training loss:\t\t0.022668\n",
      "Epoch 203 of 1000 took 1.543s\n",
      "  training loss:\t\t0.022662\n",
      "Epoch 204 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022657\n",
      "Epoch 205 of 1000 took 1.539s\n",
      "  training loss:\t\t0.022651\n",
      "Epoch 206 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022646\n",
      "Epoch 207 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022641\n",
      "Epoch 208 of 1000 took 1.542s\n",
      "  training loss:\t\t0.022635\n",
      "Epoch 209 of 1000 took 1.539s\n",
      "  training loss:\t\t0.022630\n",
      "Epoch 210 of 1000 took 1.542s\n",
      "  training loss:\t\t0.022624\n",
      "Epoch 211 of 1000 took 1.540s\n",
      "  training loss:\t\t0.022619\n",
      "Epoch 212 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022613\n",
      "Epoch 213 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022608\n",
      "Epoch 214 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022603\n",
      "Epoch 215 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022597\n",
      "Epoch 216 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022592\n",
      "Epoch 217 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022587\n",
      "Epoch 218 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022581\n",
      "Epoch 219 of 1000 took 1.540s\n",
      "  training loss:\t\t0.022576\n",
      "Epoch 220 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022570\n",
      "Epoch 221 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022565\n",
      "Epoch 222 of 1000 took 1.540s\n",
      "  training loss:\t\t0.022560\n",
      "Epoch 223 of 1000 took 1.544s\n",
      "  training loss:\t\t0.022554\n",
      "Epoch 224 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022549\n",
      "Epoch 225 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022544\n",
      "Epoch 226 of 1000 took 1.539s\n",
      "  training loss:\t\t0.022538\n",
      "Epoch 227 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022533\n",
      "Epoch 228 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022528\n",
      "Epoch 229 of 1000 took 1.542s\n",
      "  training loss:\t\t0.022522\n",
      "Epoch 230 of 1000 took 1.540s\n",
      "  training loss:\t\t0.022517\n",
      "Epoch 231 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022512\n",
      "Epoch 232 of 1000 took 1.539s\n",
      "  training loss:\t\t0.022507\n",
      "Epoch 233 of 1000 took 1.540s\n",
      "  training loss:\t\t0.022501\n",
      "Epoch 234 of 1000 took 1.540s\n",
      "  training loss:\t\t0.022496\n",
      "Epoch 235 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022491\n",
      "Epoch 236 of 1000 took 1.540s\n",
      "  training loss:\t\t0.022485\n",
      "Epoch 237 of 1000 took 1.540s\n",
      "  training loss:\t\t0.022480\n",
      "Epoch 238 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022475\n",
      "Epoch 239 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022470\n",
      "Epoch 240 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022465\n",
      "Epoch 241 of 1000 took 1.540s\n",
      "  training loss:\t\t0.022459\n",
      "Epoch 242 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022454\n",
      "Epoch 243 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022449\n",
      "Epoch 244 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022444\n",
      "Epoch 245 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022438\n",
      "Epoch 246 of 1000 took 1.544s\n",
      "  training loss:\t\t0.022433\n",
      "Epoch 247 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022428\n",
      "Epoch 248 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022423\n",
      "Epoch 249 of 1000 took 1.540s\n",
      "  training loss:\t\t0.022418\n",
      "Epoch 250 of 1000 took 1.542s\n",
      "  training loss:\t\t0.022412\n",
      "Epoch 251 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022407\n",
      "Epoch 252 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022402\n",
      "Epoch 253 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022397\n",
      "Epoch 254 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022392\n",
      "Epoch 255 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022387\n",
      "Epoch 256 of 1000 took 1.540s\n",
      "  training loss:\t\t0.022382\n",
      "Epoch 257 of 1000 took 1.540s\n",
      "  training loss:\t\t0.022376\n",
      "Epoch 258 of 1000 took 1.542s\n",
      "  training loss:\t\t0.022371\n",
      "Epoch 259 of 1000 took 1.540s\n",
      "  training loss:\t\t0.022366\n",
      "Epoch 260 of 1000 took 1.567s\n",
      "  training loss:\t\t0.022361\n",
      "Epoch 261 of 1000 took 1.551s\n",
      "  training loss:\t\t0.022356\n",
      "Epoch 262 of 1000 took 1.549s\n",
      "  training loss:\t\t0.022351\n",
      "Epoch 263 of 1000 took 1.543s\n",
      "  training loss:\t\t0.022346\n",
      "Epoch 264 of 1000 took 1.538s\n",
      "  training loss:\t\t0.022341\n",
      "Epoch 265 of 1000 took 1.537s\n",
      "  training loss:\t\t0.022335\n",
      "Epoch 266 of 1000 took 1.540s\n",
      "  training loss:\t\t0.022330\n",
      "Epoch 267 of 1000 took 1.538s\n",
      "  training loss:\t\t0.022325\n",
      "Epoch 268 of 1000 took 1.536s\n",
      "  training loss:\t\t0.022320\n",
      "Epoch 269 of 1000 took 1.549s\n",
      "  training loss:\t\t0.022315\n",
      "Epoch 270 of 1000 took 1.545s\n",
      "  training loss:\t\t0.022310\n",
      "Epoch 271 of 1000 took 1.545s\n",
      "  training loss:\t\t0.022305\n",
      "Epoch 272 of 1000 took 1.546s\n",
      "  training loss:\t\t0.022300\n",
      "Epoch 273 of 1000 took 1.545s\n",
      "  training loss:\t\t0.022295\n",
      "Epoch 274 of 1000 took 1.545s\n",
      "  training loss:\t\t0.022290\n",
      "Epoch 275 of 1000 took 1.544s\n",
      "  training loss:\t\t0.022285\n",
      "Epoch 276 of 1000 took 1.545s\n",
      "  training loss:\t\t0.022280\n",
      "Epoch 277 of 1000 took 1.544s\n",
      "  training loss:\t\t0.022275\n",
      "Epoch 278 of 1000 took 1.539s\n",
      "  training loss:\t\t0.022270\n",
      "Epoch 279 of 1000 took 1.538s\n",
      "  training loss:\t\t0.022265\n",
      "Epoch 280 of 1000 took 1.536s\n",
      "  training loss:\t\t0.022260\n",
      "Epoch 281 of 1000 took 1.537s\n",
      "  training loss:\t\t0.022255\n",
      "Epoch 282 of 1000 took 1.536s\n",
      "  training loss:\t\t0.022250\n",
      "Epoch 283 of 1000 took 1.537s\n",
      "  training loss:\t\t0.022245\n",
      "Epoch 284 of 1000 took 1.537s\n",
      "  training loss:\t\t0.022240\n",
      "Epoch 285 of 1000 took 1.535s\n",
      "  training loss:\t\t0.022235\n",
      "Epoch 286 of 1000 took 1.539s\n",
      "  training loss:\t\t0.022230\n",
      "Epoch 287 of 1000 took 1.550s\n",
      "  training loss:\t\t0.022225\n",
      "Epoch 288 of 1000 took 1.590s\n",
      "  training loss:\t\t0.022220\n",
      "Epoch 289 of 1000 took 1.617s\n",
      "  training loss:\t\t0.022215\n",
      "Epoch 290 of 1000 took 1.627s\n",
      "  training loss:\t\t0.022210\n",
      "Epoch 291 of 1000 took 1.587s\n",
      "  training loss:\t\t0.022205\n",
      "Epoch 292 of 1000 took 1.623s\n",
      "  training loss:\t\t0.022200\n",
      "Epoch 293 of 1000 took 1.547s\n",
      "  training loss:\t\t0.022195\n",
      "Epoch 294 of 1000 took 1.638s\n",
      "  training loss:\t\t0.022190\n",
      "Epoch 295 of 1000 took 1.543s\n",
      "  training loss:\t\t0.022185\n",
      "Epoch 296 of 1000 took 1.540s\n",
      "  training loss:\t\t0.022180\n",
      "Epoch 297 of 1000 took 1.538s\n",
      "  training loss:\t\t0.022175\n",
      "Epoch 298 of 1000 took 1.539s\n",
      "  training loss:\t\t0.022170\n",
      "Epoch 299 of 1000 took 1.539s\n",
      "  training loss:\t\t0.022165\n",
      "Epoch 300 of 1000 took 1.540s\n",
      "  training loss:\t\t0.022161\n",
      "Epoch 301 of 1000 took 1.540s\n",
      "  training loss:\t\t0.022156\n",
      "Epoch 302 of 1000 took 1.623s\n",
      "  training loss:\t\t0.022151\n",
      "Epoch 303 of 1000 took 1.661s\n",
      "  training loss:\t\t0.022146\n",
      "Epoch 304 of 1000 took 1.626s\n",
      "  training loss:\t\t0.022141\n",
      "Epoch 305 of 1000 took 1.653s\n",
      "  training loss:\t\t0.022136\n",
      "Epoch 306 of 1000 took 1.667s\n",
      "  training loss:\t\t0.022131\n",
      "Epoch 307 of 1000 took 1.577s\n",
      "  training loss:\t\t0.022126\n",
      "Epoch 308 of 1000 took 1.619s\n",
      "  training loss:\t\t0.022122\n",
      "Epoch 309 of 1000 took 1.597s\n",
      "  training loss:\t\t0.022117\n",
      "Epoch 310 of 1000 took 1.551s\n",
      "  training loss:\t\t0.022112\n",
      "Epoch 311 of 1000 took 1.551s\n",
      "  training loss:\t\t0.022107\n",
      "Epoch 312 of 1000 took 1.607s\n",
      "  training loss:\t\t0.022102\n",
      "Epoch 313 of 1000 took 1.707s\n",
      "  training loss:\t\t0.022097\n",
      "Epoch 314 of 1000 took 1.601s\n",
      "  training loss:\t\t0.022093\n",
      "Epoch 315 of 1000 took 1.541s\n",
      "  training loss:\t\t0.022088\n",
      "Epoch 316 of 1000 took 1.538s\n",
      "  training loss:\t\t0.022083\n",
      "Epoch 317 of 1000 took 1.537s\n",
      "  training loss:\t\t0.022078\n",
      "Epoch 318 of 1000 took 1.664s\n",
      "  training loss:\t\t0.022073\n",
      "Epoch 319 of 1000 took 1.691s\n",
      "  training loss:\t\t0.022068\n",
      "Epoch 320 of 1000 took 1.537s\n",
      "  training loss:\t\t0.022064\n",
      "Epoch 321 of 1000 took 1.537s\n",
      "  training loss:\t\t0.022059\n",
      "Epoch 322 of 1000 took 1.537s\n",
      "  training loss:\t\t0.022054\n",
      "Epoch 323 of 1000 took 1.537s\n",
      "  training loss:\t\t0.022049\n",
      "Epoch 324 of 1000 took 1.537s\n",
      "  training loss:\t\t0.022045\n",
      "Epoch 325 of 1000 took 1.537s\n",
      "  training loss:\t\t0.022040\n",
      "Epoch 326 of 1000 took 1.536s\n",
      "  training loss:\t\t0.022035\n",
      "Epoch 327 of 1000 took 1.537s\n",
      "  training loss:\t\t0.022030\n",
      "Epoch 328 of 1000 took 1.576s\n",
      "  training loss:\t\t0.022026\n",
      "Epoch 329 of 1000 took 1.613s\n",
      "  training loss:\t\t0.022021\n",
      "Epoch 330 of 1000 took 1.537s\n",
      "  training loss:\t\t0.022016\n",
      "Epoch 331 of 1000 took 1.652s\n",
      "  training loss:\t\t0.022011\n",
      "Epoch 332 of 1000 took 1.637s\n",
      "  training loss:\t\t0.022007\n",
      "Epoch 333 of 1000 took 1.644s\n",
      "  training loss:\t\t0.022002\n",
      "Epoch 334 of 1000 took 1.580s\n",
      "  training loss:\t\t0.021997\n",
      "Epoch 335 of 1000 took 1.540s\n",
      "  training loss:\t\t0.021993\n",
      "Epoch 336 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021988\n",
      "Epoch 337 of 1000 took 1.577s\n",
      "  training loss:\t\t0.021983\n",
      "Epoch 338 of 1000 took 1.542s\n",
      "  training loss:\t\t0.021978\n",
      "Epoch 339 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021974\n",
      "Epoch 340 of 1000 took 1.538s\n",
      "  training loss:\t\t0.021969\n",
      "Epoch 341 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021964\n",
      "Epoch 342 of 1000 took 1.536s\n",
      "  training loss:\t\t0.021960\n",
      "Epoch 343 of 1000 took 1.536s\n",
      "  training loss:\t\t0.021955\n",
      "Epoch 344 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021950\n",
      "Epoch 345 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021946\n",
      "Epoch 346 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021941\n",
      "Epoch 347 of 1000 took 1.536s\n",
      "  training loss:\t\t0.021936\n",
      "Epoch 348 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021932\n",
      "Epoch 349 of 1000 took 1.539s\n",
      "  training loss:\t\t0.021927\n",
      "Epoch 350 of 1000 took 1.561s\n",
      "  training loss:\t\t0.021923\n",
      "Epoch 351 of 1000 took 1.547s\n",
      "  training loss:\t\t0.021918\n",
      "Epoch 352 of 1000 took 1.627s\n",
      "  training loss:\t\t0.021913\n",
      "Epoch 353 of 1000 took 1.574s\n",
      "  training loss:\t\t0.021909\n",
      "Epoch 354 of 1000 took 1.540s\n",
      "  training loss:\t\t0.021904\n",
      "Epoch 355 of 1000 took 1.574s\n",
      "  training loss:\t\t0.021899\n",
      "Epoch 356 of 1000 took 1.560s\n",
      "  training loss:\t\t0.021895\n",
      "Epoch 357 of 1000 took 1.571s\n",
      "  training loss:\t\t0.021890\n",
      "Epoch 358 of 1000 took 1.722s\n",
      "  training loss:\t\t0.021886\n",
      "Epoch 359 of 1000 took 1.653s\n",
      "  training loss:\t\t0.021881\n",
      "Epoch 360 of 1000 took 1.550s\n",
      "  training loss:\t\t0.021876\n",
      "Epoch 361 of 1000 took 1.553s\n",
      "  training loss:\t\t0.021872\n",
      "Epoch 362 of 1000 took 1.588s\n",
      "  training loss:\t\t0.021867\n",
      "Epoch 363 of 1000 took 1.547s\n",
      "  training loss:\t\t0.021863\n",
      "Epoch 364 of 1000 took 1.550s\n",
      "  training loss:\t\t0.021858\n",
      "Epoch 365 of 1000 took 1.545s\n",
      "  training loss:\t\t0.021854\n",
      "Epoch 366 of 1000 took 1.542s\n",
      "  training loss:\t\t0.021849\n",
      "Epoch 367 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021844\n",
      "Epoch 368 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021840\n",
      "Epoch 369 of 1000 took 1.536s\n",
      "  training loss:\t\t0.021835\n",
      "Epoch 370 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021831\n",
      "Epoch 371 of 1000 took 1.542s\n",
      "  training loss:\t\t0.021826\n",
      "Epoch 372 of 1000 took 1.551s\n",
      "  training loss:\t\t0.021822\n",
      "Epoch 373 of 1000 took 1.555s\n",
      "  training loss:\t\t0.021817\n",
      "Epoch 374 of 1000 took 1.650s\n",
      "  training loss:\t\t0.021813\n",
      "Epoch 375 of 1000 took 1.652s\n",
      "  training loss:\t\t0.021808\n",
      "Epoch 376 of 1000 took 1.583s\n",
      "  training loss:\t\t0.021804\n",
      "Epoch 377 of 1000 took 1.557s\n",
      "  training loss:\t\t0.021799\n",
      "Epoch 378 of 1000 took 1.550s\n",
      "  training loss:\t\t0.021795\n",
      "Epoch 379 of 1000 took 1.549s\n",
      "  training loss:\t\t0.021790\n",
      "Epoch 380 of 1000 took 1.559s\n",
      "  training loss:\t\t0.021786\n",
      "Epoch 381 of 1000 took 1.546s\n",
      "  training loss:\t\t0.021781\n",
      "Epoch 382 of 1000 took 1.543s\n",
      "  training loss:\t\t0.021777\n",
      "Epoch 383 of 1000 took 1.541s\n",
      "  training loss:\t\t0.021772\n",
      "Epoch 384 of 1000 took 1.541s\n",
      "  training loss:\t\t0.021768\n",
      "Epoch 385 of 1000 took 1.539s\n",
      "  training loss:\t\t0.021763\n",
      "Epoch 386 of 1000 took 1.540s\n",
      "  training loss:\t\t0.021759\n",
      "Epoch 387 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021754\n",
      "Epoch 388 of 1000 took 1.545s\n",
      "  training loss:\t\t0.021750\n",
      "Epoch 389 of 1000 took 1.556s\n",
      "  training loss:\t\t0.021745\n",
      "Epoch 390 of 1000 took 1.549s\n",
      "  training loss:\t\t0.021741\n",
      "Epoch 391 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021736\n",
      "Epoch 392 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021732\n",
      "Epoch 393 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021727\n",
      "Epoch 394 of 1000 took 1.543s\n",
      "  training loss:\t\t0.021723\n",
      "Epoch 395 of 1000 took 1.549s\n",
      "  training loss:\t\t0.021719\n",
      "Epoch 396 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021714\n",
      "Epoch 397 of 1000 took 1.570s\n",
      "  training loss:\t\t0.021710\n",
      "Epoch 398 of 1000 took 1.547s\n",
      "  training loss:\t\t0.021705\n",
      "Epoch 399 of 1000 took 1.550s\n",
      "  training loss:\t\t0.021701\n",
      "Epoch 400 of 1000 took 1.638s\n",
      "  training loss:\t\t0.021697\n",
      "Epoch 401 of 1000 took 1.573s\n",
      "  training loss:\t\t0.021692\n",
      "Epoch 402 of 1000 took 1.561s\n",
      "  training loss:\t\t0.021688\n",
      "Epoch 403 of 1000 took 1.599s\n",
      "  training loss:\t\t0.021683\n",
      "Epoch 404 of 1000 took 1.580s\n",
      "  training loss:\t\t0.021679\n",
      "Epoch 405 of 1000 took 1.601s\n",
      "  training loss:\t\t0.021675\n",
      "Epoch 406 of 1000 took 1.614s\n",
      "  training loss:\t\t0.021670\n",
      "Epoch 407 of 1000 took 1.607s\n",
      "  training loss:\t\t0.021666\n",
      "Epoch 408 of 1000 took 1.690s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 409 of 1000 took 1.540s\n",
      "  training loss:\t\t0.021657\n",
      "Epoch 410 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021653\n",
      "Epoch 411 of 1000 took 1.542s\n",
      "  training loss:\t\t0.021648\n",
      "Epoch 412 of 1000 took 1.551s\n",
      "  training loss:\t\t0.021644\n",
      "Epoch 413 of 1000 took 1.547s\n",
      "  training loss:\t\t0.021640\n",
      "Epoch 414 of 1000 took 1.614s\n",
      "  training loss:\t\t0.021635\n",
      "Epoch 415 of 1000 took 1.562s\n",
      "  training loss:\t\t0.021631\n",
      "Epoch 416 of 1000 took 1.556s\n",
      "  training loss:\t\t0.021627\n",
      "Epoch 417 of 1000 took 1.550s\n",
      "  training loss:\t\t0.021622\n",
      "Epoch 418 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021618\n",
      "Epoch 419 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021614\n",
      "Epoch 420 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021609\n",
      "Epoch 421 of 1000 took 1.542s\n",
      "  training loss:\t\t0.021605\n",
      "Epoch 422 of 1000 took 1.545s\n",
      "  training loss:\t\t0.021601\n",
      "Epoch 423 of 1000 took 1.543s\n",
      "  training loss:\t\t0.021596\n",
      "Epoch 424 of 1000 took 1.542s\n",
      "  training loss:\t\t0.021592\n",
      "Epoch 425 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021588\n",
      "Epoch 426 of 1000 took 1.543s\n",
      "  training loss:\t\t0.021583\n",
      "Epoch 427 of 1000 took 1.542s\n",
      "  training loss:\t\t0.021579\n",
      "Epoch 428 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021575\n",
      "Epoch 429 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021570\n",
      "Epoch 430 of 1000 took 1.545s\n",
      "  training loss:\t\t0.021566\n",
      "Epoch 431 of 1000 took 1.543s\n",
      "  training loss:\t\t0.021562\n",
      "Epoch 432 of 1000 took 1.542s\n",
      "  training loss:\t\t0.021558\n",
      "Epoch 433 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021553\n",
      "Epoch 434 of 1000 took 1.543s\n",
      "  training loss:\t\t0.021549\n",
      "Epoch 435 of 1000 took 1.546s\n",
      "  training loss:\t\t0.021545\n",
      "Epoch 436 of 1000 took 1.626s\n",
      "  training loss:\t\t0.021541\n",
      "Epoch 437 of 1000 took 1.593s\n",
      "  training loss:\t\t0.021536\n",
      "Epoch 438 of 1000 took 1.540s\n",
      "  training loss:\t\t0.021532\n",
      "Epoch 439 of 1000 took 1.541s\n",
      "  training loss:\t\t0.021528\n",
      "Epoch 440 of 1000 took 1.562s\n",
      "  training loss:\t\t0.021524\n",
      "Epoch 441 of 1000 took 1.561s\n",
      "  training loss:\t\t0.021519\n",
      "Epoch 442 of 1000 took 1.575s\n",
      "  training loss:\t\t0.021515\n",
      "Epoch 443 of 1000 took 1.584s\n",
      "  training loss:\t\t0.021511\n",
      "Epoch 444 of 1000 took 1.543s\n",
      "  training loss:\t\t0.021507\n",
      "Epoch 445 of 1000 took 1.541s\n",
      "  training loss:\t\t0.021503\n",
      "Epoch 446 of 1000 took 1.541s\n",
      "  training loss:\t\t0.021498\n",
      "Epoch 447 of 1000 took 1.605s\n",
      "  training loss:\t\t0.021494\n",
      "Epoch 448 of 1000 took 1.586s\n",
      "  training loss:\t\t0.021490\n",
      "Epoch 449 of 1000 took 1.611s\n",
      "  training loss:\t\t0.021486\n",
      "Epoch 450 of 1000 took 1.579s\n",
      "  training loss:\t\t0.021482\n",
      "Epoch 451 of 1000 took 1.538s\n",
      "  training loss:\t\t0.021477\n",
      "Epoch 452 of 1000 took 1.539s\n",
      "  training loss:\t\t0.021473\n",
      "Epoch 453 of 1000 took 1.540s\n",
      "  training loss:\t\t0.021469\n",
      "Epoch 454 of 1000 took 1.538s\n",
      "  training loss:\t\t0.021465\n",
      "Epoch 455 of 1000 took 1.579s\n",
      "  training loss:\t\t0.021461\n",
      "Epoch 456 of 1000 took 1.629s\n",
      "  training loss:\t\t0.021456\n",
      "Epoch 457 of 1000 took 1.563s\n",
      "  training loss:\t\t0.021452\n",
      "Epoch 458 of 1000 took 1.563s\n",
      "  training loss:\t\t0.021448\n",
      "Epoch 459 of 1000 took 1.543s\n",
      "  training loss:\t\t0.021444\n",
      "Epoch 460 of 1000 took 1.582s\n",
      "  training loss:\t\t0.021440\n",
      "Epoch 461 of 1000 took 1.585s\n",
      "  training loss:\t\t0.021436\n",
      "Epoch 462 of 1000 took 1.536s\n",
      "  training loss:\t\t0.021432\n",
      "Epoch 463 of 1000 took 1.539s\n",
      "  training loss:\t\t0.021427\n",
      "Epoch 464 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021423\n",
      "Epoch 465 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021419\n",
      "Epoch 466 of 1000 took 1.539s\n",
      "  training loss:\t\t0.021415\n",
      "Epoch 467 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021411\n",
      "Epoch 468 of 1000 took 1.540s\n",
      "  training loss:\t\t0.021407\n",
      "Epoch 469 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021403\n",
      "Epoch 470 of 1000 took 1.538s\n",
      "  training loss:\t\t0.021398\n",
      "Epoch 471 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021394\n",
      "Epoch 472 of 1000 took 1.538s\n",
      "  training loss:\t\t0.021390\n",
      "Epoch 473 of 1000 took 1.538s\n",
      "  training loss:\t\t0.021386\n",
      "Epoch 474 of 1000 took 1.542s\n",
      "  training loss:\t\t0.021382\n",
      "Epoch 475 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021378\n",
      "Epoch 476 of 1000 took 1.548s\n",
      "  training loss:\t\t0.021374\n",
      "Epoch 477 of 1000 took 1.563s\n",
      "  training loss:\t\t0.021370\n",
      "Epoch 478 of 1000 took 1.552s\n",
      "  training loss:\t\t0.021366\n",
      "Epoch 479 of 1000 took 1.538s\n",
      "  training loss:\t\t0.021362\n",
      "Epoch 480 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021357\n",
      "Epoch 481 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021353\n",
      "Epoch 482 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021349\n",
      "Epoch 483 of 1000 took 1.538s\n",
      "  training loss:\t\t0.021345\n",
      "Epoch 484 of 1000 took 1.566s\n",
      "  training loss:\t\t0.021341\n",
      "Epoch 485 of 1000 took 1.553s\n",
      "  training loss:\t\t0.021337\n",
      "Epoch 486 of 1000 took 1.547s\n",
      "  training loss:\t\t0.021333\n",
      "Epoch 487 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021329\n",
      "Epoch 488 of 1000 took 1.554s\n",
      "  training loss:\t\t0.021325\n",
      "Epoch 489 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021321\n",
      "Epoch 490 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021317\n",
      "Epoch 491 of 1000 took 1.551s\n",
      "  training loss:\t\t0.021313\n",
      "Epoch 492 of 1000 took 1.546s\n",
      "  training loss:\t\t0.021309\n",
      "Epoch 493 of 1000 took 1.560s\n",
      "  training loss:\t\t0.021305\n",
      "Epoch 494 of 1000 took 1.548s\n",
      "  training loss:\t\t0.021301\n",
      "Epoch 495 of 1000 took 1.545s\n",
      "  training loss:\t\t0.021298\n",
      "Epoch 496 of 1000 took 1.553s\n",
      "  training loss:\t\t0.021293\n",
      "Epoch 497 of 1000 took 1.548s\n",
      "  training loss:\t\t0.021290\n",
      "Epoch 498 of 1000 took 1.545s\n",
      "  training loss:\t\t0.021285\n",
      "Epoch 499 of 1000 took 1.552s\n",
      "  training loss:\t\t0.021283\n",
      "Epoch 500 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021278\n",
      "Epoch 501 of 1000 took 1.537s\n",
      "  training loss:\t\t0.021276\n",
      "Epoch 502 of 1000 took 1.539s\n",
      "  training loss:\t\t0.021272\n",
      "Epoch 503 of 1000 took 1.553s\n",
      "  training loss:\t\t0.021270\n",
      "Epoch 504 of 1000 took 1.547s\n",
      "  training loss:\t\t0.021266\n",
      "Epoch 505 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021266\n",
      "Epoch 506 of 1000 took 1.543s\n",
      "  training loss:\t\t0.021263\n",
      "Epoch 507 of 1000 took 1.549s\n",
      "  training loss:\t\t0.021265\n",
      "Epoch 508 of 1000 took 1.548s\n",
      "  training loss:\t\t0.021262\n",
      "Epoch 509 of 1000 took 1.548s\n",
      "  training loss:\t\t0.021268\n",
      "Epoch 510 of 1000 took 1.542s\n",
      "  training loss:\t\t0.021268\n",
      "Epoch 511 of 1000 took 1.571s\n",
      "  training loss:\t\t0.021279\n",
      "Epoch 512 of 1000 took 1.625s\n",
      "  training loss:\t\t0.021283\n",
      "Epoch 513 of 1000 took 1.667s\n",
      "  training loss:\t\t0.021302\n",
      "Epoch 514 of 1000 took 1.573s\n",
      "  training loss:\t\t0.021314\n",
      "Epoch 515 of 1000 took 1.590s\n",
      "  training loss:\t\t0.021347\n",
      "Epoch 516 of 1000 took 1.677s\n",
      "  training loss:\t\t0.021370\n",
      "Epoch 517 of 1000 took 1.734s\n",
      "  training loss:\t\t0.021424\n",
      "Epoch 518 of 1000 took 1.728s\n",
      "  training loss:\t\t0.021462\n",
      "Epoch 519 of 1000 took 1.732s\n",
      "  training loss:\t\t0.021543\n",
      "Epoch 520 of 1000 took 1.734s\n",
      "  training loss:\t\t0.021598\n",
      "Epoch 521 of 1000 took 1.732s\n",
      "  training loss:\t\t0.021712\n",
      "Epoch 522 of 1000 took 1.737s\n",
      "  training loss:\t\t0.021773\n",
      "Epoch 523 of 1000 took 1.733s\n",
      "  training loss:\t\t0.021918\n",
      "Epoch 524 of 1000 took 1.730s\n",
      "  training loss:\t\t0.021964\n",
      "Epoch 525 of 1000 took 1.729s\n",
      "  training loss:\t\t0.022142\n",
      "Epoch 526 of 1000 took 1.733s\n",
      "  training loss:\t\t0.022154\n",
      "Epoch 527 of 1000 took 1.716s\n",
      "  training loss:\t\t0.022403\n",
      "Epoch 528 of 1000 took 1.722s\n",
      "  training loss:\t\t0.022400\n",
      "Epoch 529 of 1000 took 1.717s\n",
      "  training loss:\t\t0.022848\n",
      "Epoch 530 of 1000 took 1.720s\n",
      "  training loss:\t\t0.022725\n",
      "Epoch 531 of 1000 took 1.717s\n",
      "  training loss:\t\t0.023212\n",
      "Epoch 532 of 1000 took 1.715s\n",
      "  training loss:\t\t0.022522\n",
      "Epoch 533 of 1000 took 1.718s\n",
      "  training loss:\t\t0.022562\n",
      "Epoch 534 of 1000 took 1.723s\n",
      "  training loss:\t\t0.022023\n",
      "Epoch 535 of 1000 took 1.720s\n",
      "  training loss:\t\t0.021945\n",
      "Epoch 536 of 1000 took 1.715s\n",
      "  training loss:\t\t0.021831\n",
      "Epoch 537 of 1000 took 1.720s\n",
      "  training loss:\t\t0.021754\n",
      "Epoch 538 of 1000 took 1.717s\n",
      "  training loss:\t\t0.021757\n",
      "Epoch 539 of 1000 took 1.712s\n",
      "  training loss:\t\t0.021685\n",
      "Epoch 540 of 1000 took 1.713s\n",
      "  training loss:\t\t0.021643\n",
      "Epoch 541 of 1000 took 1.711s\n",
      "  training loss:\t\t0.021570\n",
      "Epoch 542 of 1000 took 1.709s\n",
      "  training loss:\t\t0.021511\n",
      "Epoch 543 of 1000 took 1.717s\n",
      "  training loss:\t\t0.021445\n",
      "Epoch 544 of 1000 took 1.715s\n",
      "  training loss:\t\t0.021385\n",
      "Epoch 545 of 1000 took 1.710s\n",
      "  training loss:\t\t0.021326\n",
      "Epoch 546 of 1000 took 1.713s\n",
      "  training loss:\t\t0.021273\n",
      "Epoch 547 of 1000 took 1.711s\n",
      "  training loss:\t\t0.021224\n",
      "Epoch 548 of 1000 took 1.711s\n",
      "  training loss:\t\t0.021192\n",
      "Epoch 549 of 1000 took 1.715s\n",
      "  training loss:\t\t0.021164\n",
      "Epoch 550 of 1000 took 1.713s\n",
      "  training loss:\t\t0.021162\n",
      "Epoch 551 of 1000 took 1.724s\n",
      "  training loss:\t\t0.021162\n",
      "Epoch 552 of 1000 took 1.712s\n",
      "  training loss:\t\t0.021178\n",
      "Epoch 553 of 1000 took 1.708s\n",
      "  training loss:\t\t0.021182\n",
      "Epoch 554 of 1000 took 1.710s\n",
      "  training loss:\t\t0.021185\n",
      "Epoch 555 of 1000 took 1.708s\n",
      "  training loss:\t\t0.021169\n",
      "Epoch 556 of 1000 took 1.712s\n",
      "  training loss:\t\t0.021145\n",
      "Epoch 557 of 1000 took 1.725s\n",
      "  training loss:\t\t0.021120\n",
      "Epoch 558 of 1000 took 1.718s\n",
      "  training loss:\t\t0.021101\n",
      "Epoch 559 of 1000 took 1.726s\n",
      "  training loss:\t\t0.021101\n",
      "Epoch 560 of 1000 took 1.727s\n",
      "  training loss:\t\t0.021112\n",
      "Epoch 561 of 1000 took 1.727s\n",
      "  training loss:\t\t0.021135\n",
      "Epoch 562 of 1000 took 1.722s\n",
      "  training loss:\t\t0.021150\n",
      "Epoch 563 of 1000 took 1.724s\n",
      "  training loss:\t\t0.021150\n",
      "Epoch 564 of 1000 took 1.723s\n",
      "  training loss:\t\t0.021139\n",
      "Epoch 565 of 1000 took 1.720s\n",
      "  training loss:\t\t0.021112\n",
      "Epoch 566 of 1000 took 1.718s\n",
      "  training loss:\t\t0.021101\n",
      "Epoch 567 of 1000 took 1.725s\n",
      "  training loss:\t\t0.021097\n",
      "Epoch 568 of 1000 took 1.724s\n",
      "  training loss:\t\t0.021125\n",
      "Epoch 569 of 1000 took 1.733s\n",
      "  training loss:\t\t0.021150\n",
      "Epoch 570 of 1000 took 1.727s\n",
      "  training loss:\t\t0.021178\n",
      "Epoch 571 of 1000 took 1.712s\n",
      "  training loss:\t\t0.021173\n",
      "Epoch 572 of 1000 took 1.711s\n",
      "  training loss:\t\t0.021149\n",
      "Epoch 573 of 1000 took 1.710s\n",
      "  training loss:\t\t0.021105\n",
      "Epoch 574 of 1000 took 1.708s\n",
      "  training loss:\t\t0.021070\n",
      "Epoch 575 of 1000 took 1.708s\n",
      "  training loss:\t\t0.021055\n",
      "Epoch 576 of 1000 took 1.712s\n",
      "  training loss:\t\t0.021074\n",
      "Epoch 577 of 1000 took 1.710s\n",
      "  training loss:\t\t0.021117\n",
      "Epoch 578 of 1000 took 1.711s\n",
      "  training loss:\t\t0.021167\n",
      "Epoch 579 of 1000 took 1.706s\n",
      "  training loss:\t\t0.021188\n",
      "Epoch 580 of 1000 took 1.715s\n",
      "  training loss:\t\t0.021205\n",
      "Epoch 581 of 1000 took 1.712s\n",
      "  training loss:\t\t0.021175\n",
      "Epoch 582 of 1000 took 1.706s\n",
      "  training loss:\t\t0.021183\n",
      "Epoch 583 of 1000 took 1.715s\n",
      "  training loss:\t\t0.021178\n",
      "Epoch 584 of 1000 took 1.711s\n",
      "  training loss:\t\t0.021244\n",
      "Epoch 585 of 1000 took 1.709s\n",
      "  training loss:\t\t0.021286\n",
      "Epoch 586 of 1000 took 1.713s\n",
      "  training loss:\t\t0.021359\n",
      "Epoch 587 of 1000 took 1.720s\n",
      "  training loss:\t\t0.021357\n",
      "Epoch 588 of 1000 took 1.723s\n",
      "  training loss:\t\t0.021352\n",
      "Epoch 589 of 1000 took 1.721s\n",
      "  training loss:\t\t0.021283\n",
      "Epoch 590 of 1000 took 1.716s\n",
      "  training loss:\t\t0.021252\n",
      "Epoch 591 of 1000 took 1.716s\n",
      "  training loss:\t\t0.021208\n",
      "Epoch 592 of 1000 took 1.713s\n",
      "  training loss:\t\t0.021257\n",
      "Epoch 593 of 1000 took 1.707s\n",
      "  training loss:\t\t0.021302\n",
      "Epoch 594 of 1000 took 1.706s\n",
      "  training loss:\t\t0.021443\n",
      "Epoch 595 of 1000 took 1.707s\n",
      "  training loss:\t\t0.021476\n",
      "Epoch 596 of 1000 took 1.708s\n",
      "  training loss:\t\t0.021600\n",
      "Epoch 597 of 1000 took 1.707s\n",
      "  training loss:\t\t0.021511\n",
      "Epoch 598 of 1000 took 1.712s\n",
      "  training loss:\t\t0.021577\n",
      "Epoch 599 of 1000 took 1.713s\n",
      "  training loss:\t\t0.021482\n",
      "Epoch 600 of 1000 took 1.712s\n",
      "  training loss:\t\t0.021587\n",
      "Epoch 601 of 1000 took 1.718s\n",
      "  training loss:\t\t0.021613\n",
      "Epoch 602 of 1000 took 1.720s\n",
      "  training loss:\t\t0.021759\n",
      "Epoch 603 of 1000 took 1.717s\n",
      "  training loss:\t\t0.021808\n",
      "Epoch 604 of 1000 took 1.724s\n",
      "  training loss:\t\t0.021835\n",
      "Epoch 605 of 1000 took 1.720s\n",
      "  training loss:\t\t0.021814\n",
      "Epoch 606 of 1000 took 1.723s\n",
      "  training loss:\t\t0.021704\n",
      "Epoch 607 of 1000 took 1.720s\n",
      "  training loss:\t\t0.021629\n",
      "Epoch 608 of 1000 took 1.715s\n",
      "  training loss:\t\t0.021490\n",
      "Epoch 609 of 1000 took 1.707s\n",
      "  training loss:\t\t0.021409\n",
      "Epoch 610 of 1000 took 1.709s\n",
      "  training loss:\t\t0.021308\n",
      "Epoch 611 of 1000 took 1.714s\n",
      "  training loss:\t\t0.021256\n",
      "Epoch 612 of 1000 took 1.712s\n",
      "  training loss:\t\t0.021192\n",
      "Epoch 613 of 1000 took 1.721s\n",
      "  training loss:\t\t0.021161\n",
      "Epoch 614 of 1000 took 1.714s\n",
      "  training loss:\t\t0.021116\n",
      "Epoch 615 of 1000 took 1.715s\n",
      "  training loss:\t\t0.021091\n",
      "Epoch 616 of 1000 took 1.715s\n",
      "  training loss:\t\t0.021054\n",
      "Epoch 617 of 1000 took 1.708s\n",
      "  training loss:\t\t0.021030\n",
      "Epoch 618 of 1000 took 1.710s\n",
      "  training loss:\t\t0.020997\n",
      "Epoch 619 of 1000 took 1.708s\n",
      "  training loss:\t\t0.020976\n",
      "Epoch 620 of 1000 took 1.707s\n",
      "  training loss:\t\t0.020948\n",
      "Epoch 621 of 1000 took 1.713s\n",
      "  training loss:\t\t0.020937\n",
      "Epoch 622 of 1000 took 1.707s\n",
      "  training loss:\t\t0.020919\n",
      "Epoch 623 of 1000 took 1.704s\n",
      "  training loss:\t\t0.020927\n",
      "Epoch 624 of 1000 took 1.708s\n",
      "  training loss:\t\t0.020926\n",
      "Epoch 625 of 1000 took 1.707s\n",
      "  training loss:\t\t0.020955\n",
      "Epoch 626 of 1000 took 1.703s\n",
      "  training loss:\t\t0.020970\n",
      "Epoch 627 of 1000 took 1.710s\n",
      "  training loss:\t\t0.021008\n",
      "Epoch 628 of 1000 took 1.711s\n",
      "  training loss:\t\t0.021021\n",
      "Epoch 629 of 1000 took 1.708s\n",
      "  training loss:\t\t0.021046\n",
      "Epoch 630 of 1000 took 1.714s\n",
      "  training loss:\t\t0.021042\n",
      "Epoch 631 of 1000 took 1.714s\n",
      "  training loss:\t\t0.021044\n",
      "Epoch 632 of 1000 took 1.712s\n",
      "  training loss:\t\t0.021021\n",
      "Epoch 633 of 1000 took 1.711s\n",
      "  training loss:\t\t0.021009\n",
      "Epoch 634 of 1000 took 1.709s\n",
      "  training loss:\t\t0.020981\n",
      "Epoch 635 of 1000 took 1.706s\n",
      "  training loss:\t\t0.020976\n",
      "Epoch 636 of 1000 took 1.708s\n",
      "  training loss:\t\t0.020958\n",
      "Epoch 637 of 1000 took 1.708s\n",
      "  training loss:\t\t0.020973\n",
      "Epoch 638 of 1000 took 1.711s\n",
      "  training loss:\t\t0.020968\n",
      "Epoch 639 of 1000 took 1.707s\n",
      "  training loss:\t\t0.021002\n",
      "Epoch 640 of 1000 took 1.709s\n",
      "  training loss:\t\t0.020999\n",
      "Epoch 641 of 1000 took 1.710s\n",
      "  training loss:\t\t0.021044\n",
      "Epoch 642 of 1000 took 1.704s\n",
      "  training loss:\t\t0.021033\n",
      "Epoch 643 of 1000 took 1.706s\n",
      "  training loss:\t\t0.021083\n",
      "Epoch 644 of 1000 took 1.708s\n",
      "  training loss:\t\t0.021068\n",
      "Epoch 645 of 1000 took 1.707s\n",
      "  training loss:\t\t0.021127\n",
      "Epoch 646 of 1000 took 1.708s\n",
      "  training loss:\t\t0.021120\n",
      "Epoch 647 of 1000 took 1.703s\n",
      "  training loss:\t\t0.021184\n",
      "Epoch 648 of 1000 took 1.705s\n",
      "  training loss:\t\t0.021183\n",
      "Epoch 649 of 1000 took 1.702s\n",
      "  training loss:\t\t0.021231\n",
      "Epoch 650 of 1000 took 1.704s\n",
      "  training loss:\t\t0.021218\n",
      "Epoch 651 of 1000 took 1.701s\n",
      "  training loss:\t\t0.021232\n",
      "Epoch 652 of 1000 took 1.701s\n",
      "  training loss:\t\t0.021198\n",
      "Epoch 653 of 1000 took 1.705s\n",
      "  training loss:\t\t0.021180\n",
      "Epoch 654 of 1000 took 1.707s\n",
      "  training loss:\t\t0.021131\n",
      "Epoch 655 of 1000 took 1.706s\n",
      "  training loss:\t\t0.021099\n",
      "Epoch 656 of 1000 took 1.710s\n",
      "  training loss:\t\t0.021046\n",
      "Epoch 657 of 1000 took 1.713s\n",
      "  training loss:\t\t0.021015\n",
      "Epoch 658 of 1000 took 1.705s\n",
      "  training loss:\t\t0.020969\n",
      "Epoch 659 of 1000 took 1.705s\n",
      "  training loss:\t\t0.020947\n",
      "Epoch 660 of 1000 took 1.708s\n",
      "  training loss:\t\t0.020909\n",
      "Epoch 661 of 1000 took 1.707s\n",
      "  training loss:\t\t0.020897\n",
      "Epoch 662 of 1000 took 1.706s\n",
      "  training loss:\t\t0.020867\n",
      "Epoch 663 of 1000 took 1.706s\n",
      "  training loss:\t\t0.020862\n",
      "Epoch 664 of 1000 took 1.703s\n",
      "  training loss:\t\t0.020838\n",
      "Epoch 665 of 1000 took 1.709s\n",
      "  training loss:\t\t0.020837\n",
      "Epoch 666 of 1000 took 1.706s\n",
      "  training loss:\t\t0.020817\n",
      "Epoch 667 of 1000 took 1.701s\n",
      "  training loss:\t\t0.020819\n",
      "Epoch 668 of 1000 took 1.704s\n",
      "  training loss:\t\t0.020802\n",
      "Epoch 669 of 1000 took 1.712s\n",
      "  training loss:\t\t0.020805\n",
      "Epoch 670 of 1000 took 1.708s\n",
      "  training loss:\t\t0.020789\n",
      "Epoch 671 of 1000 took 1.711s\n",
      "  training loss:\t\t0.020794\n",
      "Epoch 672 of 1000 took 1.718s\n",
      "  training loss:\t\t0.020780\n",
      "Epoch 673 of 1000 took 1.714s\n",
      "  training loss:\t\t0.020786\n",
      "Epoch 674 of 1000 took 1.713s\n",
      "  training loss:\t\t0.020775\n",
      "Epoch 675 of 1000 took 1.717s\n",
      "  training loss:\t\t0.020784\n",
      "Epoch 676 of 1000 took 1.711s\n",
      "  training loss:\t\t0.020775\n",
      "Epoch 677 of 1000 took 1.714s\n",
      "  training loss:\t\t0.020789\n",
      "Epoch 678 of 1000 took 1.693s\n",
      "  training loss:\t\t0.020782\n",
      "Epoch 679 of 1000 took 1.545s\n",
      "  training loss:\t\t0.020801\n",
      "Epoch 680 of 1000 took 1.544s\n",
      "  training loss:\t\t0.020797\n",
      "Epoch 681 of 1000 took 1.548s\n",
      "  training loss:\t\t0.020823\n",
      "Epoch 682 of 1000 took 1.546s\n",
      "  training loss:\t\t0.020819\n",
      "Epoch 683 of 1000 took 1.544s\n",
      "  training loss:\t\t0.020852\n",
      "Epoch 684 of 1000 took 1.544s\n",
      "  training loss:\t\t0.020850\n",
      "Epoch 685 of 1000 took 1.538s\n",
      "  training loss:\t\t0.020890\n",
      "Epoch 686 of 1000 took 1.556s\n",
      "  training loss:\t\t0.020888\n",
      "Epoch 687 of 1000 took 1.558s\n",
      "  training loss:\t\t0.020934\n",
      "Epoch 688 of 1000 took 1.557s\n",
      "  training loss:\t\t0.020930\n",
      "Epoch 689 of 1000 took 1.561s\n",
      "  training loss:\t\t0.020979\n",
      "Epoch 690 of 1000 took 1.559s\n",
      "  training loss:\t\t0.020972\n",
      "Epoch 691 of 1000 took 1.558s\n",
      "  training loss:\t\t0.021019\n",
      "Epoch 692 of 1000 took 1.558s\n",
      "  training loss:\t\t0.021006\n",
      "Epoch 693 of 1000 took 1.562s\n",
      "  training loss:\t\t0.021048\n",
      "Epoch 694 of 1000 took 1.557s\n",
      "  training loss:\t\t0.021028\n",
      "Epoch 695 of 1000 took 1.562s\n",
      "  training loss:\t\t0.021058\n",
      "Epoch 696 of 1000 took 1.559s\n",
      "  training loss:\t\t0.021030\n",
      "Epoch 697 of 1000 took 1.539s\n",
      "  training loss:\t\t0.021046\n",
      "Epoch 698 of 1000 took 1.538s\n",
      "  training loss:\t\t0.021013\n",
      "Epoch 699 of 1000 took 1.538s\n",
      "  training loss:\t\t0.021013\n",
      "Epoch 700 of 1000 took 1.560s\n",
      "  training loss:\t\t0.020977\n",
      "Epoch 701 of 1000 took 1.585s\n",
      "  training loss:\t\t0.020965\n",
      "Epoch 702 of 1000 took 1.538s\n",
      "  training loss:\t\t0.020926\n",
      "Epoch 703 of 1000 took 1.537s\n",
      "  training loss:\t\t0.020906\n",
      "Epoch 704 of 1000 took 1.537s\n",
      "  training loss:\t\t0.020864\n",
      "Epoch 705 of 1000 took 1.538s\n",
      "  training loss:\t\t0.020836\n",
      "Epoch 706 of 1000 took 1.538s\n",
      "  training loss:\t\t0.020791\n",
      "Epoch 707 of 1000 took 1.537s\n",
      "  training loss:\t\t0.020761\n",
      "Epoch 708 of 1000 took 1.538s\n",
      "  training loss:\t\t0.020717\n",
      "Epoch 709 of 1000 took 1.538s\n",
      "  training loss:\t\t0.020689\n",
      "Epoch 710 of 1000 took 1.538s\n",
      "  training loss:\t\t0.020651\n",
      "Epoch 711 of 1000 took 1.538s\n",
      "  training loss:\t\t0.020632\n",
      "Epoch 712 of 1000 took 1.539s\n",
      "  training loss:\t\t0.020607\n",
      "Epoch 713 of 1000 took 1.538s\n",
      "  training loss:\t\t0.020598\n",
      "Epoch 714 of 1000 took 1.539s\n",
      "  training loss:\t\t0.020585\n",
      "Epoch 715 of 1000 took 1.538s\n",
      "  training loss:\t\t0.020584\n",
      "Epoch 716 of 1000 took 1.538s\n",
      "  training loss:\t\t0.020575\n",
      "Epoch 717 of 1000 took 1.537s\n",
      "  training loss:\t\t0.020573\n",
      "Epoch 718 of 1000 took 1.538s\n",
      "  training loss:\t\t0.020563\n",
      "Epoch 719 of 1000 took 1.560s\n",
      "  training loss:\t\t0.020562\n",
      "Epoch 720 of 1000 took 1.658s\n",
      "  training loss:\t\t0.020555\n",
      "Epoch 721 of 1000 took 1.654s\n",
      "  training loss:\t\t0.020560\n",
      "Epoch 722 of 1000 took 1.649s\n",
      "  training loss:\t\t0.020556\n",
      "Epoch 723 of 1000 took 1.661s\n",
      "  training loss:\t\t0.020563\n",
      "Epoch 724 of 1000 took 1.648s\n",
      "  training loss:\t\t0.020556\n",
      "Epoch 725 of 1000 took 1.661s\n",
      "  training loss:\t\t0.020556\n",
      "Epoch 726 of 1000 took 1.588s\n",
      "  training loss:\t\t0.020545\n",
      "Epoch 727 of 1000 took 1.650s\n",
      "  training loss:\t\t0.020545\n",
      "Epoch 728 of 1000 took 1.657s\n",
      "  training loss:\t\t0.020541\n",
      "Epoch 729 of 1000 took 1.731s\n",
      "  training loss:\t\t0.020552\n",
      "Epoch 730 of 1000 took 1.729s\n",
      "  training loss:\t\t0.020557\n",
      "Epoch 731 of 1000 took 1.730s\n",
      "  training loss:\t\t0.020577\n",
      "Epoch 732 of 1000 took 1.743s\n",
      "  training loss:\t\t0.020580\n",
      "Epoch 733 of 1000 took 1.734s\n",
      "  training loss:\t\t0.020601\n",
      "Epoch 734 of 1000 took 1.732s\n",
      "  training loss:\t\t0.020601\n",
      "Epoch 735 of 1000 took 1.736s\n",
      "  training loss:\t\t0.020630\n",
      "Epoch 736 of 1000 took 1.732s\n",
      "  training loss:\t\t0.020638\n",
      "Epoch 737 of 1000 took 1.731s\n",
      "  training loss:\t\t0.020684\n",
      "Epoch 738 of 1000 took 1.733s\n",
      "  training loss:\t\t0.020698\n",
      "Epoch 739 of 1000 took 1.736s\n",
      "  training loss:\t\t0.020755\n",
      "Epoch 740 of 1000 took 1.641s\n",
      "  training loss:\t\t0.020770\n",
      "Epoch 741 of 1000 took 1.548s\n",
      "  training loss:\t\t0.020840\n",
      "Epoch 742 of 1000 took 1.583s\n",
      "  training loss:\t\t0.020857\n",
      "Epoch 743 of 1000 took 1.665s\n",
      "  training loss:\t\t0.020946\n",
      "Epoch 744 of 1000 took 1.646s\n",
      "  training loss:\t\t0.020960\n",
      "Epoch 745 of 1000 took 1.571s\n",
      "  training loss:\t\t0.021054\n",
      "Epoch 746 of 1000 took 1.589s\n",
      "  training loss:\t\t0.021046\n",
      "Epoch 747 of 1000 took 1.624s\n",
      "  training loss:\t\t0.021112\n",
      "Epoch 748 of 1000 took 1.543s\n",
      "  training loss:\t\t0.021077\n",
      "Epoch 749 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021100\n",
      "Epoch 750 of 1000 took 1.543s\n",
      "  training loss:\t\t0.021062\n",
      "Epoch 751 of 1000 took 1.543s\n",
      "  training loss:\t\t0.021054\n",
      "Epoch 752 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021024\n",
      "Epoch 753 of 1000 took 1.543s\n",
      "  training loss:\t\t0.020991\n",
      "Epoch 754 of 1000 took 1.543s\n",
      "  training loss:\t\t0.020954\n",
      "Epoch 755 of 1000 took 1.542s\n",
      "  training loss:\t\t0.020895\n",
      "Epoch 756 of 1000 took 1.544s\n",
      "  training loss:\t\t0.020837\n",
      "Epoch 757 of 1000 took 1.543s\n",
      "  training loss:\t\t0.020759\n",
      "Epoch 758 of 1000 took 1.543s\n",
      "  training loss:\t\t0.020685\n",
      "Epoch 759 of 1000 took 1.543s\n",
      "  training loss:\t\t0.020608\n",
      "Epoch 760 of 1000 took 1.543s\n",
      "  training loss:\t\t0.020543\n",
      "Epoch 761 of 1000 took 1.543s\n",
      "  training loss:\t\t0.020491\n",
      "Epoch 762 of 1000 took 1.547s\n",
      "  training loss:\t\t0.020468\n",
      "Epoch 763 of 1000 took 1.550s\n",
      "  training loss:\t\t0.020465\n",
      "Epoch 764 of 1000 took 1.543s\n",
      "  training loss:\t\t0.020497\n",
      "Epoch 765 of 1000 took 1.545s\n",
      "  training loss:\t\t0.020532\n",
      "Epoch 766 of 1000 took 1.543s\n",
      "  training loss:\t\t0.020570\n",
      "Epoch 767 of 1000 took 1.543s\n",
      "  training loss:\t\t0.020581\n",
      "Epoch 768 of 1000 took 1.544s\n",
      "  training loss:\t\t0.020564\n",
      "Epoch 769 of 1000 took 1.548s\n",
      "  training loss:\t\t0.020525\n",
      "Epoch 770 of 1000 took 1.542s\n",
      "  training loss:\t\t0.020474\n",
      "Epoch 771 of 1000 took 1.545s\n",
      "  training loss:\t\t0.020435\n",
      "Epoch 772 of 1000 took 1.546s\n",
      "  training loss:\t\t0.020422\n",
      "Epoch 773 of 1000 took 1.543s\n",
      "  training loss:\t\t0.020440\n",
      "Epoch 774 of 1000 took 1.543s\n",
      "  training loss:\t\t0.020477\n",
      "Epoch 775 of 1000 took 1.543s\n",
      "  training loss:\t\t0.020511\n",
      "Epoch 776 of 1000 took 1.578s\n",
      "  training loss:\t\t0.020527\n",
      "Epoch 777 of 1000 took 1.548s\n",
      "  training loss:\t\t0.020501\n",
      "Epoch 778 of 1000 took 1.548s\n",
      "  training loss:\t\t0.020468\n",
      "Epoch 779 of 1000 took 1.545s\n",
      "  training loss:\t\t0.020422\n",
      "Epoch 780 of 1000 took 1.547s\n",
      "  training loss:\t\t0.020413\n",
      "Epoch 781 of 1000 took 1.549s\n",
      "  training loss:\t\t0.020428\n",
      "Epoch 782 of 1000 took 1.549s\n",
      "  training loss:\t\t0.020484\n",
      "Epoch 783 of 1000 took 1.568s\n",
      "  training loss:\t\t0.020531\n",
      "Epoch 784 of 1000 took 1.544s\n",
      "  training loss:\t\t0.020558\n",
      "Epoch 785 of 1000 took 1.545s\n",
      "  training loss:\t\t0.020544\n",
      "Epoch 786 of 1000 took 1.544s\n",
      "  training loss:\t\t0.020498\n",
      "Epoch 787 of 1000 took 1.548s\n",
      "  training loss:\t\t0.020444\n",
      "Epoch 788 of 1000 took 1.555s\n",
      "  training loss:\t\t0.020410\n",
      "Epoch 789 of 1000 took 1.553s\n",
      "  training loss:\t\t0.020406\n",
      "Epoch 790 of 1000 took 1.553s\n",
      "  training loss:\t\t0.020445\n",
      "Epoch 791 of 1000 took 1.548s\n",
      "  training loss:\t\t0.020491\n",
      "Epoch 792 of 1000 took 1.549s\n",
      "  training loss:\t\t0.020542\n",
      "Epoch 793 of 1000 took 1.548s\n",
      "  training loss:\t\t0.020537\n",
      "Epoch 794 of 1000 took 1.546s\n",
      "  training loss:\t\t0.020533\n",
      "Epoch 795 of 1000 took 1.547s\n",
      "  training loss:\t\t0.020484\n",
      "Epoch 796 of 1000 took 1.548s\n",
      "  training loss:\t\t0.020486\n",
      "Epoch 797 of 1000 took 1.549s\n",
      "  training loss:\t\t0.020492\n",
      "Epoch 798 of 1000 took 1.550s\n",
      "  training loss:\t\t0.020567\n",
      "Epoch 799 of 1000 took 1.550s\n",
      "  training loss:\t\t0.020632\n",
      "Epoch 800 of 1000 took 1.550s\n",
      "  training loss:\t\t0.020701\n",
      "Epoch 801 of 1000 took 1.550s\n",
      "  training loss:\t\t0.020720\n",
      "Epoch 802 of 1000 took 1.548s\n",
      "  training loss:\t\t0.020708\n",
      "Epoch 803 of 1000 took 1.548s\n",
      "  training loss:\t\t0.020666\n",
      "Epoch 804 of 1000 took 1.550s\n",
      "  training loss:\t\t0.020626\n",
      "Epoch 805 of 1000 took 1.547s\n",
      "  training loss:\t\t0.020583\n",
      "Epoch 806 of 1000 took 1.548s\n",
      "  training loss:\t\t0.020583\n",
      "Epoch 807 of 1000 took 1.549s\n",
      "  training loss:\t\t0.020570\n",
      "Epoch 808 of 1000 took 1.548s\n",
      "  training loss:\t\t0.020605\n",
      "Epoch 809 of 1000 took 1.547s\n",
      "  training loss:\t\t0.020590\n",
      "Epoch 810 of 1000 took 1.549s\n",
      "  training loss:\t\t0.020617\n",
      "Epoch 811 of 1000 took 1.548s\n",
      "  training loss:\t\t0.020578\n",
      "Epoch 812 of 1000 took 1.549s\n",
      "  training loss:\t\t0.020600\n",
      "Epoch 813 of 1000 took 1.551s\n",
      "  training loss:\t\t0.020579\n",
      "Epoch 814 of 1000 took 1.547s\n",
      "  training loss:\t\t0.020634\n",
      "Epoch 815 of 1000 took 1.548s\n",
      "  training loss:\t\t0.020672\n",
      "Epoch 816 of 1000 took 1.578s\n",
      "  training loss:\t\t0.020749\n",
      "Epoch 817 of 1000 took 1.666s\n",
      "  training loss:\t\t0.020802\n",
      "Epoch 818 of 1000 took 1.682s\n",
      "  training loss:\t\t0.020822\n",
      "Epoch 819 of 1000 took 1.669s\n",
      "  training loss:\t\t0.020825\n",
      "Epoch 820 of 1000 took 1.597s\n",
      "  training loss:\t\t0.020766\n",
      "Epoch 821 of 1000 took 1.585s\n",
      "  training loss:\t\t0.020712\n",
      "Epoch 822 of 1000 took 1.631s\n",
      "  training loss:\t\t0.020618\n",
      "Epoch 823 of 1000 took 1.782s\n",
      "  training loss:\t\t0.020539\n",
      "Epoch 824 of 1000 took 1.741s\n",
      "  training loss:\t\t0.020452\n",
      "Epoch 825 of 1000 took 1.753s\n",
      "  training loss:\t\t0.020382\n",
      "Epoch 826 of 1000 took 1.718s\n",
      "  training loss:\t\t0.020321\n",
      "Epoch 827 of 1000 took 1.716s\n",
      "  training loss:\t\t0.020280\n",
      "Epoch 828 of 1000 took 1.710s\n",
      "  training loss:\t\t0.020251\n",
      "Epoch 829 of 1000 took 1.705s\n",
      "  training loss:\t\t0.020243\n",
      "Epoch 830 of 1000 took 1.721s\n",
      "  training loss:\t\t0.020243\n",
      "Epoch 831 of 1000 took 1.713s\n",
      "  training loss:\t\t0.020259\n",
      "Epoch 832 of 1000 took 1.710s\n",
      "  training loss:\t\t0.020270\n",
      "Epoch 833 of 1000 took 1.713s\n",
      "  training loss:\t\t0.020282\n",
      "Epoch 834 of 1000 took 1.713s\n",
      "  training loss:\t\t0.020277\n",
      "Epoch 835 of 1000 took 1.710s\n",
      "  training loss:\t\t0.020260\n",
      "Epoch 836 of 1000 took 1.709s\n",
      "  training loss:\t\t0.020235\n",
      "Epoch 837 of 1000 took 1.707s\n",
      "  training loss:\t\t0.020210\n",
      "Epoch 838 of 1000 took 1.705s\n",
      "  training loss:\t\t0.020195\n",
      "Epoch 839 of 1000 took 1.706s\n",
      "  training loss:\t\t0.020196\n",
      "Epoch 840 of 1000 took 1.707s\n",
      "  training loss:\t\t0.020211\n",
      "Epoch 841 of 1000 took 1.706s\n",
      "  training loss:\t\t0.020231\n",
      "Epoch 842 of 1000 took 1.707s\n",
      "  training loss:\t\t0.020241\n",
      "Epoch 843 of 1000 took 1.701s\n",
      "  training loss:\t\t0.020241\n",
      "Epoch 844 of 1000 took 1.701s\n",
      "  training loss:\t\t0.020222\n",
      "Epoch 845 of 1000 took 1.704s\n",
      "  training loss:\t\t0.020209\n",
      "Epoch 846 of 1000 took 1.707s\n",
      "  training loss:\t\t0.020196\n",
      "Epoch 847 of 1000 took 1.703s\n",
      "  training loss:\t\t0.020213\n",
      "Epoch 848 of 1000 took 1.706s\n",
      "  training loss:\t\t0.020234\n",
      "Epoch 849 of 1000 took 1.713s\n",
      "  training loss:\t\t0.020272\n",
      "Epoch 850 of 1000 took 1.708s\n",
      "  training loss:\t\t0.020291\n",
      "Epoch 851 of 1000 took 1.710s\n",
      "  training loss:\t\t0.020307\n",
      "Epoch 852 of 1000 took 1.710s\n",
      "  training loss:\t\t0.020303\n",
      "Epoch 853 of 1000 took 1.712s\n",
      "  training loss:\t\t0.020320\n",
      "Epoch 854 of 1000 took 1.718s\n",
      "  training loss:\t\t0.020337\n",
      "Epoch 855 of 1000 took 1.571s\n",
      "  training loss:\t\t0.020408\n",
      "Epoch 856 of 1000 took 1.547s\n",
      "  training loss:\t\t0.020467\n",
      "Epoch 857 of 1000 took 1.616s\n",
      "  training loss:\t\t0.020588\n",
      "Epoch 858 of 1000 took 1.562s\n",
      "  training loss:\t\t0.020641\n",
      "Epoch 859 of 1000 took 1.543s\n",
      "  training loss:\t\t0.020771\n",
      "Epoch 860 of 1000 took 1.545s\n",
      "  training loss:\t\t0.020789\n",
      "Epoch 861 of 1000 took 1.553s\n",
      "  training loss:\t\t0.020914\n",
      "Epoch 862 of 1000 took 1.549s\n",
      "  training loss:\t\t0.020938\n",
      "Epoch 863 of 1000 took 1.598s\n",
      "  training loss:\t\t0.021046\n",
      "Epoch 864 of 1000 took 1.544s\n",
      "  training loss:\t\t0.021098\n",
      "Epoch 865 of 1000 took 1.549s\n",
      "  training loss:\t\t0.021129\n",
      "Epoch 866 of 1000 took 1.591s\n",
      "  training loss:\t\t0.021173\n",
      "Epoch 867 of 1000 took 1.546s\n",
      "  training loss:\t\t0.021107\n",
      "Epoch 868 of 1000 took 1.579s\n",
      "  training loss:\t\t0.021136\n",
      "Epoch 869 of 1000 took 1.588s\n",
      "  training loss:\t\t0.021019\n",
      "Epoch 870 of 1000 took 1.547s\n",
      "  training loss:\t\t0.021053\n",
      "Epoch 871 of 1000 took 1.545s\n",
      "  training loss:\t\t0.020923\n",
      "Epoch 872 of 1000 took 1.597s\n",
      "  training loss:\t\t0.020960\n",
      "Epoch 873 of 1000 took 1.542s\n",
      "  training loss:\t\t0.020827\n",
      "Epoch 874 of 1000 took 1.543s\n",
      "  training loss:\t\t0.020845\n",
      "Epoch 875 of 1000 took 1.594s\n",
      "  training loss:\t\t0.020713\n",
      "Epoch 876 of 1000 took 1.584s\n",
      "  training loss:\t\t0.020701\n",
      "Epoch 877 of 1000 took 1.545s\n",
      "  training loss:\t\t0.020583\n",
      "Epoch 878 of 1000 took 1.612s\n",
      "  training loss:\t\t0.020551\n",
      "Epoch 879 of 1000 took 1.645s\n",
      "  training loss:\t\t0.020454\n",
      "Epoch 880 of 1000 took 1.606s\n",
      "  training loss:\t\t0.020418\n",
      "Epoch 881 of 1000 took 1.689s\n",
      "  training loss:\t\t0.020343\n",
      "Epoch 882 of 1000 took 1.654s\n",
      "  training loss:\t\t0.020311\n",
      "Epoch 883 of 1000 took 1.619s\n",
      "  training loss:\t\t0.020254\n",
      "Epoch 884 of 1000 took 1.572s\n",
      "  training loss:\t\t0.020232\n",
      "Epoch 885 of 1000 took 1.623s\n",
      "  training loss:\t\t0.020190\n",
      "Epoch 886 of 1000 took 1.672s\n",
      "  training loss:\t\t0.020179\n",
      "Epoch 887 of 1000 took 1.564s\n",
      "  training loss:\t\t0.020151\n",
      "Epoch 888 of 1000 took 1.576s\n",
      "  training loss:\t\t0.020152\n",
      "Epoch 889 of 1000 took 1.562s\n",
      "  training loss:\t\t0.020138\n",
      "Epoch 890 of 1000 took 1.553s\n",
      "  training loss:\t\t0.020151\n",
      "Epoch 891 of 1000 took 1.549s\n",
      "  training loss:\t\t0.020150\n",
      "Epoch 892 of 1000 took 1.564s\n",
      "  training loss:\t\t0.020166\n",
      "Epoch 893 of 1000 took 1.589s\n",
      "  training loss:\t\t0.020167\n",
      "Epoch 894 of 1000 took 1.606s\n",
      "  training loss:\t\t0.020173\n",
      "Epoch 895 of 1000 took 1.635s\n",
      "  training loss:\t\t0.020165\n",
      "Epoch 896 of 1000 took 1.619s\n",
      "  training loss:\t\t0.020151\n",
      "Epoch 897 of 1000 took 1.565s\n",
      "  training loss:\t\t0.020130\n",
      "Epoch 898 of 1000 took 1.589s\n",
      "  training loss:\t\t0.020104\n",
      "Epoch 899 of 1000 took 1.683s\n",
      "  training loss:\t\t0.020086\n",
      "Epoch 900 of 1000 took 1.688s\n",
      "  training loss:\t\t0.020069\n",
      "Epoch 901 of 1000 took 1.604s\n",
      "  training loss:\t\t0.020078\n",
      "Epoch 902 of 1000 took 1.560s\n",
      "  training loss:\t\t0.020091\n",
      "Epoch 903 of 1000 took 1.567s\n",
      "  training loss:\t\t0.020138\n",
      "Epoch 904 of 1000 took 1.549s\n",
      "  training loss:\t\t0.020186\n",
      "Epoch 905 of 1000 took 1.554s\n",
      "  training loss:\t\t0.020254\n",
      "Epoch 906 of 1000 took 1.555s\n",
      "  training loss:\t\t0.020318\n",
      "Epoch 907 of 1000 took 1.549s\n",
      "  training loss:\t\t0.020378\n",
      "Epoch 908 of 1000 took 1.553s\n",
      "  training loss:\t\t0.020443\n",
      "Epoch 909 of 1000 took 1.702s\n",
      "  training loss:\t\t0.020484\n",
      "Epoch 910 of 1000 took 1.671s\n",
      "  training loss:\t\t0.020538\n",
      "Epoch 911 of 1000 took 1.684s\n",
      "  training loss:\t\t0.020562\n",
      "Epoch 912 of 1000 took 1.595s\n",
      "  training loss:\t\t0.020589\n",
      "Epoch 913 of 1000 took 1.576s\n",
      "  training loss:\t\t0.020594\n",
      "Epoch 914 of 1000 took 1.537s\n",
      "  training loss:\t\t0.020582\n",
      "Epoch 915 of 1000 took 1.599s\n",
      "  training loss:\t\t0.020568\n",
      "Epoch 916 of 1000 took 1.541s\n",
      "  training loss:\t\t0.020529\n",
      "Epoch 917 of 1000 took 1.544s\n",
      "  training loss:\t\t0.020502\n",
      "Epoch 918 of 1000 took 1.549s\n",
      "  training loss:\t\t0.020467\n",
      "Epoch 919 of 1000 took 1.546s\n",
      "  training loss:\t\t0.020436\n",
      "Epoch 920 of 1000 took 1.546s\n",
      "  training loss:\t\t0.020423\n",
      "Epoch 921 of 1000 took 1.544s\n",
      "  training loss:\t\t0.020391\n",
      "Epoch 922 of 1000 took 1.541s\n",
      "  training loss:\t\t0.020393\n",
      "Epoch 923 of 1000 took 1.554s\n",
      "  training loss:\t\t0.020354\n",
      "Epoch 924 of 1000 took 1.551s\n",
      "  training loss:\t\t0.020354\n",
      "Epoch 925 of 1000 took 1.542s\n",
      "  training loss:\t\t0.020304\n",
      "Epoch 926 of 1000 took 1.579s\n",
      "  training loss:\t\t0.020293\n",
      "Epoch 927 of 1000 took 1.727s\n",
      "  training loss:\t\t0.020238\n",
      "Epoch 928 of 1000 took 1.607s\n",
      "  training loss:\t\t0.020220\n",
      "Epoch 929 of 1000 took 1.608s\n",
      "  training loss:\t\t0.020169\n",
      "Epoch 930 of 1000 took 1.590s\n",
      "  training loss:\t\t0.020154\n",
      "Epoch 931 of 1000 took 1.551s\n",
      "  training loss:\t\t0.020113\n",
      "Epoch 932 of 1000 took 1.601s\n",
      "  training loss:\t\t0.020111\n",
      "Epoch 933 of 1000 took 1.637s\n",
      "  training loss:\t\t0.020090\n",
      "Epoch 934 of 1000 took 1.653s\n",
      "  training loss:\t\t0.020115\n",
      "Epoch 935 of 1000 took 1.575s\n",
      "  training loss:\t\t0.020125\n",
      "Epoch 936 of 1000 took 1.644s\n",
      "  training loss:\t\t0.020183\n",
      "Epoch 937 of 1000 took 1.659s\n",
      "  training loss:\t\t0.020229\n",
      "Epoch 938 of 1000 took 1.606s\n",
      "  training loss:\t\t0.020306\n",
      "Epoch 939 of 1000 took 1.603s\n",
      "  training loss:\t\t0.020369\n",
      "Epoch 940 of 1000 took 1.556s\n",
      "  training loss:\t\t0.020422\n",
      "Epoch 941 of 1000 took 1.536s\n",
      "  training loss:\t\t0.020469\n",
      "Epoch 942 of 1000 took 1.536s\n",
      "  training loss:\t\t0.020468\n",
      "Epoch 943 of 1000 took 1.538s\n",
      "  training loss:\t\t0.020483\n",
      "Epoch 944 of 1000 took 1.536s\n",
      "  training loss:\t\t0.020433\n",
      "Epoch 945 of 1000 took 1.536s\n",
      "  training loss:\t\t0.020421\n",
      "Epoch 946 of 1000 took 1.536s\n",
      "  training loss:\t\t0.020348\n",
      "Epoch 947 of 1000 took 1.536s\n",
      "  training loss:\t\t0.020329\n",
      "Epoch 948 of 1000 took 1.535s\n",
      "  training loss:\t\t0.020258\n",
      "Epoch 949 of 1000 took 1.538s\n",
      "  training loss:\t\t0.020242\n",
      "Epoch 950 of 1000 took 1.535s\n",
      "  training loss:\t\t0.020183\n",
      "Epoch 951 of 1000 took 1.536s\n",
      "  training loss:\t\t0.020171\n",
      "Epoch 952 of 1000 took 1.535s\n",
      "  training loss:\t\t0.020126\n",
      "Epoch 953 of 1000 took 1.535s\n",
      "  training loss:\t\t0.020115\n",
      "Epoch 954 of 1000 took 1.535s\n",
      "  training loss:\t\t0.020078\n",
      "Epoch 955 of 1000 took 1.539s\n",
      "  training loss:\t\t0.020069\n",
      "Epoch 956 of 1000 took 1.535s\n",
      "  training loss:\t\t0.020037\n",
      "Epoch 957 of 1000 took 1.537s\n",
      "  training loss:\t\t0.020031\n",
      "Epoch 958 of 1000 took 1.537s\n",
      "  training loss:\t\t0.020004\n",
      "Epoch 959 of 1000 took 1.537s\n",
      "  training loss:\t\t0.020004\n",
      "Epoch 960 of 1000 took 1.535s\n",
      "  training loss:\t\t0.019984\n",
      "Epoch 961 of 1000 took 1.537s\n",
      "  training loss:\t\t0.019996\n",
      "Epoch 962 of 1000 took 1.537s\n",
      "  training loss:\t\t0.019987\n",
      "Epoch 963 of 1000 took 1.577s\n",
      "  training loss:\t\t0.020016\n",
      "Epoch 964 of 1000 took 1.541s\n",
      "  training loss:\t\t0.020024\n",
      "Epoch 965 of 1000 took 1.574s\n",
      "  training loss:\t\t0.020069\n",
      "Epoch 966 of 1000 took 1.544s\n",
      "  training loss:\t\t0.020090\n",
      "Epoch 967 of 1000 took 1.540s\n",
      "  training loss:\t\t0.020138\n",
      "Epoch 968 of 1000 took 1.670s\n",
      "  training loss:\t\t0.020159\n",
      "Epoch 969 of 1000 took 1.663s\n",
      "  training loss:\t\t0.020189\n",
      "Epoch 970 of 1000 took 1.630s\n",
      "  training loss:\t\t0.020196\n",
      "Epoch 971 of 1000 took 1.561s\n",
      "  training loss:\t\t0.020198\n",
      "Epoch 972 of 1000 took 1.655s\n",
      "  training loss:\t\t0.020186\n",
      "Epoch 973 of 1000 took 1.574s\n",
      "  training loss:\t\t0.020164\n",
      "Epoch 974 of 1000 took 1.547s\n",
      "  training loss:\t\t0.020139\n",
      "Epoch 975 of 1000 took 1.631s\n",
      "  training loss:\t\t0.020106\n",
      "Epoch 976 of 1000 took 1.590s\n",
      "  training loss:\t\t0.020076\n",
      "Epoch 977 of 1000 took 1.575s\n",
      "  training loss:\t\t0.020044\n",
      "Epoch 978 of 1000 took 1.544s\n",
      "  training loss:\t\t0.020017\n",
      "Epoch 979 of 1000 took 1.593s\n",
      "  training loss:\t\t0.019991\n",
      "Epoch 980 of 1000 took 1.542s\n",
      "  training loss:\t\t0.019970\n",
      "Epoch 981 of 1000 took 1.679s\n",
      "  training loss:\t\t0.019953\n",
      "Epoch 982 of 1000 took 1.564s\n",
      "  training loss:\t\t0.019937\n",
      "Epoch 983 of 1000 took 1.591s\n",
      "  training loss:\t\t0.019928\n",
      "Epoch 984 of 1000 took 1.618s\n",
      "  training loss:\t\t0.019917\n",
      "Epoch 985 of 1000 took 1.537s\n",
      "  training loss:\t\t0.019917\n",
      "Epoch 986 of 1000 took 1.540s\n",
      "  training loss:\t\t0.019913\n",
      "Epoch 987 of 1000 took 1.538s\n",
      "  training loss:\t\t0.019925\n",
      "Epoch 988 of 1000 took 1.546s\n",
      "  training loss:\t\t0.019930\n",
      "Epoch 989 of 1000 took 1.578s\n",
      "  training loss:\t\t0.019959\n",
      "Epoch 990 of 1000 took 1.544s\n",
      "  training loss:\t\t0.019976\n",
      "Epoch 991 of 1000 took 1.548s\n",
      "  training loss:\t\t0.020030\n",
      "Epoch 992 of 1000 took 1.567s\n",
      "  training loss:\t\t0.020064\n",
      "Epoch 993 of 1000 took 1.538s\n",
      "  training loss:\t\t0.020149\n",
      "Epoch 994 of 1000 took 1.584s\n",
      "  training loss:\t\t0.020203\n",
      "Epoch 995 of 1000 took 1.582s\n",
      "  training loss:\t\t0.020322\n",
      "Epoch 996 of 1000 took 1.541s\n",
      "  training loss:\t\t0.020397\n",
      "Epoch 997 of 1000 took 1.626s\n",
      "  training loss:\t\t0.020532\n",
      "Epoch 998 of 1000 took 1.652s\n",
      "  training loss:\t\t0.020617\n",
      "Epoch 999 of 1000 took 1.634s\n",
      "  training loss:\t\t0.020715\n",
      "Epoch 1000 of 1000 took 1.686s\n",
      "  training loss:\t\t0.020797\n"
     ]
    }
   ],
   "source": [
    "# num_epochs = 1000\n",
    "# updates = lasagne.updates.nesterov_momentum(\n",
    "#     loss, params, learning_rate=0.005, momentum=0.975)\n",
    "# # updates = lasagne.updates.rmsprop(\n",
    "# #     loss, params[10:12], learning_rate=0.01)\n",
    "# train_fn = theano.function([input_var, target_var], loss, updates=updates,on_unused_input='warn')\n",
    "# print(\"Starting training...\")\n",
    "# best_err = 0.024\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_err = 0\n",
    "#     train_batches = 0\n",
    "#     start_time = time.time()\n",
    "#     for batch in iterate_minibatches(data, data_out, 500, shuffle=False):\n",
    "#         inputs, targets = batch\n",
    "#         train_err += train_fn(inputs, targets)\n",
    "#         train_batches += 1\n",
    "#         # Then we print the results for this epoch:\n",
    "#     print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "#         epoch + 1, num_epochs, time.time() - start_time))\n",
    "#     print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "#     if train_err / train_batches < best_err:\n",
    "#         best_err = train_err / train_batches\n",
    "#         np.savez('ARE_MNIST.npz', *lasagne.layers.get_all_param_values(network))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with np.load('ARE_MNIST.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "lasagne.layers.set_all_param_values(network, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_images(X1,X2):\n",
    "    image1 = Image.fromarray(get_picture_array(255 * X1))\n",
    "    new_size = (image1.size[0] * 2, image1.size[1])\n",
    "    new_im = Image.new('L', new_size)\n",
    "    new_im.paste(image1, (0,0))\n",
    "    image2 = Image.fromarray(get_picture_array(255 * X2))\n",
    "    new_im.paste(image2, (image1.size[0],0))\n",
    "    new_im.save('temp.png', format=\"PNG\")\n",
    "    return IPImage('temp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_func = theano.function([input_var], [lasagne.layers.get_output(network)])\n",
    "data_pred = np.zeros((5000,784))\n",
    "i = 0\n",
    "for batch in iterate_minibatches(data, data_out, 1000, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    data_pred[1000*i:1000*(i+1),:] = output_func(inputs)[0]\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare the original image with the reconstructed image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAAA4CAAAAAAAgCOWAAADaElEQVR4nNXYTWgcZRjA8d9MdtM2\nH01SbYmxVlHbYFrFlooKxeRQLS1eehCLKCrqwYPoqeilevIgCOrBHgoietCTJAoWaYVIq1KxVG1V\nBC2xlH6sEpI0Zje7OzseuizZ1OysOWTa5zYP/Oed/7zfT2CRsdqsoggEQi1CgYqySIxAxjIZsaJC\nNRdaLlxsg4uNYLHgSnllce01l18U1zIQCsUqc3ItS2+YWSxYmuNHLBDM86OiMo+KriHD+r7KaJM1\nrTDP8cq4dgyzyiK06jHgHptlnPCJ367ot/pIax6ul/WAd+d83Yg9Sg3AXgWhXrsNWatDK6b97m1f\nyTewTMNwwFMeEeoT1I2xD7zk0oJgv053echmHcIqVxKZdNzXjhpb4P+kYThiV+2hfhYN+mZB8G5r\nbXWvXpPGnNNlkzW6ZOXlfOSdq8UwwyG7kPNe1fB+g02AgcC4Xxz2pT+VrfOsnVplFR12YMERnobh\nfsMouVhNdTqlD8O+bwAW5Vz0rdMmVHDOBQHKTnrD1IJcGoaRs3WpHXrAWcUGYMm4SbP+URELtOvX\nJWPcq8YbcOnvFo96zgqwLwGNRMhqkdVnr+3aREYdb0ila/iYV9wmC35ouFfQKiMWWuZGW22zSbuK\ncfuUG3JpGN7sCdvBtupaOuVlnys0BNu1W+lW/TZapd0KFHznfEKDS2+4yYh185JHHEgEO603oN9q\n3UKhkrIpH1dvGwtHGn0Y1C4YYfUs8rCdDiaAbdbo1WnWRQWBdp0mnUlscOkNTxnyuC9qY/IZLzQF\nxgp+dNQxOSvcaY8NKmYSuTT68IzX5yRea9Jwys9OyimIhS4YdIc2XYlc+rvFjibBGRPOV3fMirxu\nyxLnIGkYLq9bM5/2VpNgXHfDv8EtAgV/JXJLb3h97UTTY5c3tSEvnwh2uE7OhLLAak/qlncica9I\nw3Bv7Yb0oC1ijNpvNBHstcWQaWfdbshNWv3toNlEbukNn697zPnMi018J502Wq9DRUZW5JwPfZpY\nw0hzpfnDjCMOONUkmFcQqSiakHPM+04nnPMux9LfgHN6DDtkpHZ7ai7us0GfVcb85FdTTdmRXp3m\n/8c6oUiLaZcUmxidlyO8CvbDZmOmWjEt19VNG0cgew0ZBtU6fqSkrCKu1YP/2zcQyKZR1f8XNZMI\nRf82/PoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_images(data[3007],data_pred[3007])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'action'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_layers = lasagne.layers.get_all_layers(network)\n",
    "action_layer = all_layers[9]\n",
    "action_layer.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_W = action_layer.W.get_value()\n",
    "original_b = action_layer.b.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate initial weights for the four action transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action1_W = np.random.randn(original_W.shape[0],original_W.shape[1]).astype(np.float32)\n",
    "action1_b = np.random.randn(original_b.shape[0]).astype(np.float32)\n",
    "action2_W = np.random.randn(original_W.shape[0],original_W.shape[1]).astype(np.float32)\n",
    "action2_b = np.random.randn(original_b.shape[0]).astype(np.float32)\n",
    "action3_W = np.random.randn(original_W.shape[0],original_W.shape[1]).astype(np.float32)\n",
    "action3_b = np.random.randn(original_b.shape[0]).astype(np.float32)\n",
    "action4_W = np.random.randn(original_W.shape[0],original_W.shape[1]).astype(np.float32)\n",
    "action4_b = np.random.randn(original_b.shape[0]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change the weights at the action layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action_layer.W.set_value(action1_W)\n",
    "action_layer.b.set_value(action1_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[action.W, action.b]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[10:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fix other weights, updates the action layer using the 1000 action1 trainning image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1 of 100 took 0.213s\n",
      "  training loss:\t\t0.021687\n",
      "Epoch 2 of 100 took 0.199s\n",
      "  training loss:\t\t0.021666\n",
      "Epoch 3 of 100 took 0.195s\n",
      "  training loss:\t\t0.021665\n",
      "Epoch 4 of 100 took 0.192s\n",
      "  training loss:\t\t0.021665\n",
      "Epoch 5 of 100 took 0.195s\n",
      "  training loss:\t\t0.021664\n",
      "Epoch 6 of 100 took 0.192s\n",
      "  training loss:\t\t0.021664\n",
      "Epoch 7 of 100 took 0.195s\n",
      "  training loss:\t\t0.021664\n",
      "Epoch 8 of 100 took 0.192s\n",
      "  training loss:\t\t0.021664\n",
      "Epoch 9 of 100 took 0.195s\n",
      "  training loss:\t\t0.021663\n",
      "Epoch 10 of 100 took 0.192s\n",
      "  training loss:\t\t0.021663\n",
      "Epoch 11 of 100 took 0.195s\n",
      "  training loss:\t\t0.021663\n",
      "Epoch 12 of 100 took 0.193s\n",
      "  training loss:\t\t0.021663\n",
      "Epoch 13 of 100 took 0.195s\n",
      "  training loss:\t\t0.021663\n",
      "Epoch 14 of 100 took 0.192s\n",
      "  training loss:\t\t0.021663\n",
      "Epoch 15 of 100 took 0.196s\n",
      "  training loss:\t\t0.021663\n",
      "Epoch 16 of 100 took 0.192s\n",
      "  training loss:\t\t0.021663\n",
      "Epoch 17 of 100 took 0.195s\n",
      "  training loss:\t\t0.021663\n",
      "Epoch 18 of 100 took 0.192s\n",
      "  training loss:\t\t0.021663\n",
      "Epoch 19 of 100 took 0.195s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 20 of 100 took 0.192s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 21 of 100 took 0.195s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 22 of 100 took 0.193s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 23 of 100 took 0.195s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 24 of 100 took 0.192s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 25 of 100 took 0.195s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 26 of 100 took 0.192s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 27 of 100 took 0.195s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 28 of 100 took 0.192s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 29 of 100 took 0.196s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 30 of 100 took 0.193s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 31 of 100 took 0.214s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 32 of 100 took 0.203s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 33 of 100 took 0.217s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 34 of 100 took 0.204s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 35 of 100 took 0.226s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 36 of 100 took 0.195s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 37 of 100 took 0.196s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 38 of 100 took 0.193s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 39 of 100 took 0.196s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 40 of 100 took 0.193s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 41 of 100 took 0.196s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 42 of 100 took 0.219s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 43 of 100 took 0.213s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 44 of 100 took 0.195s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 45 of 100 took 0.195s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 46 of 100 took 0.193s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 47 of 100 took 0.195s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 48 of 100 took 0.192s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 49 of 100 took 0.200s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 50 of 100 took 0.222s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 51 of 100 took 0.202s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 52 of 100 took 0.208s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 53 of 100 took 0.221s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 54 of 100 took 0.215s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 55 of 100 took 0.218s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 56 of 100 took 0.204s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 57 of 100 took 0.201s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 58 of 100 took 0.205s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 59 of 100 took 0.205s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 60 of 100 took 0.205s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 61 of 100 took 0.222s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 62 of 100 took 0.206s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 63 of 100 took 0.199s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 64 of 100 took 0.223s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 65 of 100 took 0.203s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 66 of 100 took 0.199s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 67 of 100 took 0.224s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 68 of 100 took 0.211s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 69 of 100 took 0.201s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 70 of 100 took 0.216s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 71 of 100 took 0.212s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 72 of 100 took 0.198s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 73 of 100 took 0.220s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 74 of 100 took 0.206s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 75 of 100 took 0.219s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 76 of 100 took 0.227s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 77 of 100 took 0.196s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 78 of 100 took 0.194s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 79 of 100 took 0.193s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 80 of 100 took 0.194s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 81 of 100 took 0.193s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 82 of 100 took 0.194s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 83 of 100 took 0.193s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 84 of 100 took 0.194s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 85 of 100 took 0.192s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 86 of 100 took 0.194s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 87 of 100 took 0.209s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 88 of 100 took 0.224s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 89 of 100 took 0.202s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 90 of 100 took 0.223s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 91 of 100 took 0.194s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 92 of 100 took 0.194s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 93 of 100 took 0.193s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 94 of 100 took 0.194s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 95 of 100 took 0.197s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 96 of 100 took 0.221s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 97 of 100 took 0.194s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 98 of 100 took 0.204s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 99 of 100 took 0.213s\n",
      "  training loss:\t\t0.021661\n",
      "Epoch 100 of 100 took 0.193s\n",
      "  training loss:\t\t0.021661\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "# updates = lasagne.updates.nesterov_momentum(\n",
    "#     loss, params[10:12], learning_rate=0.01, momentum=0.9)\n",
    "updates = lasagne.updates.rmsprop(\n",
    "    loss, params[10:12], learning_rate=0.001)\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates,on_unused_input='warn')\n",
    "print(\"Starting training...\")\n",
    "best_err = 0.021664\n",
    "for epoch in range(num_epochs):\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(data0, data1_out, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    if train_err / train_batches < best_err:\n",
    "        best_err = train_err / train_batches\n",
    "        action1_W  = action_layer.W.get_value()\n",
    "        action1_b = action_layer.b.get_value()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('action1_weight.npz',action1_W,action1_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate test set and compare the predicted image after action transform with the ground truth image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = X[2000:2500,:,:,:]\n",
    "\n",
    "output_func = theano.function([input_var], [lasagne.layers.get_output(network)])\n",
    "a1_pred = np.zeros((500,784))\n",
    "a1_pred = output_func(test_set)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = np.arange(500)\n",
    "np.random.shuffle(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare_images_array(index):\n",
    "    length = len(index)\n",
    "    original_image = Image.fromarray(get_picture_array(255 * test_set[index[0]]))\n",
    "    width,height = original_image.size\n",
    "    new_size = (width * length, height*2)\n",
    "    new_im = Image.new('L', new_size)\n",
    "    for i in range(length):\n",
    "        a1_image = Image.fromarray(get_picture_array(255 * action1(test_set)[index[i]]))\n",
    "        a1_pred_image = Image.fromarray(get_picture_array(255 * a1_pred[index[i]]))\n",
    "        new_im.paste(a1_image, (i*width,0))\n",
    "        new_im.paste(a1_pred_image, (i*width,height))\n",
    "    new_im.save('test.png', format=\"PNG\")\n",
    "    return IPImage('test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first row is the ground truth image after action1\n",
    "### second row is the predicted image given the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGAAAABwCAAAAAB0/Fn9AAAvn0lEQVR4nO3dd5xdVb3//+dp02cy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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_images_array(index[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similarly we can learn other transforms at the action layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_layer.W.set_value(action2_W)\n",
    "action_layer.b.set_value(action2_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1 of 1000 took 0.216s\n",
      "  training loss:\t\t0.025591\n",
      "Epoch 2 of 1000 took 0.197s\n",
      "  training loss:\t\t0.025418\n",
      "Epoch 3 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025371\n",
      "Epoch 4 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025354\n",
      "Epoch 5 of 1000 took 0.194s\n",
      "  training loss:\t\t0.025346\n",
      "Epoch 6 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025341\n",
      "Epoch 7 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025336\n",
      "Epoch 8 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025332\n",
      "Epoch 9 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025328\n",
      "Epoch 10 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025324\n",
      "Epoch 11 of 1000 took 0.196s\n",
      "  training loss:\t\t0.025320\n",
      "Epoch 12 of 1000 took 0.218s\n",
      "  training loss:\t\t0.025316\n",
      "Epoch 13 of 1000 took 0.202s\n",
      "  training loss:\t\t0.025312\n",
      "Epoch 14 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025309\n",
      "Epoch 15 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025305\n",
      "Epoch 16 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025301\n",
      "Epoch 17 of 1000 took 0.202s\n",
      "  training loss:\t\t0.025297\n",
      "Epoch 18 of 1000 took 0.224s\n",
      "  training loss:\t\t0.025293\n",
      "Epoch 19 of 1000 took 0.214s\n",
      "  training loss:\t\t0.025290\n",
      "Epoch 20 of 1000 took 0.214s\n",
      "  training loss:\t\t0.025286\n",
      "Epoch 21 of 1000 took 0.194s\n",
      "  training loss:\t\t0.025282\n",
      "Epoch 22 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025278\n",
      "Epoch 23 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025274\n",
      "Epoch 24 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025271\n",
      "Epoch 25 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025267\n",
      "Epoch 26 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025263\n",
      "Epoch 27 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025259\n",
      "Epoch 28 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025256\n",
      "Epoch 29 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025252\n",
      "Epoch 30 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025248\n",
      "Epoch 31 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025244\n",
      "Epoch 32 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025240\n",
      "Epoch 33 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025236\n",
      "Epoch 34 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025233\n",
      "Epoch 35 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025229\n",
      "Epoch 36 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025225\n",
      "Epoch 37 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025221\n",
      "Epoch 38 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025217\n",
      "Epoch 39 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025213\n",
      "Epoch 40 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025209\n",
      "Epoch 41 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025205\n",
      "Epoch 42 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025202\n",
      "Epoch 43 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025198\n",
      "Epoch 44 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025194\n",
      "Epoch 45 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025190\n",
      "Epoch 46 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025186\n",
      "Epoch 47 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025182\n",
      "Epoch 48 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025178\n",
      "Epoch 49 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025174\n",
      "Epoch 50 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025170\n",
      "Epoch 51 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025166\n",
      "Epoch 52 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025162\n",
      "Epoch 53 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025158\n",
      "Epoch 54 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025154\n",
      "Epoch 55 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025150\n",
      "Epoch 56 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025146\n",
      "Epoch 57 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025142\n",
      "Epoch 58 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025138\n",
      "Epoch 59 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025134\n",
      "Epoch 60 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025130\n",
      "Epoch 61 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025126\n",
      "Epoch 62 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025121\n",
      "Epoch 63 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025117\n",
      "Epoch 64 of 1000 took 0.194s\n",
      "  training loss:\t\t0.025113\n",
      "Epoch 65 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025109\n",
      "Epoch 66 of 1000 took 0.194s\n",
      "  training loss:\t\t0.025105\n",
      "Epoch 67 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025101\n",
      "Epoch 68 of 1000 took 0.194s\n",
      "  training loss:\t\t0.025097\n",
      "Epoch 69 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025092\n",
      "Epoch 70 of 1000 took 0.194s\n",
      "  training loss:\t\t0.025088\n",
      "Epoch 71 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025084\n",
      "Epoch 72 of 1000 took 0.194s\n",
      "  training loss:\t\t0.025080\n",
      "Epoch 73 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025076\n",
      "Epoch 74 of 1000 took 0.194s\n",
      "  training loss:\t\t0.025071\n",
      "Epoch 75 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025067\n",
      "Epoch 76 of 1000 took 0.194s\n",
      "  training loss:\t\t0.025063\n",
      "Epoch 77 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025059\n",
      "Epoch 78 of 1000 took 0.194s\n",
      "  training loss:\t\t0.025054\n",
      "Epoch 79 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025050\n",
      "Epoch 80 of 1000 took 0.194s\n",
      "  training loss:\t\t0.025046\n",
      "Epoch 81 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025042\n",
      "Epoch 82 of 1000 took 0.194s\n",
      "  training loss:\t\t0.025037\n",
      "Epoch 83 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025033\n",
      "Epoch 84 of 1000 took 0.195s\n",
      "  training loss:\t\t0.025029\n",
      "Epoch 85 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025024\n",
      "Epoch 86 of 1000 took 0.194s\n",
      "  training loss:\t\t0.025020\n",
      "Epoch 87 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025016\n",
      "Epoch 88 of 1000 took 0.194s\n",
      "  training loss:\t\t0.025011\n",
      "Epoch 89 of 1000 took 0.192s\n",
      "  training loss:\t\t0.025007\n",
      "Epoch 90 of 1000 took 0.194s\n",
      "  training loss:\t\t0.025002\n",
      "Epoch 91 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024998\n",
      "Epoch 92 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024994\n",
      "Epoch 93 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024989\n",
      "Epoch 94 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024985\n",
      "Epoch 95 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024980\n",
      "Epoch 96 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024976\n",
      "Epoch 97 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024971\n",
      "Epoch 98 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024967\n",
      "Epoch 99 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024963\n",
      "Epoch 100 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024958\n",
      "Epoch 101 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024954\n",
      "Epoch 102 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024949\n",
      "Epoch 103 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024944\n",
      "Epoch 104 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024940\n",
      "Epoch 105 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024935\n",
      "Epoch 106 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024931\n",
      "Epoch 107 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024926\n",
      "Epoch 108 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024922\n",
      "Epoch 109 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024917\n",
      "Epoch 110 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024913\n",
      "Epoch 111 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024908\n",
      "Epoch 112 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024903\n",
      "Epoch 113 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024899\n",
      "Epoch 114 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024894\n",
      "Epoch 115 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024889\n",
      "Epoch 116 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024885\n",
      "Epoch 117 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024880\n",
      "Epoch 118 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024875\n",
      "Epoch 119 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024871\n",
      "Epoch 120 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024866\n",
      "Epoch 121 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024861\n",
      "Epoch 122 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024857\n",
      "Epoch 123 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024852\n",
      "Epoch 124 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024847\n",
      "Epoch 125 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024842\n",
      "Epoch 126 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024838\n",
      "Epoch 127 of 1000 took 0.193s\n",
      "  training loss:\t\t0.024833\n",
      "Epoch 128 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024828\n",
      "Epoch 129 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024823\n",
      "Epoch 130 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024819\n",
      "Epoch 131 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024814\n",
      "Epoch 132 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024809\n",
      "Epoch 133 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024804\n",
      "Epoch 134 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024799\n",
      "Epoch 135 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024795\n",
      "Epoch 136 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024790\n",
      "Epoch 137 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024785\n",
      "Epoch 138 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024780\n",
      "Epoch 139 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024775\n",
      "Epoch 140 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024770\n",
      "Epoch 141 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024765\n",
      "Epoch 142 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024760\n",
      "Epoch 143 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024755\n",
      "Epoch 144 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024751\n",
      "Epoch 145 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024746\n",
      "Epoch 146 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024741\n",
      "Epoch 147 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024736\n",
      "Epoch 148 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024731\n",
      "Epoch 149 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024726\n",
      "Epoch 150 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024721\n",
      "Epoch 151 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024716\n",
      "Epoch 152 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024711\n",
      "Epoch 153 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024706\n",
      "Epoch 154 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024701\n",
      "Epoch 155 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024696\n",
      "Epoch 156 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024691\n",
      "Epoch 157 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024686\n",
      "Epoch 158 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024681\n",
      "Epoch 159 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024676\n",
      "Epoch 160 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024671\n",
      "Epoch 161 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024666\n",
      "Epoch 162 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024661\n",
      "Epoch 163 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024656\n",
      "Epoch 164 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024650\n",
      "Epoch 165 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024645\n",
      "Epoch 166 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024640\n",
      "Epoch 167 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024635\n",
      "Epoch 168 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024630\n",
      "Epoch 169 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024625\n",
      "Epoch 170 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024620\n",
      "Epoch 171 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024615\n",
      "Epoch 172 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024610\n",
      "Epoch 173 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024604\n",
      "Epoch 174 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024599\n",
      "Epoch 175 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024594\n",
      "Epoch 176 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024589\n",
      "Epoch 177 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024584\n",
      "Epoch 178 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024579\n",
      "Epoch 179 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024573\n",
      "Epoch 180 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024568\n",
      "Epoch 181 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024563\n",
      "Epoch 182 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024558\n",
      "Epoch 183 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024553\n",
      "Epoch 184 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024547\n",
      "Epoch 185 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024542\n",
      "Epoch 186 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024537\n",
      "Epoch 187 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024532\n",
      "Epoch 188 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024526\n",
      "Epoch 189 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024521\n",
      "Epoch 190 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024516\n",
      "Epoch 191 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024511\n",
      "Epoch 192 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024505\n",
      "Epoch 193 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024500\n",
      "Epoch 194 of 1000 took 0.206s\n",
      "  training loss:\t\t0.024495\n",
      "Epoch 195 of 1000 took 0.197s\n",
      "  training loss:\t\t0.024490\n",
      "Epoch 196 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024484\n",
      "Epoch 197 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024479\n",
      "Epoch 198 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024474\n",
      "Epoch 199 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024468\n",
      "Epoch 200 of 1000 took 0.196s\n",
      "  training loss:\t\t0.024463\n",
      "Epoch 201 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024458\n",
      "Epoch 202 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024453\n",
      "Epoch 203 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024447\n",
      "Epoch 204 of 1000 took 0.196s\n",
      "  training loss:\t\t0.024442\n",
      "Epoch 205 of 1000 took 0.198s\n",
      "  training loss:\t\t0.024437\n",
      "Epoch 206 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024431\n",
      "Epoch 207 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024426\n",
      "Epoch 208 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024421\n",
      "Epoch 209 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024415\n",
      "Epoch 210 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024410\n",
      "Epoch 211 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024405\n",
      "Epoch 212 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024399\n",
      "Epoch 213 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024394\n",
      "Epoch 214 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024389\n",
      "Epoch 215 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024383\n",
      "Epoch 216 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024378\n",
      "Epoch 217 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024373\n",
      "Epoch 218 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024367\n",
      "Epoch 219 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024362\n",
      "Epoch 220 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024357\n",
      "Epoch 221 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024351\n",
      "Epoch 222 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024346\n",
      "Epoch 223 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024341\n",
      "Epoch 224 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024335\n",
      "Epoch 225 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024330\n",
      "Epoch 226 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024325\n",
      "Epoch 227 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024319\n",
      "Epoch 228 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024314\n",
      "Epoch 229 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024309\n",
      "Epoch 230 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024303\n",
      "Epoch 231 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024298\n",
      "Epoch 232 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024293\n",
      "Epoch 233 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024287\n",
      "Epoch 234 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024282\n",
      "Epoch 235 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024277\n",
      "Epoch 236 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024271\n",
      "Epoch 237 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024266\n",
      "Epoch 238 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024261\n",
      "Epoch 239 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024255\n",
      "Epoch 240 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024250\n",
      "Epoch 241 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024245\n",
      "Epoch 242 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024239\n",
      "Epoch 243 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024234\n",
      "Epoch 244 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024229\n",
      "Epoch 245 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024223\n",
      "Epoch 246 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024218\n",
      "Epoch 247 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024213\n",
      "Epoch 248 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024207\n",
      "Epoch 249 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024202\n",
      "Epoch 250 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024197\n",
      "Epoch 251 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024191\n",
      "Epoch 252 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024186\n",
      "Epoch 253 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024181\n",
      "Epoch 254 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024176\n",
      "Epoch 255 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024170\n",
      "Epoch 256 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024165\n",
      "Epoch 257 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024160\n",
      "Epoch 258 of 1000 took 0.196s\n",
      "  training loss:\t\t0.024154\n",
      "Epoch 259 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024149\n",
      "Epoch 260 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024144\n",
      "Epoch 261 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024138\n",
      "Epoch 262 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024133\n",
      "Epoch 263 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024128\n",
      "Epoch 264 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024123\n",
      "Epoch 265 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024117\n",
      "Epoch 266 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024112\n",
      "Epoch 267 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024107\n",
      "Epoch 268 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024102\n",
      "Epoch 269 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024096\n",
      "Epoch 270 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024091\n",
      "Epoch 271 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024086\n",
      "Epoch 272 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024081\n",
      "Epoch 273 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024075\n",
      "Epoch 274 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024070\n",
      "Epoch 275 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024065\n",
      "Epoch 276 of 1000 took 0.194s\n",
      "  training loss:\t\t0.024060\n",
      "Epoch 277 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024054\n",
      "Epoch 278 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024049\n",
      "Epoch 279 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024044\n",
      "Epoch 280 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024039\n",
      "Epoch 281 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024033\n",
      "Epoch 282 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024028\n",
      "Epoch 283 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024023\n",
      "Epoch 284 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024018\n",
      "Epoch 285 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024013\n",
      "Epoch 286 of 1000 took 0.195s\n",
      "  training loss:\t\t0.024007\n",
      "Epoch 287 of 1000 took 0.192s\n",
      "  training loss:\t\t0.024002\n",
      "Epoch 288 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023997\n",
      "Epoch 289 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023992\n",
      "Epoch 290 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023987\n",
      "Epoch 291 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023981\n",
      "Epoch 292 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023976\n",
      "Epoch 293 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023971\n",
      "Epoch 294 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023966\n",
      "Epoch 295 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023961\n",
      "Epoch 296 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023955\n",
      "Epoch 297 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023950\n",
      "Epoch 298 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023945\n",
      "Epoch 299 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023940\n",
      "Epoch 300 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023935\n",
      "Epoch 301 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023930\n",
      "Epoch 302 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023924\n",
      "Epoch 303 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023919\n",
      "Epoch 304 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023914\n",
      "Epoch 305 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023909\n",
      "Epoch 306 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023904\n",
      "Epoch 307 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023899\n",
      "Epoch 308 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023894\n",
      "Epoch 309 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023888\n",
      "Epoch 310 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023883\n",
      "Epoch 311 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023878\n",
      "Epoch 312 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023873\n",
      "Epoch 313 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023868\n",
      "Epoch 314 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023863\n",
      "Epoch 315 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023858\n",
      "Epoch 316 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023853\n",
      "Epoch 317 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023847\n",
      "Epoch 318 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023842\n",
      "Epoch 319 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023837\n",
      "Epoch 320 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023832\n",
      "Epoch 321 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023827\n",
      "Epoch 322 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023822\n",
      "Epoch 323 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023817\n",
      "Epoch 324 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023812\n",
      "Epoch 325 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023807\n",
      "Epoch 326 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023802\n",
      "Epoch 327 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023797\n",
      "Epoch 328 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023792\n",
      "Epoch 329 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023786\n",
      "Epoch 330 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023781\n",
      "Epoch 331 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023776\n",
      "Epoch 332 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023771\n",
      "Epoch 333 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023766\n",
      "Epoch 334 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023761\n",
      "Epoch 335 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023756\n",
      "Epoch 336 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023751\n",
      "Epoch 337 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023746\n",
      "Epoch 338 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023741\n",
      "Epoch 339 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023736\n",
      "Epoch 340 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023731\n",
      "Epoch 341 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023726\n",
      "Epoch 342 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023721\n",
      "Epoch 343 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023716\n",
      "Epoch 344 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023711\n",
      "Epoch 345 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023706\n",
      "Epoch 346 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023701\n",
      "Epoch 347 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023696\n",
      "Epoch 348 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023691\n",
      "Epoch 349 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023686\n",
      "Epoch 350 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023681\n",
      "Epoch 351 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023676\n",
      "Epoch 352 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023671\n",
      "Epoch 353 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023666\n",
      "Epoch 354 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023661\n",
      "Epoch 355 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023656\n",
      "Epoch 356 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023651\n",
      "Epoch 357 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023646\n",
      "Epoch 358 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023641\n",
      "Epoch 359 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023636\n",
      "Epoch 360 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023632\n",
      "Epoch 361 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023627\n",
      "Epoch 362 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023622\n",
      "Epoch 363 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023617\n",
      "Epoch 364 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023612\n",
      "Epoch 365 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023607\n",
      "Epoch 366 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023602\n",
      "Epoch 367 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023597\n",
      "Epoch 368 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023592\n",
      "Epoch 369 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023587\n",
      "Epoch 370 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023582\n",
      "Epoch 371 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023577\n",
      "Epoch 372 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023573\n",
      "Epoch 373 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023568\n",
      "Epoch 374 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023563\n",
      "Epoch 375 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023558\n",
      "Epoch 376 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023553\n",
      "Epoch 377 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023548\n",
      "Epoch 378 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023543\n",
      "Epoch 379 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023538\n",
      "Epoch 380 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023534\n",
      "Epoch 381 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023529\n",
      "Epoch 382 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023524\n",
      "Epoch 383 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023519\n",
      "Epoch 384 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023514\n",
      "Epoch 385 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023509\n",
      "Epoch 386 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023505\n",
      "Epoch 387 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023500\n",
      "Epoch 388 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023495\n",
      "Epoch 389 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023490\n",
      "Epoch 390 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023485\n",
      "Epoch 391 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023480\n",
      "Epoch 392 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023476\n",
      "Epoch 393 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023471\n",
      "Epoch 394 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023466\n",
      "Epoch 395 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023461\n",
      "Epoch 396 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023456\n",
      "Epoch 397 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023452\n",
      "Epoch 398 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023447\n",
      "Epoch 399 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023442\n",
      "Epoch 400 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023437\n",
      "Epoch 401 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023433\n",
      "Epoch 402 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023428\n",
      "Epoch 403 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023423\n",
      "Epoch 404 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023418\n",
      "Epoch 405 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023414\n",
      "Epoch 406 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023409\n",
      "Epoch 407 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023404\n",
      "Epoch 408 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023399\n",
      "Epoch 409 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023395\n",
      "Epoch 410 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023390\n",
      "Epoch 411 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023385\n",
      "Epoch 412 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023380\n",
      "Epoch 413 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023376\n",
      "Epoch 414 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023371\n",
      "Epoch 415 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023366\n",
      "Epoch 416 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023362\n",
      "Epoch 417 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023357\n",
      "Epoch 418 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023352\n",
      "Epoch 419 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023348\n",
      "Epoch 420 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023343\n",
      "Epoch 421 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023338\n",
      "Epoch 422 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023334\n",
      "Epoch 423 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023329\n",
      "Epoch 424 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023324\n",
      "Epoch 425 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023320\n",
      "Epoch 426 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023315\n",
      "Epoch 427 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023310\n",
      "Epoch 428 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023306\n",
      "Epoch 429 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023301\n",
      "Epoch 430 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023296\n",
      "Epoch 431 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023292\n",
      "Epoch 432 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023287\n",
      "Epoch 433 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023282\n",
      "Epoch 434 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023278\n",
      "Epoch 435 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023273\n",
      "Epoch 436 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023269\n",
      "Epoch 437 of 1000 took 0.193s\n",
      "  training loss:\t\t0.023264\n",
      "Epoch 438 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023259\n",
      "Epoch 439 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023255\n",
      "Epoch 440 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023250\n",
      "Epoch 441 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023246\n",
      "Epoch 442 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023241\n",
      "Epoch 443 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023236\n",
      "Epoch 444 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023232\n",
      "Epoch 445 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023227\n",
      "Epoch 446 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023223\n",
      "Epoch 447 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023218\n",
      "Epoch 448 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023214\n",
      "Epoch 449 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023209\n",
      "Epoch 450 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023204\n",
      "Epoch 451 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023200\n",
      "Epoch 452 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023195\n",
      "Epoch 453 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023191\n",
      "Epoch 454 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023186\n",
      "Epoch 455 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023182\n",
      "Epoch 456 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023177\n",
      "Epoch 457 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023173\n",
      "Epoch 458 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023168\n",
      "Epoch 459 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023164\n",
      "Epoch 460 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023159\n",
      "Epoch 461 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023155\n",
      "Epoch 462 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023150\n",
      "Epoch 463 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023146\n",
      "Epoch 464 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023141\n",
      "Epoch 465 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023137\n",
      "Epoch 466 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023132\n",
      "Epoch 467 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023128\n",
      "Epoch 468 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023124\n",
      "Epoch 469 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023119\n",
      "Epoch 470 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023115\n",
      "Epoch 471 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023110\n",
      "Epoch 472 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023106\n",
      "Epoch 473 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023101\n",
      "Epoch 474 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023097\n",
      "Epoch 475 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023092\n",
      "Epoch 476 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023088\n",
      "Epoch 477 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023084\n",
      "Epoch 478 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023079\n",
      "Epoch 479 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023075\n",
      "Epoch 480 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023070\n",
      "Epoch 481 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023066\n",
      "Epoch 482 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023062\n",
      "Epoch 483 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023057\n",
      "Epoch 484 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023053\n",
      "Epoch 485 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023049\n",
      "Epoch 486 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023044\n",
      "Epoch 487 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023040\n",
      "Epoch 488 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023035\n",
      "Epoch 489 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023031\n",
      "Epoch 490 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023027\n",
      "Epoch 491 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023022\n",
      "Epoch 492 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023018\n",
      "Epoch 493 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023014\n",
      "Epoch 494 of 1000 took 0.195s\n",
      "  training loss:\t\t0.023009\n",
      "Epoch 495 of 1000 took 0.192s\n",
      "  training loss:\t\t0.023005\n",
      "Epoch 496 of 1000 took 0.194s\n",
      "  training loss:\t\t0.023001\n",
      "Epoch 497 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022996\n",
      "Epoch 498 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022992\n",
      "Epoch 499 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022988\n",
      "Epoch 500 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022984\n",
      "Epoch 501 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022979\n",
      "Epoch 502 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022975\n",
      "Epoch 503 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022971\n",
      "Epoch 504 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022966\n",
      "Epoch 505 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022962\n",
      "Epoch 506 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022958\n",
      "Epoch 507 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022954\n",
      "Epoch 508 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022949\n",
      "Epoch 509 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022945\n",
      "Epoch 510 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022941\n",
      "Epoch 511 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022937\n",
      "Epoch 512 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022933\n",
      "Epoch 513 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022928\n",
      "Epoch 514 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022924\n",
      "Epoch 515 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022920\n",
      "Epoch 516 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022916\n",
      "Epoch 517 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022912\n",
      "Epoch 518 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022907\n",
      "Epoch 519 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022903\n",
      "Epoch 520 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022899\n",
      "Epoch 521 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022895\n",
      "Epoch 522 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022891\n",
      "Epoch 523 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022886\n",
      "Epoch 524 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022882\n",
      "Epoch 525 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022878\n",
      "Epoch 526 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022874\n",
      "Epoch 527 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022870\n",
      "Epoch 528 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022866\n",
      "Epoch 529 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022862\n",
      "Epoch 530 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022857\n",
      "Epoch 531 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022853\n",
      "Epoch 532 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022849\n",
      "Epoch 533 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022845\n",
      "Epoch 534 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022841\n",
      "Epoch 535 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022837\n",
      "Epoch 536 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022833\n",
      "Epoch 537 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022829\n",
      "Epoch 538 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022825\n",
      "Epoch 539 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022821\n",
      "Epoch 540 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022817\n",
      "Epoch 541 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022813\n",
      "Epoch 542 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022808\n",
      "Epoch 543 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022804\n",
      "Epoch 544 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022800\n",
      "Epoch 545 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022796\n",
      "Epoch 546 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022792\n",
      "Epoch 547 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022788\n",
      "Epoch 548 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022784\n",
      "Epoch 549 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022780\n",
      "Epoch 550 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022776\n",
      "Epoch 551 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022772\n",
      "Epoch 552 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022768\n",
      "Epoch 553 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022764\n",
      "Epoch 554 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022760\n",
      "Epoch 555 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022756\n",
      "Epoch 556 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022752\n",
      "Epoch 557 of 1000 took 0.193s\n",
      "  training loss:\t\t0.022748\n",
      "Epoch 558 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022745\n",
      "Epoch 559 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022741\n",
      "Epoch 560 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022737\n",
      "Epoch 561 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022733\n",
      "Epoch 562 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022729\n",
      "Epoch 563 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022725\n",
      "Epoch 564 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022721\n",
      "Epoch 565 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022717\n",
      "Epoch 566 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022713\n",
      "Epoch 567 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022709\n",
      "Epoch 568 of 1000 took 0.196s\n",
      "  training loss:\t\t0.022705\n",
      "Epoch 569 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022701\n",
      "Epoch 570 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022698\n",
      "Epoch 571 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022694\n",
      "Epoch 572 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022690\n",
      "Epoch 573 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022686\n",
      "Epoch 574 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022682\n",
      "Epoch 575 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022678\n",
      "Epoch 576 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022674\n",
      "Epoch 577 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022671\n",
      "Epoch 578 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022667\n",
      "Epoch 579 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022663\n",
      "Epoch 580 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022659\n",
      "Epoch 581 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022655\n",
      "Epoch 582 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022652\n",
      "Epoch 583 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022648\n",
      "Epoch 584 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022644\n",
      "Epoch 585 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022640\n",
      "Epoch 586 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022636\n",
      "Epoch 587 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022633\n",
      "Epoch 588 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022629\n",
      "Epoch 589 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022625\n",
      "Epoch 590 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022621\n",
      "Epoch 591 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022618\n",
      "Epoch 592 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022614\n",
      "Epoch 593 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022610\n",
      "Epoch 594 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022606\n",
      "Epoch 595 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022603\n",
      "Epoch 596 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022599\n",
      "Epoch 597 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022595\n",
      "Epoch 598 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022592\n",
      "Epoch 599 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022588\n",
      "Epoch 600 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022584\n",
      "Epoch 601 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022580\n",
      "Epoch 602 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022577\n",
      "Epoch 603 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022573\n",
      "Epoch 604 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022569\n",
      "Epoch 605 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022566\n",
      "Epoch 606 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022562\n",
      "Epoch 607 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022559\n",
      "Epoch 608 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022555\n",
      "Epoch 609 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022551\n",
      "Epoch 610 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022548\n",
      "Epoch 611 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022544\n",
      "Epoch 612 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022540\n",
      "Epoch 613 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022537\n",
      "Epoch 614 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022533\n",
      "Epoch 615 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022530\n",
      "Epoch 616 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022526\n",
      "Epoch 617 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022522\n",
      "Epoch 618 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022519\n",
      "Epoch 619 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022515\n",
      "Epoch 620 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022512\n",
      "Epoch 621 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022508\n",
      "Epoch 622 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022505\n",
      "Epoch 623 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022501\n",
      "Epoch 624 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022498\n",
      "Epoch 625 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022494\n",
      "Epoch 626 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022490\n",
      "Epoch 627 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022487\n",
      "Epoch 628 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022483\n",
      "Epoch 629 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022480\n",
      "Epoch 630 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022476\n",
      "Epoch 631 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022473\n",
      "Epoch 632 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022469\n",
      "Epoch 633 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022466\n",
      "Epoch 634 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022462\n",
      "Epoch 635 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022459\n",
      "Epoch 636 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022456\n",
      "Epoch 637 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022452\n",
      "Epoch 638 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022449\n",
      "Epoch 639 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022445\n",
      "Epoch 640 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022442\n",
      "Epoch 641 of 1000 took 0.193s\n",
      "  training loss:\t\t0.022438\n",
      "Epoch 642 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022435\n",
      "Epoch 643 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022432\n",
      "Epoch 644 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022428\n",
      "Epoch 645 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022425\n",
      "Epoch 646 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022421\n",
      "Epoch 647 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022418\n",
      "Epoch 648 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022415\n",
      "Epoch 649 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022411\n",
      "Epoch 650 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022408\n",
      "Epoch 651 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022404\n",
      "Epoch 652 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022401\n",
      "Epoch 653 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022398\n",
      "Epoch 654 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022394\n",
      "Epoch 655 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022391\n",
      "Epoch 656 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022388\n",
      "Epoch 657 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022384\n",
      "Epoch 658 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022381\n",
      "Epoch 659 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022378\n",
      "Epoch 660 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022374\n",
      "Epoch 661 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022371\n",
      "Epoch 662 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022368\n",
      "Epoch 663 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022364\n",
      "Epoch 664 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022361\n",
      "Epoch 665 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022358\n",
      "Epoch 666 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022355\n",
      "Epoch 667 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022351\n",
      "Epoch 668 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022348\n",
      "Epoch 669 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022345\n",
      "Epoch 670 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022342\n",
      "Epoch 671 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022338\n",
      "Epoch 672 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022335\n",
      "Epoch 673 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022332\n",
      "Epoch 674 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022329\n",
      "Epoch 675 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022325\n",
      "Epoch 676 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022322\n",
      "Epoch 677 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022319\n",
      "Epoch 678 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022316\n",
      "Epoch 679 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022313\n",
      "Epoch 680 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022309\n",
      "Epoch 681 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022306\n",
      "Epoch 682 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022303\n",
      "Epoch 683 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022300\n",
      "Epoch 684 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022297\n",
      "Epoch 685 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022294\n",
      "Epoch 686 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022290\n",
      "Epoch 687 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022287\n",
      "Epoch 688 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022284\n",
      "Epoch 689 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022281\n",
      "Epoch 690 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022278\n",
      "Epoch 691 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022275\n",
      "Epoch 692 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022272\n",
      "Epoch 693 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022269\n",
      "Epoch 694 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022265\n",
      "Epoch 695 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022262\n",
      "Epoch 696 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022259\n",
      "Epoch 697 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022256\n",
      "Epoch 698 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022253\n",
      "Epoch 699 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022250\n",
      "Epoch 700 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022247\n",
      "Epoch 701 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022244\n",
      "Epoch 702 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022241\n",
      "Epoch 703 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022238\n",
      "Epoch 704 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022235\n",
      "Epoch 705 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022232\n",
      "Epoch 706 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022229\n",
      "Epoch 707 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022226\n",
      "Epoch 708 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022223\n",
      "Epoch 709 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022220\n",
      "Epoch 710 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022217\n",
      "Epoch 711 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022214\n",
      "Epoch 712 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022211\n",
      "Epoch 713 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022208\n",
      "Epoch 714 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022205\n",
      "Epoch 715 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022202\n",
      "Epoch 716 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022199\n",
      "Epoch 717 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022196\n",
      "Epoch 718 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022193\n",
      "Epoch 719 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022190\n",
      "Epoch 720 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022187\n",
      "Epoch 721 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022184\n",
      "Epoch 722 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022181\n",
      "Epoch 723 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022178\n",
      "Epoch 724 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022175\n",
      "Epoch 725 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022172\n",
      "Epoch 726 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022169\n",
      "Epoch 727 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022166\n",
      "Epoch 728 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022163\n",
      "Epoch 729 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022161\n",
      "Epoch 730 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022158\n",
      "Epoch 731 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022155\n",
      "Epoch 732 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022152\n",
      "Epoch 733 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022149\n",
      "Epoch 734 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022146\n",
      "Epoch 735 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022143\n",
      "Epoch 736 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022140\n",
      "Epoch 737 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022138\n",
      "Epoch 738 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022135\n",
      "Epoch 739 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022132\n",
      "Epoch 740 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022129\n",
      "Epoch 741 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022126\n",
      "Epoch 742 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022123\n",
      "Epoch 743 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022121\n",
      "Epoch 744 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022118\n",
      "Epoch 745 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022115\n",
      "Epoch 746 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022112\n",
      "Epoch 747 of 1000 took 0.193s\n",
      "  training loss:\t\t0.022109\n",
      "Epoch 748 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022107\n",
      "Epoch 749 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022104\n",
      "Epoch 750 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022101\n",
      "Epoch 751 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022098\n",
      "Epoch 752 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022096\n",
      "Epoch 753 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022093\n",
      "Epoch 754 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022090\n",
      "Epoch 755 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022087\n",
      "Epoch 756 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022085\n",
      "Epoch 757 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022082\n",
      "Epoch 758 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022079\n",
      "Epoch 759 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022076\n",
      "Epoch 760 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022074\n",
      "Epoch 761 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022071\n",
      "Epoch 762 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022068\n",
      "Epoch 763 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022066\n",
      "Epoch 764 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022063\n",
      "Epoch 765 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022060\n",
      "Epoch 766 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022058\n",
      "Epoch 767 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022055\n",
      "Epoch 768 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022052\n",
      "Epoch 769 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022050\n",
      "Epoch 770 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022047\n",
      "Epoch 771 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022044\n",
      "Epoch 772 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022042\n",
      "Epoch 773 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022039\n",
      "Epoch 774 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022036\n",
      "Epoch 775 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022034\n",
      "Epoch 776 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022031\n",
      "Epoch 777 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022028\n",
      "Epoch 778 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022026\n",
      "Epoch 779 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022023\n",
      "Epoch 780 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022021\n",
      "Epoch 781 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022018\n",
      "Epoch 782 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022015\n",
      "Epoch 783 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022013\n",
      "Epoch 784 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022010\n",
      "Epoch 785 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022008\n",
      "Epoch 786 of 1000 took 0.194s\n",
      "  training loss:\t\t0.022005\n",
      "Epoch 787 of 1000 took 0.192s\n",
      "  training loss:\t\t0.022003\n",
      "Epoch 788 of 1000 took 0.195s\n",
      "  training loss:\t\t0.022000\n",
      "Epoch 789 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021997\n",
      "Epoch 790 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021995\n",
      "Epoch 791 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021992\n",
      "Epoch 792 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021990\n",
      "Epoch 793 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021987\n",
      "Epoch 794 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021985\n",
      "Epoch 795 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021982\n",
      "Epoch 796 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021980\n",
      "Epoch 797 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021977\n",
      "Epoch 798 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021975\n",
      "Epoch 799 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021972\n",
      "Epoch 800 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021970\n",
      "Epoch 801 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021967\n",
      "Epoch 802 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021965\n",
      "Epoch 803 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021962\n",
      "Epoch 804 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021960\n",
      "Epoch 805 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021957\n",
      "Epoch 806 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021955\n",
      "Epoch 807 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021952\n",
      "Epoch 808 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021950\n",
      "Epoch 809 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021947\n",
      "Epoch 810 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021945\n",
      "Epoch 811 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021943\n",
      "Epoch 812 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021940\n",
      "Epoch 813 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021938\n",
      "Epoch 814 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021935\n",
      "Epoch 815 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021933\n",
      "Epoch 816 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021930\n",
      "Epoch 817 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021928\n",
      "Epoch 818 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021926\n",
      "Epoch 819 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021923\n",
      "Epoch 820 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021921\n",
      "Epoch 821 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021918\n",
      "Epoch 822 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021916\n",
      "Epoch 823 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021914\n",
      "Epoch 824 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021911\n",
      "Epoch 825 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021909\n",
      "Epoch 826 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021907\n",
      "Epoch 827 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021904\n",
      "Epoch 828 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021902\n",
      "Epoch 829 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021900\n",
      "Epoch 830 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021897\n",
      "Epoch 831 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021895\n",
      "Epoch 832 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021893\n",
      "Epoch 833 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021890\n",
      "Epoch 834 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021888\n",
      "Epoch 835 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021886\n",
      "Epoch 836 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021883\n",
      "Epoch 837 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021881\n",
      "Epoch 838 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021879\n",
      "Epoch 839 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021876\n",
      "Epoch 840 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021874\n",
      "Epoch 841 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021872\n",
      "Epoch 842 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021869\n",
      "Epoch 843 of 1000 took 0.193s\n",
      "  training loss:\t\t0.021867\n",
      "Epoch 844 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021865\n",
      "Epoch 845 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021863\n",
      "Epoch 846 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021860\n",
      "Epoch 847 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021858\n",
      "Epoch 848 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021856\n",
      "Epoch 849 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021854\n",
      "Epoch 850 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021851\n",
      "Epoch 851 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021849\n",
      "Epoch 852 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021847\n",
      "Epoch 853 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021845\n",
      "Epoch 854 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021842\n",
      "Epoch 855 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021840\n",
      "Epoch 856 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021838\n",
      "Epoch 857 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021836\n",
      "Epoch 858 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021834\n",
      "Epoch 859 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021831\n",
      "Epoch 860 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021829\n",
      "Epoch 861 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021827\n",
      "Epoch 862 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021825\n",
      "Epoch 863 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021823\n",
      "Epoch 864 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021821\n",
      "Epoch 865 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021818\n",
      "Epoch 866 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021816\n",
      "Epoch 867 of 1000 took 0.193s\n",
      "  training loss:\t\t0.021814\n",
      "Epoch 868 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021812\n",
      "Epoch 869 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021810\n",
      "Epoch 870 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021808\n",
      "Epoch 871 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021805\n",
      "Epoch 872 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021803\n",
      "Epoch 873 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021801\n",
      "Epoch 874 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021799\n",
      "Epoch 875 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021797\n",
      "Epoch 876 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021795\n",
      "Epoch 877 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021793\n",
      "Epoch 878 of 1000 took 0.196s\n",
      "  training loss:\t\t0.021791\n",
      "Epoch 879 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021788\n",
      "Epoch 880 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021786\n",
      "Epoch 881 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021784\n",
      "Epoch 882 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021782\n",
      "Epoch 883 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021780\n",
      "Epoch 884 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021778\n",
      "Epoch 885 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021776\n",
      "Epoch 886 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021774\n",
      "Epoch 887 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021772\n",
      "Epoch 888 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021770\n",
      "Epoch 889 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021768\n",
      "Epoch 890 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021765\n",
      "Epoch 891 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021763\n",
      "Epoch 892 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021761\n",
      "Epoch 893 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021759\n",
      "Epoch 894 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021757\n",
      "Epoch 895 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021755\n",
      "Epoch 896 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021753\n",
      "Epoch 897 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021751\n",
      "Epoch 898 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021749\n",
      "Epoch 899 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021747\n",
      "Epoch 900 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021745\n",
      "Epoch 901 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021743\n",
      "Epoch 902 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021741\n",
      "Epoch 903 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021739\n",
      "Epoch 904 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021737\n",
      "Epoch 905 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021735\n",
      "Epoch 906 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021733\n",
      "Epoch 907 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021731\n",
      "Epoch 908 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021729\n",
      "Epoch 909 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021727\n",
      "Epoch 910 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021725\n",
      "Epoch 911 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021723\n",
      "Epoch 912 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021721\n",
      "Epoch 913 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021719\n",
      "Epoch 914 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021717\n",
      "Epoch 915 of 1000 took 0.193s\n",
      "  training loss:\t\t0.021715\n",
      "Epoch 916 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021713\n",
      "Epoch 917 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021711\n",
      "Epoch 918 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021710\n",
      "Epoch 919 of 1000 took 0.193s\n",
      "  training loss:\t\t0.021708\n",
      "Epoch 920 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021706\n",
      "Epoch 921 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021704\n",
      "Epoch 922 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021702\n",
      "Epoch 923 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021700\n",
      "Epoch 924 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021698\n",
      "Epoch 925 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021696\n",
      "Epoch 926 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021694\n",
      "Epoch 927 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021692\n",
      "Epoch 928 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021690\n",
      "Epoch 929 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021688\n",
      "Epoch 930 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021687\n",
      "Epoch 931 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021685\n",
      "Epoch 932 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021683\n",
      "Epoch 933 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021681\n",
      "Epoch 934 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021679\n",
      "Epoch 935 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021677\n",
      "Epoch 936 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021675\n",
      "Epoch 937 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021673\n",
      "Epoch 938 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021672\n",
      "Epoch 939 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021670\n",
      "Epoch 940 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021668\n",
      "Epoch 941 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021666\n",
      "Epoch 942 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021664\n",
      "Epoch 943 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021662\n",
      "Epoch 944 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021660\n",
      "Epoch 945 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021659\n",
      "Epoch 946 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021657\n",
      "Epoch 947 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021655\n",
      "Epoch 948 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021653\n",
      "Epoch 949 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021651\n",
      "Epoch 950 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021649\n",
      "Epoch 951 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021648\n",
      "Epoch 952 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021646\n",
      "Epoch 953 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021644\n",
      "Epoch 954 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021642\n",
      "Epoch 955 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021640\n",
      "Epoch 956 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021639\n",
      "Epoch 957 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021637\n",
      "Epoch 958 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021635\n",
      "Epoch 959 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021633\n",
      "Epoch 960 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021631\n",
      "Epoch 961 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021630\n",
      "Epoch 962 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021628\n",
      "Epoch 963 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021626\n",
      "Epoch 964 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021624\n",
      "Epoch 965 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021623\n",
      "Epoch 966 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021621\n",
      "Epoch 967 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021619\n",
      "Epoch 968 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021617\n",
      "Epoch 969 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021616\n",
      "Epoch 970 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021614\n",
      "Epoch 971 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021612\n",
      "Epoch 972 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021610\n",
      "Epoch 973 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021609\n",
      "Epoch 974 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021607\n",
      "Epoch 975 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021605\n",
      "Epoch 976 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021603\n",
      "Epoch 977 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021602\n",
      "Epoch 978 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021600\n",
      "Epoch 979 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021598\n",
      "Epoch 980 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021597\n",
      "Epoch 981 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021595\n",
      "Epoch 982 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021593\n",
      "Epoch 983 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021591\n",
      "Epoch 984 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021590\n",
      "Epoch 985 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021588\n",
      "Epoch 986 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021586\n",
      "Epoch 987 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021585\n",
      "Epoch 988 of 1000 took 0.194s\n",
      "  training loss:\t\t0.021583\n",
      "Epoch 989 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021581\n",
      "Epoch 990 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021580\n",
      "Epoch 991 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021578\n",
      "Epoch 992 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021576\n",
      "Epoch 993 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021575\n",
      "Epoch 994 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021573\n",
      "Epoch 995 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021571\n",
      "Epoch 996 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021570\n",
      "Epoch 997 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021568\n",
      "Epoch 998 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021566\n",
      "Epoch 999 of 1000 took 0.192s\n",
      "  training loss:\t\t0.021565\n",
      "Epoch 1000 of 1000 took 0.195s\n",
      "  training loss:\t\t0.021563\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "# updates = lasagne.updates.nesterov_momentum(\n",
    "#     loss, params[10:12], learning_rate=0.01, momentum=0.9)\n",
    "updates = lasagne.updates.rmsprop(\n",
    "    loss, params[10:12], learning_rate=0.001)\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates,on_unused_input='warn')\n",
    "print(\"Starting training...\")\n",
    "best_err = 0.025\n",
    "for epoch in range(num_epochs):\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(data0, data2_out, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    if train_err / train_batches < best_err:\n",
    "        best_err = train_err / train_batches\n",
    "        action2_W  = action_layer.W.get_value()\n",
    "        action2_b = action_layer.b.get_value()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savez('action2_weight.npz',action2_W,action2_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_func = theano.function([input_var], [lasagne.layers.get_output(network)])\n",
    "a2_pred = np.zeros((500,784))\n",
    "a2_pred = output_func(test_set)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_images_array(index):\n",
    "    length = len(index)\n",
    "    original_image = Image.fromarray(get_picture_array(255 * test_set[index[0]]))\n",
    "    width,height = original_image.size\n",
    "    new_size = (width * length, height*2)\n",
    "    new_im = Image.new('L', new_size)\n",
    "    for i in range(length):\n",
    "        a1_image = Image.fromarray(get_picture_array(255 * action2(test_set)[index[i]]))\n",
    "        a1_pred_image = Image.fromarray(get_picture_array(255 * a2_pred[index[i]]))\n",
    "        new_im.paste(a1_image, (i*width,0))\n",
    "        new_im.paste(a1_pred_image, (i*width,height))\n",
    "    new_im.save('test.png', format=\"PNG\")\n",
    "    return IPImage('test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGAAAABwCAAAAAB0/Fn9AAAsQUlEQVR4nO3dd4BkVZn//9et2Lkn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\n8ppOcZ2W6NFKQa0O2hkpmIiIiDHj/wNdReWD6c7pIwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_images_array(index[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
